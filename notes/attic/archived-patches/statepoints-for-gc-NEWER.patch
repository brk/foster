diff --git a/.hgignore b/.hgignore
--- a/.hgignore
+++ b/.hgignore
@@ -19,23 +19,30 @@
 CMakeCache.txt
 cmake_install.cmake
 diff.txt
-meGCstats.txt
 _obj/
 tmp/
+disabled.hg/
+dox/
 notes/_build/
 \.vscode/
 glob:test/c2f/*.txt
 glob:test/c2f/*.foster
 glob:compiler/me/*.protox
+glob:test/**/gmp-*/
+glob:test/**/libtommath/
+glob:test/**/libtommath*.a
 compiler/me/dist/
 compiler/me/src/FosterIL_capnp.foster
 compiler/me/src/FosterIL_capnp.hs
 compiler/me/src/Foster/Tokens.hs
+compiler/me/src/Llvmpb/
+compiler/me/src/Llvmpb.hs
 compiler/me/src/Main.hsext
 runtime/gc/foster_gc_reconfig-inl.h
 glob:compiler/me/**.hi
 glob:compiler/me/**.*-boot
 glob:test/**.exe
+glob:*.lkshw
 glob:*\#
 compiler/me/.stack-work/
 syntax: glob
diff --git a/CMakeLists.txt b/CMakeLists.txt
--- a/CMakeLists.txt
+++ b/CMakeLists.txt
@@ -100,6 +100,10 @@
   PATHS
     $ENV{HOME}/.local/capnp-c++-0.6.1/bin
     $ENV{HOME}/sw/local/capnp-c++-0.6.1/bin
+    $ENV{HOME}/sw/local/capnp-c++-0.6.0/bin
+    $ENV{HOME}/sw/local/capnproto-0.5.0/bin
+    $ENV{HOME}/sw/local/capnproto-0.5.3/bin
+    $ENV{HOME}/sw/local/capnp-c++-0.5.3/bin
   DOC "capnp: Cap'n Proto compiler driver")
 
 find_program(MINUPROTO NAMES minuproto
@@ -465,14 +469,12 @@
   COMPILE_FLAGS "${ANTLR_C_FLAGS}")
 
 add_library(fosterc_llvm STATIC
-  compiler/llvm/passes/GCBarrierOptimizer.cpp
   compiler/llvm/passes/GCRootSafetyChecker.cpp
   compiler/llvm/passes/EscapingAllocaFinder.cpp
   compiler/llvm/passes/FosterMallocSpecializer.cpp
   compiler/llvm/passes/CallingConventionChecker.cpp
   compiler/llvm/passes/TimerCheckerInsertion.cpp
   compiler/llvm/passes/BitcastLoadRecognizer.cpp
-  compiler/llvm/passes/RewriteStatepointsForGCAlt.cpp
   compiler/llvm/passes/FosterPasses.cpp
   compiler/llvm/plugins/FosterGC.cpp
 )
diff --git a/compiler/base/LLVMUtils.cpp b/compiler/base/LLVMUtils.cpp
--- a/compiler/base/LLVMUtils.cpp
+++ b/compiler/base/LLVMUtils.cpp
@@ -261,45 +261,6 @@
   return p->isPointerTy() && typesEq(p->getContainedType(0), t);
 }
 
-bool ptrsWithDifferentAddrSpaces(llvm::Type* t1, llvm::Type* t2) {
-  if (auto p1 = dyn_cast<llvm::PointerType>(t1)) {
-    if (auto p2 = dyn_cast<llvm::PointerType>(t2)) {
-      return p1->getAddressSpace() != p2->getAddressSpace();
-    }
-  }
-  return false;
-}
-
-bool isLargishStructPointerTy(llvm::Type* ty) {
-  if (auto pt = llvm::dyn_cast<llvm::PointerType>(ty)) {
-    if (auto st = llvm::dyn_cast<llvm::StructType>(pt->getElementType())) {
-      return st->getNumElements() >= 2;
-    }
-  }
-  return false;
-}
-
-llvm::Value* emitBitcast(llvm::Value* v, llvm::Type* dstTy, llvm::StringRef msg) {
-  llvm::Type* srcTy = v->getType();
-  if (srcTy->isVoidTy()) {
-    return getNullOrZero(dstTy);
-  }
-  if (isFunctionPointerTy(srcTy) && isLargishStructPointerTy(dstTy)) {
-    ASSERT(false) << "cannot cast " << str(srcTy) << " to " << str(dstTy) << "\n" << str(v);
-  }
-  if (dstTy->isPointerTy() != srcTy->isPointerTy()) {
-    foster::builder.GetInsertBlock()->getParent()->dump();
-    ASSERT(false) << "cannot cast " << str(srcTy) << " to " << str(dstTy) << "\ndue to pointer-type mismatch\n" << str(v);
-  }
-  if (ptrsWithDifferentAddrSpaces(srcTy, dstTy)) {
-    v = foster::builder.CreateAddrSpaceCast(v,
-          llvm::PointerType::get(srcTy->getContainedType(0),
-            dyn_cast<llvm::PointerType>(dstTy)->getAddressSpace()));
-  }
-  return foster::builder.CreateBitCast(v, dstTy, msg);
-}
-
-
 void storeNullPointerToSlot(llvm::Value* slot) {
   foster::builder.CreateStore(
     llvm::ConstantPointerNull::getNullValue(slot->getType()->getContainedType(0)),
@@ -307,14 +268,6 @@
 }
 
 
-bool isPointerToOpaque(llvm::Type* p) {
-  if (!p->isPointerTy()) return false;
-  if (auto sty = dyn_cast<StructType>(p->getContainedType(0))) {
-    return sty->isOpaque();
-  }
-  return false;
-}
-
 
 bool is32Bit() {
   #if defined(__x86_64__) || defined(__x86_64)
@@ -360,15 +313,11 @@
 }
 
 llvm::PointerType* getHeapPtrTo(llvm::Type* t) {
-  //return llvm::PointerType::getUnqual(t);
-  return llvm::PointerType::get(t, 1);
-}
-
-llvm::PointerType* asNonHeap(llvm::Type* t) {
-   return rawPtrTo(t->getContainedType(0));
+  return llvm::PointerType::getUnqual(t);
+  //return llvm::PointerType::get(t, 1);
 }
 
 bool isFosterFunction(llvm::Function& F) {
-  return F.hasGC() && F.getGC() == std::string("statepoint-example");
+  return F.hasGC() && F.getGC() == std::string("fostergc");
 }
 
diff --git a/compiler/fosterlower.cpp b/compiler/fosterlower.cpp
--- a/compiler/fosterlower.cpp
+++ b/compiler/fosterlower.cpp
@@ -164,11 +164,6 @@
   cl::desc("Emit GC write barriers for object initialization"),
   cl::cat(FosterOptCat));
 
-static cl::list<std::string>
-linkAgainstBCs("link-against",
-  cl::desc("Link against the provided bitcode module(s)"),
-  cl::cat(FosterOptCat));
-
 void printVersionInfo(llvm::raw_ostream& out) {
   out << "Foster version: " << FOSTER_VERSION_STR << "\n";
   cl::PrintVersionMessage();
@@ -244,58 +239,6 @@
   return prog;
 }
 
-bool isEmptyStruct(llvm::Type* t) {
-  if (llvm::StructType* s = dyn_cast<StructType>(t)) {
-    return s->getNumElements() == 0;
-  }
-  return false;
-}
-
-bool isVoidAndPointerToEmptyStruct(llvm::Type* t1, llvm::Type* t2) {
-  return t1->isVoidTy() && t2->isPointerTy() && isEmptyStruct(t2->getContainedType(0));
-}
-
-bool isZeroLengthArray(llvm::Type* t) {
-  if (auto arty = dyn_cast<ArrayType>(t)) {
-    return arty->getNumElements() == 0;
-  }
-  return false;
-}
-
-bool isArrayLike(llvm::Type* t) {
-  if (t->isPointerTy()) {
-    if (llvm::StructType* s = dyn_cast<StructType>(t->getContainedType(0))) {
-      if (s->getNumElements() == 2 && s->getContainedType(0) == foster::builder.getInt64Ty()
-        && isZeroLengthArray(s->getContainedType(1)))
-        return true;
-    }
-  }
-  return false;
-}
-
-bool typesAreCastable(llvm::Type* t1, llvm::Type* t2) {
-  if (t1 == t2) return true;
-  if (isVoidAndPointerToEmptyStruct(t1, t2) || isVoidAndPointerToEmptyStruct(t2, t1)) {
-    return true;
-  }
-  if (isArrayLike(t1) || isArrayLike(t2)) return true;
-
-  if (isFunctionPointerTy(t1) && isFunctionPointerTy(t2)) {
-    auto f1 = dyn_cast<FunctionType>(t1->getContainedType(0));
-    auto f2 = dyn_cast<FunctionType>(t2->getContainedType(0));
-    if (!typesAreCastable(f1->getReturnType(), f2->getReturnType())) return false;
-    ArrayRef<Type*> a1 = f1->params();
-    ArrayRef<Type*> a2 = f2->params();
-    if (a1.size() != a2.size()) return false;
-    for (size_t i = 0; i < a1.size(); ++i) {
-      if (!typesAreCastable(a1[i], a2[i])) return false;
-    }
-    return true;
-  }
-  if (isPointerToOpaque(t1) || isPointerToOpaque(t2)) return true;
-  return false;
-}
-
 bool
 areDeclaredValueTypesOK(llvm::Module* mod,
      const std::vector<LLDecl*>& decls) {
@@ -330,28 +273,20 @@
     ASSERT(v) << "unable to find module entry for " << d->getName();
     llvm::Type* ty = t->getLLVMType();
     if (v->getType() != ty) {
-      if (llvm::isa<Constant>(v) && isPointerToType(v->getType(), ty)) {
-        // Imported constants should be automatically dereferenced.
-        d->autoDeref = true;
-      }
       // TODO check to see if type are sanely bit-castable
       //      (e.g. they only differ underneath a pointer...).
-      else if (d->getName() == "foster_stdin_read_bytes"
+      if (d->getName() == "foster_stdin_read_bytes"
        || d->getName() == "foster_posix_read_bytes"
        || d->getName() == "foster_posix_write_bytes"
        || d->getName() == "foster_posix_write_bytes_to_file") {
         gDeclaredSymbolTypes[d->getName()] = ty;
       } else {
-        if (typesAreCastable(v->getType(), ty)) {
-          // TODO generate cast?
-        } else {
-          EDiag() << "mismatch between declared and imported types"
-                  << " for symbol " << d->getName() << ":\n"
-                  << "Declared: " << str(t) << "\n"
-                  << " in LLVM: " << str(ty) << "\n"
-                  << "Imported: " << str(v->getType()) << "\n";
-          return false;
-        }
+        EDiag() << "mismatch between declared and imported types"
+                << " for symbol " << d->getName() << ":\n"
+                << "Declared: " << str(t) << "\n"
+                << " in LLVM: " << str(ty) << "\n"
+                << "Imported: " << str(v->getType()) << "\n";
+        return false;
       }
     }
     }
@@ -408,20 +343,12 @@
     llvm::Type* i64 = foster::builder.getInt64Ty();
     module->getOrInsertFunction("opaquely_i64",
         FunctionType::get(i64, llvm::makeArrayRef(i64), /*isVarArg=*/ false));
-
-    module->getOrInsertFunction("foster__get_ctor_as_ptr",
-        FunctionType::get(foster::builder.getInt8PtrTy(), { foster::builder.getInt8Ty() }, /*isVarArg=*/ false));        
   }
 
   if (!optStandalone) {
     libfoster_bc = readLLVMModuleFromPath(optBitcodeLibsDir + "/foster_runtime.bc");
     foster::putModuleFunctionsInScope(libfoster_bc.get(), module);
 
-    for (auto arg : linkAgainstBCs) {
-      auto bc = readLLVMModuleFromPath(arg);
-      linkTo(std::move(bc), arg, *module);
-    }
-
     // The module is "unclean" because it now has types that refer to libfoster_bc;
     // remove the taint by round-tripping to disk. Usually takes ~1ms.
     { ScopedTimer timer("llvm.roundtrip");
diff --git a/compiler/fosteroptc.cpp b/compiler/fosteroptc.cpp
--- a/compiler/fosteroptc.cpp
+++ b/compiler/fosteroptc.cpp
@@ -197,7 +197,6 @@
       return GV.getName() == "foster__runtime__main__wrapper"
           || GV.getName() == "foster__main"
           || GV.getName() == "foster__gcmaps"
-          || GV.getName() == "__LLVM_StackMaps"
           || GV.getName() == "foster_coro_delete_self_reference";
     };
     passes.add(createInternalizePass(PreserveThese));
@@ -587,10 +586,6 @@
   }
 }
 
-namespace foster {
-  llvm::ModulePass *createRewriteStatepointsForGCAltLegacyPass();
-}
-
 int main(int argc, char** argv) {
   int program_status = 0;
   foster::linkFosterGC(); // statically, not dynamically
@@ -634,43 +629,6 @@
       }
     }
 
-    if (true) {
-      legacy::PassManager passes;
-      legacy::FunctionPassManager fpasses(module);
-
-      {
-        auto F = module->getFunction("foster_gc_safepoint_poll");
-        auto FT = F->getFunctionType();
-        std::vector<llvm::Type*> argTys;
-        auto FfuncT = llvm::FunctionType::get(FT->getReturnType(), argTys, false);
-        llvm::GlobalValue::LinkageTypes linkage = llvm::GlobalValue::ExternalLinkage;
-        auto Ffunc = Function::Create(FfuncT, linkage, "gc.safepoint_poll", module);
-
-        llvm::IRBuilder<> tmpBuilder(foster::fosterLLVMContext);
-        BasicBlock* BB = BasicBlock::Create(tmpBuilder.getContext(), "entry", Ffunc);
-        tmpBuilder.SetInsertPoint(BB);
-        
-        std::vector<llvm::Value*> args;
-        auto callInst = tmpBuilder.CreateCall(F, args);
-        callInst->setTailCall(true);
-        tmpBuilder.CreateRetVoid();
-      }
-
-      fpasses.add(createPlaceSafepointsPass());
-      passes.add(foster::createRewriteStatepointsForGCAltLegacyPass());
-      //passes.add(llvm::createRewriteStatepointsForGCLegacyPass());
-
-      llvm::outs() << "Beginning safepoint placement pass" << "\n";
-      foster::runFunctionPassesOverModule(fpasses, module);
-
-      llvm::outs() << "Beginning safepoint rewriting pass" << "\n";
-      passes.run(*module);
-    }
-
-    if (optDumpPostOptIR) {
-      dumpModuleToFile(module,  (gOutputNameBase + ".final.ll"));
-    }
-
     compileToNativeAssemblyOrObject(module, optOutputName);
   }
 
diff --git a/compiler/include/foster/base/LLVMUtils.h b/compiler/include/foster/base/LLVMUtils.h
--- a/compiler/include/foster/base/LLVMUtils.h
+++ b/compiler/include/foster/base/LLVMUtils.h
@@ -69,12 +69,8 @@
 // returns true if p == t*
 bool isPointerToType(llvm::Type* p, llvm::Type* t);
 
-bool isPointerToOpaque(llvm::Type* p);
-
 bool typesEq(llvm::Type* t1, llvm::Type* t2);
 
-llvm::Value* emitBitcast(llvm::Value* v, llvm::Type* dstTy, llvm::StringRef msg = "");
-
 void storeNullPointerToSlot(llvm::Value* slot);
 
 bool is32Bit();
@@ -86,7 +82,6 @@
 llvm::PointerType* getHeapPtrTo(llvm::Type*);
 llvm::PointerType* rawPtrTo(llvm::Type*);
 llvm::PointerType* ptrTo(llvm::Type*);
-llvm::PointerType* asNonHeap(llvm::Type*);
 
 
 bool isFosterFunction(llvm::Function& F);
diff --git a/compiler/include/foster/passes/CodegenPass-impl.h b/compiler/include/foster/passes/CodegenPass-impl.h
--- a/compiler/include/foster/passes/CodegenPass-impl.h
+++ b/compiler/include/foster/passes/CodegenPass-impl.h
@@ -31,6 +31,8 @@
                         std::string    desiredName,
                         CtorRepr       ctorRepr,
                         llvm::Module*  mod);
+llvm::GlobalVariable* getTypeMapForType(TypeAST*, CtorRepr ctorRepr,
+                                        llvm::Module*, ArrayOrNot);
 
 bool mayContainGCablePointers(llvm::Type* ty);
 bool containsGCablePointers(TypeAST* typ, llvm::Type* ty);
@@ -43,6 +45,7 @@
                                                    llvm::Value* len64,
                                                    const std::string& srclines);
 Value* getUnitValue();
+Value* getElementFromComposite(Value* compositeValue, int, const std::string& msg);
 Value* getPointerToIndex(Value* compositeValue,
                          Value* idxValue,
                          const std::string& name);
@@ -112,7 +115,6 @@
   WorklistLIFO<std::string, LLBlock*>   worklistBlocks;
   std::map<std::string, llvm::Value*> staticStrings; // anonymous strings, by contents
   std::map<std::string, llvm::Value*> globalValues; // named values
-  std::map<std::string, llvm::Value*> autoDerefs;
 
   std::string currentProcName;
 
@@ -165,10 +167,6 @@
   void emitLazyCoroPrimInfo(bool isYield, llvm::Function* fn,
                            llvm::Type* retTy,
                            llvm::Type* argTypes);
-
-  llvm::GlobalVariable* getTypeMapForType(TypeAST*, CtorRepr ctorRepr,
-                                          llvm::Module*, ArrayOrNot);
-  void emitTypeMapListGlobal();
 };
 
 #endif // header guard
diff --git a/compiler/include/foster/passes/FosterLL.h b/compiler/include/foster/passes/FosterLL.h
--- a/compiler/include/foster/passes/FosterLL.h
+++ b/compiler/include/foster/passes/FosterLL.h
@@ -113,9 +113,9 @@
   string name;
   TypeAST* type;
   bool isForeign;
-  bool autoDeref;
   explicit LLDecl(const string& name, TypeAST* type, bool isForeign)
-      : name(name), type(type), isForeign(isForeign), autoDeref(false) {}
+      : name(name), type(type), isForeign(isForeign) {}
+  llvm::Value* codegen(CodegenPass* pass);
   const string getName() const { return name; }
   TypeAST*     getType() const { return type; }
 };
@@ -132,13 +132,9 @@
 struct LLTopItem {
   string   name;
   LLArrayLiteral*  arrlit;
-  LLExpr*  lit;
 
   explicit LLTopItem(const string& name, LLArrayLiteral* arrlit)
-      : name(name), arrlit(arrlit), lit(nullptr) {}
-
-  explicit LLTopItem(const string& name, LLExpr* elit)
-      : name(name), arrlit(nullptr), lit(elit) {}
+      : name(name), arrlit(arrlit) {}
 };
 
 struct LLModule {
@@ -436,10 +432,8 @@
 
 struct LLUnboxedTuple : public LLExpr {
   std::vector<LLVar*> vars;
-  bool isStatic;
 
-  explicit LLUnboxedTuple(const std::vector<LLVar*>& vars, bool isStatic)
-    : LLExpr("LLUnboxedTuple"), vars(vars), isStatic(isStatic) {}
+  explicit LLUnboxedTuple(const std::vector<LLVar*>& vars) : LLExpr("LLUnboxedTuple"), vars(vars) {}
   virtual llvm::Value* codegen(CodegenPass* pass);
 };
 
@@ -514,15 +508,6 @@
   virtual llvm::Value* codegen(CodegenPass* pass);
 };
 
-struct LLGlobalAppCtor : public LLExpr {
-  std::vector<LLVar*> args;
-  CtorInfo ctor;
-
-  explicit LLGlobalAppCtor(CtorInfo ctor, std::vector<LLVar*> args)
-    : LLExpr("LLGlobalAppCtor"), args(args), ctor(ctor) {}
-  virtual llvm::Value* codegen(CodegenPass* pass);
-};
-
 ///////////////////////////////////////////////////////////
 ///////////////////////////////////////////////////////////
 
diff --git a/compiler/llvm/passes/BitcastLoadRecognizer.cpp b/compiler/llvm/passes/BitcastLoadRecognizer.cpp
--- a/compiler/llvm/passes/BitcastLoadRecognizer.cpp
+++ b/compiler/llvm/passes/BitcastLoadRecognizer.cpp
@@ -447,7 +447,7 @@
 
 INITIALIZE_PASS(BitcastLoadRecognizer, "foster-bitcast-recognizer",
                 "Peephole optimization of suboptimal bitcasting pattern",
-                true, false);
+                false, false);
 
 namespace foster {
 
diff --git a/compiler/llvm/passes/CallingConventionChecker.cpp b/compiler/llvm/passes/CallingConventionChecker.cpp
--- a/compiler/llvm/passes/CallingConventionChecker.cpp
+++ b/compiler/llvm/passes/CallingConventionChecker.cpp
@@ -57,7 +57,7 @@
 
 INITIALIZE_PASS(CallingConventionChecker, "foster-calling-convention-checker",
                 "Check that known calls use the right calling convention",
-                true, false);
+                false, false);
 
 namespace foster {
 
diff --git a/compiler/llvm/passes/FosterPasses.cpp b/compiler/llvm/passes/FosterPasses.cpp
--- a/compiler/llvm/passes/FosterPasses.cpp
+++ b/compiler/llvm/passes/FosterPasses.cpp
@@ -19,7 +19,6 @@
 
   llvm::legacy::PassManager passes;
   passes.add(llvm::createDeadInstEliminationPass());
-  passes.add(foster::createGCBarrierOptimizerPass());
   passes.run(mod);
 }
 
diff --git a/compiler/llvm/passes/FosterPasses.h b/compiler/llvm/passes/FosterPasses.h
--- a/compiler/llvm/passes/FosterPasses.h
+++ b/compiler/llvm/passes/FosterPasses.h
@@ -17,14 +17,12 @@
 void runWarningPasses(llvm::Module& mod);
 
 llvm::Pass* createGCMallocFinderPass();
-llvm::Pass* createGCBarrierOptimizerPass();
 llvm::Pass* createGCRootSafetyCheckerPass();
 llvm::Pass* createMemallocSpecializerPass();
 llvm::Pass* createEscapingAllocaFinderPass();
 llvm::Pass* createTimerChecksInsertionPass();
 llvm::Pass* createBitcastLoadRecognizerPass();
 llvm::Pass* createCallingConventionCheckerPass();
-llvm::Pass* createRewriteStatepointsForGCAltPass();
 
 }
 
diff --git a/compiler/llvm/passes/GCBarrierOptimizer.cpp b/compiler/llvm/passes/GCBarrierOptimizer.cpp
deleted file mode 100644
--- a/compiler/llvm/passes/GCBarrierOptimizer.cpp
+++ /dev/null
@@ -1,249 +0,0 @@
-// Copyright (c) 2018 Ben Karel. All rights reserved.
-// Use of this source code is governed by a BSD-style license that can be
-// found in the LICENSE.txt file or at http://eschew.org/txt/bsd.txt
-
-#include "llvm/Pass.h"
-#include "llvm/Analysis/CallGraph.h"
-#include "llvm/Analysis/CallGraphSCCPass.h"
-#include "llvm/IR/Function.h"
-#include "llvm/IR/LLVMContext.h"
-#include "llvm/IR/Intrinsics.h"
-#include "llvm/IR/Instructions.h"
-#include "llvm/IR/IRBuilder.h"
-#include "llvm/Support/raw_ostream.h"
-#include "llvm/ADT/StringSwitch.h"
-#include "llvm/IR/InstIterator.h"
-#include "llvm/IR/IntrinsicInst.h"
-#include "llvm/IR/CallSite.h"
-
-#include "base/GenericGraph.h"
-
-#include <set>
-#include <map>
-
-using namespace llvm;
-
-// When we have a barrier to write a pointer P to slot S,
-// we can elide the barrier if P points into the subheap that contains S.
-//
-// One common instance of this pattern is
-// * Allocate X
-// * Allocate Y
-// * Write Y into X
-// (with no intervening calls to subheap_activate()).
-//
-// Note that this optimization is *not* valid for a generational write barrier!
-
-namespace llvm {
-  void initializeGCBarrierOptimizerPass(llvm::PassRegistry&);
-}
-
-namespace {
-
-struct GCBarrierOptimizer : public CallGraphSCCPass {
-  static char ID;
-  explicit GCBarrierOptimizer() : CallGraphSCCPass(ID) {
-    llvm::initializeGCBarrierOptimizerPass(*PassRegistry::getPassRegistry());
-  }
-
-  llvm::StringRef getPassName() const { return "GCBarrierOptimizer"; }
-
-  void getAnalysisUsage(AnalysisUsage &AU) const {
-    AU.setPreservesCFG();
-    CallGraphSCCPass::getAnalysisUsage(AU);
-  }
-
-  //  * Iterate over each basic block, maintaining a list of objects allocated
-  //    in the current subheap. Function parameters are treated conservatively.
-  //  * When calling a function that might activate a new subheap, clear the list.
-  //  * When a barrier writes two objects in the list, elide it.
-
-  typedef std::set<llvm::Value*> ValueSet;
-  typedef std::map<llvm::Value*, llvm::Value*> ValueValueMap;
-
-  std::set<llvm::Function*> functionsThatMightChangeSubheaps;
-  std::set<llvm::Function*> allocatorFunctions;
-
-  bool mightChangeSubheaps(Function* F) const {
-    return (!F) || functionsThatMightChangeSubheaps.count(F) == 1;
-  }
-
-  virtual bool doInitialization(CallGraph& CG) {
-    Function* f = CG.getModule().getFunction("foster_subheap_activate");
-    functionsThatMightChangeSubheaps.insert(f);
-
-    Function* a = CG.getModule().getFunction("memalloc_cell");
-    llvm::outs() << "GC Barrier optimizer starting with " << *f << " and " << *a << "\n";
-    allocatorFunctions.insert(a);
-    allocatorFunctions.insert(CG.getModule().getFunction("memalloc_cell_16"));
-    allocatorFunctions.insert(CG.getModule().getFunction("memalloc_cell_32"));
-    allocatorFunctions.insert(CG.getModule().getFunction("memalloc_cell_48"));
-    allocatorFunctions.insert(CG.getModule().getFunction("memalloc_array"));
-    return false;
-  }
-
-  virtual bool runOnSCC(CallGraphSCC& SCC) {
-
-    // If any function in the SCC might activate a new subheap, they all must be marked.
-    bool mightActivateNewSubheap = false;
-    for (CallGraphNode* cgn : SCC) {
-      for (unsigned i = 0; i < cgn->size(); ++i) {
-        llvm::Function* callee = cgn[i].getFunction();
-        if (mightChangeSubheaps(callee)) {
-          mightActivateNewSubheap = true;
-        }
-      }
-    }
-
-    if (mightActivateNewSubheap) {
-      for (CallGraphNode* cgn : SCC) {
-        functionsThatMightChangeSubheaps.insert(cgn->getFunction());
-      }
-    }
-
-    ValueSet objectsInCurrentSubheap;
-    ValueSet rootSlotsHoldingCurrentSubheapObjects;
-
-    // Now that we have an accounting of which functions might cause subheap activation,
-    // go through each function and elide unnecessary write barriers.
-    for (CallGraphNode* cgn : SCC) {
-      Function* F = cgn->getFunction();
-
-      if (!F) continue;
-      if (!F->hasGC()) continue;
-      if (!llvm::StringRef(F->getGC()).startswith("fostergc")) continue;
-
-      int numReturnInstructionsTotal = 0;
-      int numReturnsOfFreshAllocations = 0;
-
-      int numTotalWriteBarriers = 0;
-      int numElidableWriteBarriers = 0;
-
-      std::vector<Instruction*> markedForDeath;
-
-      for (BasicBlock& bb : *F) {
-        for (Instruction& I : bb) {
-          Value* bare = I.stripPointerCasts();
-
-          if (ReturnInst* ri = dyn_cast<ReturnInst>(bare)) {
-            ++numReturnInstructionsTotal;
-            Value* rv = ri->getReturnValue();
-            if (rv) {
-              if (CallInst* rvc = dyn_cast<CallInst>(rv->stripPointerCasts())) {
-                if (allocatorFunctions.count(rvc->getCalledFunction()) == 1) {
-                  ++numReturnsOfFreshAllocations;
-                }
-              } else if (isa<Constant>(rv)) {
-                  // Constants include GlobalValues, null pointers, and undef,
-                  // none of which would lead to triggering write barriers.
-                  ++numReturnsOfFreshAllocations;
-              }
-            }
-          }
-
-          if (StoreInst* si = dyn_cast<StoreInst>(bare)) {
-            Value* slot = si->getPointerOperand()->stripPointerCasts();
-            if (isa<AllocaInst>(slot)) {
-              if (objectsInCurrentSubheap.count(si->getValueOperand()->stripPointerCasts())) {
-                rootSlotsHoldingCurrentSubheapObjects.insert(slot);
-              }
-            }
-          }
-
-          if (CallInst* ci = dyn_cast<CallInst>(bare)) {
-            Function* calleeOrNull = ci->getCalledFunction();
-            if (mightChangeSubheaps(calleeOrNull)) {
-              objectsInCurrentSubheap.clear();
-              rootSlotsHoldingCurrentSubheapObjects.clear();
-            }
-
-            if (allocatorFunctions.count(calleeOrNull) == 1) {
-              objectsInCurrentSubheap.insert(ci);
-            }
-
-            // Intrinsics, such as write barriers, are call instructions.
-            if (IntrinsicInst* ii = dyn_cast<IntrinsicInst>(ci)) {
-              if (ii->getIntrinsicID() == llvm::Intrinsic::gcwrite) {
-                ++numTotalWriteBarriers;
-
-                Value* ptr  = ii->getArgOperand(0)->stripPointerCasts();
-                Value* slot = ii->getArgOperand(2)->stripPointerCasts();
-                if (auto gep = dyn_cast<GetElementPtrInst>(slot)) {
-                  slot = gep->getPointerOperand()->stripPointerCasts();
-                }
-
-                bool ptrInCurrentSubheap = objectsInCurrentSubheap.count(ptr) == 1
-                                            || isa<Constant>(ptr);
-                bool slotInCurrentSubheap = objectsInCurrentSubheap.count(slot) == 1
-                                            || isa<Constant>(slot);
-                if (auto load = dyn_cast<LoadInst>(ptr)) {
-                  ptrInCurrentSubheap = rootSlotsHoldingCurrentSubheapObjects.count(
-                                            load->getPointerOperand()->stripPointerCasts()) == 1;
-                }
-                if (auto load = dyn_cast<LoadInst>(slot)) {
-                  slotInCurrentSubheap = rootSlotsHoldingCurrentSubheapObjects.count(
-                                            load->getPointerOperand()->stripPointerCasts()) == 1;
-                }
-
-                if (ptrInCurrentSubheap && slotInCurrentSubheap) {
-                  ++numElidableWriteBarriers;
-                  llvm::outs() << "specializing gcwrite of " << ptr->getName() << " to " << slot->getName() << "\n";
-                  auto si = new StoreInst(ii->getArgOperand(0), ii->getArgOperand(2), ii);
-                  ii->replaceAllUsesWith(si);
-                  markedForDeath.push_back(ii);
-                } else {
-                  llvm::outs() << "cannot specialize gcwrite of " << ptr->getName() << " to " << slot->getName() << ";"
-                        << "ptr? " << ptrInCurrentSubheap << "; slot? " << slotInCurrentSubheap << "\n";
-                }
-              }
-            }
-          }
-        }
-      }
-
-      for (auto inst : markedForDeath) {
-        inst->eraseFromParent();
-      }
-
-      if (numTotalWriteBarriers > 0) {
-        llvm::outs() << "numTotalWriteBarriers: " << numTotalWriteBarriers << "\n";
-      }
-      if (numElidableWriteBarriers > 0) {
-        llvm::outs() << "numElidableWriteBarriers: " << numElidableWriteBarriers << "\n";
-      }
-
-      if ( (numReturnInstructionsTotal > 0)
-            && (numReturnsOfFreshAllocations == numReturnInstructionsTotal) ) {
-        allocatorFunctions.insert(F);
-
-        llvm::outs() << "marking " << F->getName() << " as an allocating function\n";
-      } else {
-        llvm::outs() << "        " << F->getName() << " not'n allocating function"
-                << "(" << numReturnInstructionsTotal << " vs " << numReturnsOfFreshAllocations << ")\n";
-      }
-    }
-
-    return false;
-  }
-};
-
-char GCBarrierOptimizer::ID = 0;
-
-} // unnamed namespace
-
-/*
-INITIALIZE_PASS(GCBarrierOptimizer, "foster-gc-barrier-optimizer",
-                "Flow-based optimization of GC write barriers",
-                false, false)
-*/
-
-INITIALIZE_PASS_BEGIN(GCBarrierOptimizer, "foster-gc-barrier-optimizer", "Flow-based optimization of GC write barriers", /*cfgonly=*/true, /*analysis=*/false)
-INITIALIZE_PASS_DEPENDENCY(CallGraphWrapperPass)
-INITIALIZE_PASS_END(GCBarrierOptimizer, "foster-gc-barrier-optimizer", "Flow-based optimization of GC write barriers", /*cfgonly=*/true, /*analysis=*/false)
-
-
-namespace foster {
-
-Pass* createGCBarrierOptimizerPass() { return new GCBarrierOptimizer(); }
-
-}
diff --git a/compiler/llvm/passes/GCRootSafetyChecker.cpp b/compiler/llvm/passes/GCRootSafetyChecker.cpp
--- a/compiler/llvm/passes/GCRootSafetyChecker.cpp
+++ b/compiler/llvm/passes/GCRootSafetyChecker.cpp
@@ -221,7 +221,7 @@
 
 INITIALIZE_PASS(GCRootSafetyChecker, "foster-gc-root-safety-checker",
                 "Incomplete and unsound identification of dodgy gc root usage",
-                true, false)
+                false, false)
 
 namespace foster {
 
diff --git a/compiler/llvm/passes/RewriteStatepointsForGCAlt.cpp b/compiler/llvm/passes/RewriteStatepointsForGCAlt.cpp
deleted file mode 100644
--- a/compiler/llvm/passes/RewriteStatepointsForGCAlt.cpp
+++ /dev/null
@@ -1,2856 +0,0 @@
-//===- RewriteStatepointsForGC.cpp - Make GC relocations explicit ---------===//
-//
-//                     The LLVM Compiler Infrastructure
-//
-// This file is distributed under the University of Illinois Open Source
-// License. See LICENSE.TXT for details.
-//
-//===----------------------------------------------------------------------===//
-//
-// Rewrite call/invoke instructions so as to make potential relocations
-// performed by the garbage collector explicit in the IR.
-//
-//===----------------------------------------------------------------------===//
-
-#include "llvm/Transforms/Scalar/RewriteStatepointsForGC.h"
-
-#include "llvm/ADT/ArrayRef.h"
-#include "llvm/ADT/DenseMap.h"
-#include "llvm/ADT/DenseSet.h"
-#include "llvm/ADT/MapVector.h"
-#include "llvm/ADT/None.h"
-#include "llvm/ADT/Optional.h"
-#include "llvm/ADT/STLExtras.h"
-#include "llvm/ADT/SetVector.h"
-#include "llvm/ADT/SmallSet.h"
-#include "llvm/ADT/SmallVector.h"
-#include "llvm/ADT/StringRef.h"
-#include "llvm/ADT/iterator_range.h"
-#include "llvm/Analysis/TargetLibraryInfo.h"
-#include "llvm/Analysis/TargetTransformInfo.h"
-#include "llvm/IR/Argument.h"
-#include "llvm/IR/Attributes.h"
-#include "llvm/IR/BasicBlock.h"
-#include "llvm/IR/CallSite.h"
-#include "llvm/IR/CallingConv.h"
-#include "llvm/IR/Constant.h"
-#include "llvm/IR/Constants.h"
-#include "llvm/IR/DataLayout.h"
-#include "llvm/IR/DerivedTypes.h"
-#include "llvm/IR/Dominators.h"
-#include "llvm/IR/Function.h"
-#include "llvm/IR/IRBuilder.h"
-#include "llvm/IR/InstIterator.h"
-#include "llvm/IR/InstrTypes.h"
-#include "llvm/IR/Instruction.h"
-#include "llvm/IR/Instructions.h"
-#include "llvm/IR/IntrinsicInst.h"
-#include "llvm/IR/Intrinsics.h"
-#include "llvm/IR/LLVMContext.h"
-#include "llvm/IR/MDBuilder.h"
-#include "llvm/IR/Metadata.h"
-#include "llvm/IR/Module.h"
-#include "llvm/IR/Statepoint.h"
-#include "llvm/IR/Type.h"
-#include "llvm/IR/User.h"
-#include "llvm/IR/Value.h"
-#include "llvm/IR/ValueHandle.h"
-#include "llvm/Pass.h"
-#include "llvm/Support/Casting.h"
-#include "llvm/Support/CommandLine.h"
-#include "llvm/Support/Compiler.h"
-#include "llvm/Support/Debug.h"
-#include "llvm/Support/ErrorHandling.h"
-#include "llvm/Support/raw_ostream.h"
-#include "llvm/Transforms/Scalar.h"
-#include "llvm/Transforms/Utils/BasicBlockUtils.h"
-#include "llvm/Transforms/Utils/Local.h"
-#include "llvm/Transforms/Utils/PromoteMemToReg.h"
-#include <algorithm>
-#include <cassert>
-#include <cstddef>
-#include <cstdint>
-#include <iterator>
-#include <set>
-#include <string>
-#include <utility>
-#include <vector>
-
-#define DEBUG_TYPE "rewrite-statepoints-for-gc"
-
-using namespace llvm;
-
-// Print the liveset found at the insert location
-static cl::opt<bool> PrintLiveSet("alt-spp-print-liveset", cl::Hidden,
-                                  cl::init(false));
-static cl::opt<bool> PrintLiveSetSize("alt-spp-print-liveset-size", cl::Hidden,
-                                      cl::init(false));
-
-// Print out the base pointers for debugging
-static cl::opt<bool> PrintBasePointers("alt-spp-print-base-pointers", cl::Hidden,
-                                       cl::init(false));
-
-// Cost threshold measuring when it is profitable to rematerialize value instead
-// of relocating it
-static cl::opt<unsigned>
-RematerializationThreshold("alt-spp-rematerialization-threshold", cl::Hidden,
-                           cl::init(6));
-
-#ifdef EXPENSIVE_CHECKS
-static bool ClobberNonLive = true;
-#else
-static bool ClobberNonLive = false;
-#endif
-
-static cl::opt<bool, true> ClobberNonLiveOverride("alt-rs4gc-clobber-non-live",
-                                                  cl::location(ClobberNonLive),
-                                                  cl::Hidden);
-
-static cl::opt<bool>
-    AllowStatepointWithNoDeoptInfo("alt-rs4gc-allow-statepoint-with-no-deopt-info",
-                                   cl::Hidden, cl::init(true));
-
-/// The IR fed into RewriteStatepointsForGC may have had attributes and
-/// metadata implying dereferenceability that are no longer valid/correct after
-/// RewriteStatepointsForGC has run. This is because semantically, after
-/// RewriteStatepointsForGC runs, all calls to gc.statepoint "free" the entire
-/// heap. stripNonValidData (conservatively) restores
-/// correctness by erasing all attributes in the module that externally imply
-/// dereferenceability. Similar reasoning also applies to the noalias
-/// attributes and metadata. gc.statepoint can touch the entire heap including
-/// noalias objects.
-/// Apart from attributes and metadata, we also remove instructions that imply
-/// constant physical memory: llvm.invariant.start.
-static void stripNonValidData(Module &M);
-
-static bool shouldRewriteStatepointsIn(Function &F);
-
-PreservedAnalyses RewriteStatepointsForGC::run(Module &M,
-                                               ModuleAnalysisManager &AM) {
-  bool Changed = false;
-  auto &FAM = AM.getResult<FunctionAnalysisManagerModuleProxy>(M).getManager();
-  for (Function &F : M) {
-    // Nothing to do for declarations.
-    if (F.isDeclaration() || F.empty())
-      continue;
-
-    // Policy choice says not to rewrite - the most common reason is that we're
-    // compiling code without a GCStrategy.
-    if (!shouldRewriteStatepointsIn(F))
-      continue;
-
-    auto &DT = FAM.getResult<DominatorTreeAnalysis>(F);
-    auto &TTI = FAM.getResult<TargetIRAnalysis>(F);
-    auto &TLI = FAM.getResult<TargetLibraryAnalysis>(F);
-    Changed |= runOnFunction(F, DT, TTI, TLI);
-  }
-  if (!Changed)
-    return PreservedAnalyses::all();
-
-  // stripNonValidData asserts that shouldRewriteStatepointsIn
-  // returns true for at least one function in the module.  Since at least
-  // one function changed, we know that the precondition is satisfied.
-  stripNonValidData(M);
-
-  PreservedAnalyses PA;
-  PA.preserve<TargetIRAnalysis>();
-  PA.preserve<TargetLibraryAnalysis>();
-  return PA;
-}
-
-namespace llvm {
-  void initializeRewriteStatepointsForGCAltLegacyPassPass(llvm::PassRegistry&);
-}
-
-namespace {
-
-class RewriteStatepointsForGCAltLegacyPass : public ModulePass {
-  RewriteStatepointsForGC Impl;
-
-public:
-  static char ID; // Pass identification, replacement for typeid
-
-  RewriteStatepointsForGCAltLegacyPass() : ModulePass(ID), Impl() {
-    initializeRewriteStatepointsForGCAltLegacyPassPass(
-        *PassRegistry::getPassRegistry());
-  }
-
-  bool runOnModule(Module &M) override {
-    bool Changed = false;
-    const TargetLibraryInfo &TLI =
-        getAnalysis<TargetLibraryInfoWrapperPass>().getTLI();
-    for (Function &F : M) {
-      // Nothing to do for declarations.
-      if (F.isDeclaration() || F.empty())
-        continue;
-
-      // Policy choice says not to rewrite - the most common reason is that
-      // we're compiling code without a GCStrategy.
-      if (!shouldRewriteStatepointsIn(F))
-        continue;
-
-      TargetTransformInfo &TTI =
-          getAnalysis<TargetTransformInfoWrapperPass>().getTTI(F);
-      auto &DT = getAnalysis<DominatorTreeWrapperPass>(F).getDomTree();
-
-      Changed |= Impl.runOnFunction(F, DT, TTI, TLI);
-    }
-
-    if (!Changed)
-      return false;
-
-    // stripNonValidData asserts that shouldRewriteStatepointsIn
-    // returns true for at least one function in the module.  Since at least
-    // one function changed, we know that the precondition is satisfied.
-    stripNonValidData(M);
-    return true;
-  }
-
-  void getAnalysisUsage(AnalysisUsage &AU) const override {
-    // We add and rewrite a bunch of instructions, but don't really do much
-    // else.  We could in theory preserve a lot more analyses here.
-    AU.addRequired<DominatorTreeWrapperPass>();
-    AU.addRequired<TargetTransformInfoWrapperPass>();
-    AU.addRequired<TargetLibraryInfoWrapperPass>();
-  }
-};
-
-} // end anonymous namespace
-
-char RewriteStatepointsForGCAltLegacyPass::ID = 0;
-
-namespace foster {
-  ModulePass *createRewriteStatepointsForGCAltLegacyPass() {
-    return new RewriteStatepointsForGCAltLegacyPass();
-  }
-}
-
-INITIALIZE_PASS_BEGIN(RewriteStatepointsForGCAltLegacyPass,
-                      "rewrite-statepoints-for-gc-alt",
-                      "Make relocations explicit at statepoints", false, false)
-INITIALIZE_PASS_DEPENDENCY(DominatorTreeWrapperPass)
-INITIALIZE_PASS_DEPENDENCY(TargetTransformInfoWrapperPass)
-INITIALIZE_PASS_END(RewriteStatepointsForGCAltLegacyPass,
-                    "rewrite-statepoints-for-gc-alt",
-                    "Make relocations explicit at statepoints", false, false)
-
-namespace {
-
-struct GCPtrLivenessData {
-  /// Values defined in this block.
-  MapVector<BasicBlock *, SetVector<Value *>> KillSet;
-
-  /// Values used in this block (and thus live); does not included values
-  /// killed within this block.
-  MapVector<BasicBlock *, SetVector<Value *>> LiveSet;
-
-  /// Values live into this basic block (i.e. used by any
-  /// instruction in this basic block or ones reachable from here)
-  MapVector<BasicBlock *, SetVector<Value *>> LiveIn;
-
-  /// Values live out of this basic block (i.e. live into
-  /// any successor block)
-  MapVector<BasicBlock *, SetVector<Value *>> LiveOut;
-};
-
-// The type of the internal cache used inside the findBasePointers family
-// of functions.  From the callers perspective, this is an opaque type and
-// should not be inspected.
-//
-// In the actual implementation this caches two relations:
-// - The base relation itself (i.e. this pointer is based on that one)
-// - The base defining value relation (i.e. before base_phi insertion)
-// Generally, after the execution of a full findBasePointer call, only the
-// base relation will remain.  Internally, we add a mixture of the two
-// types, then update all the second type to the first type
-using DefiningValueMapTy = MapVector<Value *, Value *>;
-using StatepointLiveSetTy = SetVector<Value *>;
-using RematerializedValueMapTy =
-    MapVector<AssertingVH<Instruction>, AssertingVH<Value>>;
-
-struct PartiallyConstructedSafepointRecord {
-  /// The set of values known to be live across this safepoint
-  StatepointLiveSetTy LiveSet;
-
-  /// Mapping from live pointers to a base-defining-value
-  MapVector<Value *, Value *> PointerToBase;
-
-  /// The *new* gc.statepoint instruction itself.  This produces the token
-  /// that normal path gc.relocates and the gc.result are tied to.
-  Instruction *StatepointToken;
-
-  /// Instruction to which exceptional gc relocates are attached
-  /// Makes it easier to iterate through them during relocationViaAlloca.
-  Instruction *UnwindToken;
-
-  /// Record live values we are rematerialized instead of relocating.
-  /// They are not included into 'LiveSet' field.
-  /// Maps rematerialized copy to it's original value.
-  RematerializedValueMapTy RematerializedValues;
-};
-
-} // end anonymous namespace
-
-static ArrayRef<Use> GetDeoptBundleOperands(ImmutableCallSite CS) {
-  Optional<OperandBundleUse> DeoptBundle =
-      CS.getOperandBundle(LLVMContext::OB_deopt);
-
-  if (!DeoptBundle.hasValue()) {
-    assert(AllowStatepointWithNoDeoptInfo &&
-           "Found non-leaf call without deopt info!");
-    return None;
-  }
-
-  return DeoptBundle.getValue().Inputs;
-}
-
-/// Compute the live-in set for every basic block in the function
-static void computeLiveInValues(DominatorTree &DT, Function &F,
-                                GCPtrLivenessData &Data);
-
-/// Given results from the dataflow liveness computation, find the set of live
-/// Values at a particular instruction.
-static void findLiveSetAtInst(Instruction *inst, GCPtrLivenessData &Data,
-                              StatepointLiveSetTy &out);
-
-// TODO: Once we can get to the GCStrategy, this becomes
-// Optional<bool> isGCManagedPointer(const Type *Ty) const override {
-
-static bool isGCPointerType(Type *T) {
-  if (auto *PT = dyn_cast<PointerType>(T))
-    // For the sake of this example GC, we arbitrarily pick addrspace(1) as our
-    // GC managed heap.  We know that a pointer into this heap needs to be
-    // updated and that no other pointer does.
-    return PT->getAddressSpace() == 1;
-  return false;
-}
-
-// Return true if this type is one which a) is a gc pointer or contains a GC
-// pointer and b) is of a type this code expects to encounter as a live value.
-// (The insertion code will assert that a type which matches (a) and not (b)
-// is not encountered.)
-static bool isHandledGCPointerType(Type *T) {
-  // We fully support gc pointers
-  if (isGCPointerType(T))
-    return true;
-  // We partially support vectors of gc pointers. The code will assert if it
-  // can't handle something.
-  if (auto VT = dyn_cast<VectorType>(T))
-    if (isGCPointerType(VT->getElementType()))
-      return true;
-  return false;
-}
-
-#ifndef NDEBUG
-/// Returns true if this type contains a gc pointer whether we know how to
-/// handle that type or not.
-static bool containsGCPtrType(Type *Ty) {
-  if (isGCPointerType(Ty))
-    return true;
-  if (VectorType *VT = dyn_cast<VectorType>(Ty))
-    return isGCPointerType(VT->getScalarType());
-  if (ArrayType *AT = dyn_cast<ArrayType>(Ty))
-    return containsGCPtrType(AT->getElementType());
-  if (StructType *ST = dyn_cast<StructType>(Ty))
-    return llvm::any_of(ST->subtypes(), containsGCPtrType);
-  return false;
-}
-
-// Returns true if this is a type which a) is a gc pointer or contains a GC
-// pointer and b) is of a type which the code doesn't expect (i.e. first class
-// aggregates).  Used to trip assertions.
-static bool isUnhandledGCPointerType(Type *Ty) {
-  return containsGCPtrType(Ty) && !isHandledGCPointerType(Ty);
-}
-#endif
-
-// Return the name of the value suffixed with the provided value, or if the
-// value didn't have a name, the default value specified.
-static std::string suffixed_name_or(Value *V, StringRef Suffix,
-                                    StringRef DefaultName) {
-  return V->hasName() ? (V->getName() + Suffix).str() : DefaultName.str();
-}
-
-// Conservatively identifies any definitions which might be live at the
-// given instruction. The  analysis is performed immediately before the
-// given instruction. Values defined by that instruction are not considered
-// live.  Values used by that instruction are considered live.
-static void
-analyzeParsePointLiveness(DominatorTree &DT,
-                          GCPtrLivenessData &OriginalLivenessData, CallSite CS,
-                          PartiallyConstructedSafepointRecord &Result) {
-  Instruction *Inst = CS.getInstruction();
-
-  StatepointLiveSetTy LiveSet;
-  findLiveSetAtInst(Inst, OriginalLivenessData, LiveSet);
-
-  if (PrintLiveSet) {
-    dbgs() << "Live Variables:\n";
-    for (Value *V : LiveSet)
-      dbgs() << " " << V->getName() << " " << *V << "\n";
-  }
-  if (PrintLiveSetSize) {
-    dbgs() << "Safepoint For: " << CS.getCalledValue()->getName() << "\n";
-    dbgs() << "Number live values: " << LiveSet.size() << "\n";
-  }
-  Result.LiveSet = LiveSet;
-}
-
-static bool isKnownBaseResult(Value *V);
-
-namespace {
-
-/// A single base defining value - An immediate base defining value for an
-/// instruction 'Def' is an input to 'Def' whose base is also a base of 'Def'.
-/// For instructions which have multiple pointer [vector] inputs or that
-/// transition between vector and scalar types, there is no immediate base
-/// defining value.  The 'base defining value' for 'Def' is the transitive
-/// closure of this relation stopping at the first instruction which has no
-/// immediate base defining value.  The b.d.v. might itself be a base pointer,
-/// but it can also be an arbitrary derived pointer. 
-struct BaseDefiningValueResult {
-  /// Contains the value which is the base defining value.
-  Value * const BDV;
-
-  /// True if the base defining value is also known to be an actual base
-  /// pointer.
-  const bool IsKnownBase;
-
-  BaseDefiningValueResult(Value *BDV, bool IsKnownBase)
-    : BDV(BDV), IsKnownBase(IsKnownBase) {
-#ifndef NDEBUG
-    // Check consistency between new and old means of checking whether a BDV is
-    // a base.
-    bool MustBeBase = isKnownBaseResult(BDV);
-    assert(!MustBeBase || MustBeBase == IsKnownBase);
-#endif
-  }
-};
-
-} // end anonymous namespace
-
-static BaseDefiningValueResult findBaseDefiningValue(Value *I);
-
-/// Return a base defining value for the 'Index' element of the given vector
-/// instruction 'I'.  If Index is null, returns a BDV for the entire vector
-/// 'I'.  As an optimization, this method will try to determine when the 
-/// element is known to already be a base pointer.  If this can be established,
-/// the second value in the returned pair will be true.  Note that either a
-/// vector or a pointer typed value can be returned.  For the former, the
-/// vector returned is a BDV (and possibly a base) of the entire vector 'I'.
-/// If the later, the return pointer is a BDV (or possibly a base) for the
-/// particular element in 'I'.  
-static BaseDefiningValueResult
-findBaseDefiningValueOfVector(Value *I) {
-  // Each case parallels findBaseDefiningValue below, see that code for
-  // detailed motivation.
-
-  if (isa<Argument>(I))
-    // An incoming argument to the function is a base pointer
-    return BaseDefiningValueResult(I, true);
-
-  if (isa<Constant>(I))
-    // Base of constant vector consists only of constant null pointers. 
-    // For reasoning see similar case inside 'findBaseDefiningValue' function.
-    return BaseDefiningValueResult(ConstantAggregateZero::get(I->getType()),
-                                   true);
-
-  if (isa<LoadInst>(I))
-    return BaseDefiningValueResult(I, true);
-
-  if (isa<InsertElementInst>(I))
-    // We don't know whether this vector contains entirely base pointers or
-    // not.  To be conservatively correct, we treat it as a BDV and will
-    // duplicate code as needed to construct a parallel vector of bases.
-    return BaseDefiningValueResult(I, false);
-
-  if (isa<ShuffleVectorInst>(I))
-    // We don't know whether this vector contains entirely base pointers or
-    // not.  To be conservatively correct, we treat it as a BDV and will
-    // duplicate code as needed to construct a parallel vector of bases.
-    // TODO: There a number of local optimizations which could be applied here
-    // for particular sufflevector patterns.
-    return BaseDefiningValueResult(I, false);
-
-  // The behavior of getelementptr instructions is the same for vector and
-  // non-vector data types.
-  if (auto *GEP = dyn_cast<GetElementPtrInst>(I))
-    return findBaseDefiningValue(GEP->getPointerOperand());
-
-  // If the pointer comes through a bitcast of a vector of pointers to
-  // a vector of another type of pointer, then look through the bitcast
-  if (auto *BC = dyn_cast<BitCastInst>(I))
-    return findBaseDefiningValue(BC->getOperand(0));
-
-  // A PHI or Select is a base defining value.  The outer findBasePointer
-  // algorithm is responsible for constructing a base value for this BDV.
-  assert((isa<SelectInst>(I) || isa<PHINode>(I)) &&
-         "unknown vector instruction - no base found for vector element");
-  return BaseDefiningValueResult(I, false);
-}
-
-/// Helper function for findBasePointer - Will return a value which either a)
-/// defines the base pointer for the input, b) blocks the simple search
-/// (i.e. a PHI or Select of two derived pointers), or c) involves a change
-/// from pointer to vector type or back.
-static BaseDefiningValueResult findBaseDefiningValue(Value *I) {
-  assert(I->getType()->isPtrOrPtrVectorTy() &&
-         "Illegal to ask for the base pointer of a non-pointer type");
-
-  if (I->getType()->isVectorTy())
-    return findBaseDefiningValueOfVector(I);
-
-  if (isa<Argument>(I))
-    // An incoming argument to the function is a base pointer
-    // We should have never reached here if this argument isn't an gc value
-    return BaseDefiningValueResult(I, true);
-
-  if (isa<Constant>(I)) {
-    // We assume that objects with a constant base (e.g. a global) can't move
-    // and don't need to be reported to the collector because they are always
-    // live. Besides global references, all kinds of constants (e.g. undef, 
-    // constant expressions, null pointers) can be introduced by the inliner or
-    // the optimizer, especially on dynamically dead paths.
-    // Here we treat all of them as having single null base. By doing this we
-    // trying to avoid problems reporting various conflicts in a form of 
-    // "phi (const1, const2)" or "phi (const, regular gc ptr)".
-    // See constant.ll file for relevant test cases.
-
-    return BaseDefiningValueResult(
-        ConstantPointerNull::get(cast<PointerType>(I->getType())), true);
-  }
-
-  if (CastInst *CI = dyn_cast<CastInst>(I)) {
-    Value *Def = CI->stripPointerCasts();
-    // If stripping pointer casts changes the address space there is an
-    // addrspacecast in between.
-    if (!(cast<PointerType>(Def->getType())->getAddressSpace() ==
-               cast<PointerType>(CI->getType())->getAddressSpace())) {
-      llvm::outs() << "bad addrspacecast\n"
-                   << "Def: " << *Def << "\n"
-                   << "CI : " << *CI  << "\n";
-    }
-    //assert(cast<PointerType>(Def->getType())->getAddressSpace() ==
-    //           cast<PointerType>(CI->getType())->getAddressSpace() &&
-    //       "unsupported addrspacecast");
-    // If we find a cast instruction here, it means we've found a cast which is
-    // not simply a pointer cast (i.e. an inttoptr).  We don't know how to
-    // handle int->ptr conversion.
-    if (isa<CastInst>(Def)) {
-      llvm::outs() << "saw another bad cast: " << *Def << "\n";
-    }
-    //assert(!isa<CastInst>(Def) && "shouldn't find another cast here");
-    return findBaseDefiningValue(Def);
-  }
-
-  if (isa<LoadInst>(I))
-    // The value loaded is an gc base itself
-    return BaseDefiningValueResult(I, true);
-
-  if (GetElementPtrInst *GEP = dyn_cast<GetElementPtrInst>(I))
-    // The base of this GEP is the base
-    return findBaseDefiningValue(GEP->getPointerOperand());
-
-  if (IntrinsicInst *II = dyn_cast<IntrinsicInst>(I)) {
-    switch (II->getIntrinsicID()) {
-    default:
-      // fall through to general call handling
-      break;
-    case Intrinsic::experimental_gc_statepoint:
-      llvm_unreachable("statepoints don't produce pointers");
-    case Intrinsic::experimental_gc_relocate:
-      // Rerunning safepoint insertion after safepoints are already
-      // inserted is not supported.  It could probably be made to work,
-      // but why are you doing this?  There's no good reason.
-      llvm_unreachable("repeat safepoint insertion is not supported");
-    case Intrinsic::gcroot:
-      // Currently, this mechanism hasn't been extended to work with gcroot.
-      // There's no reason it couldn't be, but I haven't thought about the
-      // implications much.
-      llvm_unreachable(
-          "interaction with the gcroot mechanism is not supported");
-    }
-  }
-  // We assume that functions in the source language only return base
-  // pointers.  This should probably be generalized via attributes to support
-  // both source language and internal functions.
-  if (isa<CallInst>(I) || isa<InvokeInst>(I))
-    return BaseDefiningValueResult(I, true);
-
-  // TODO: I have absolutely no idea how to implement this part yet.  It's not
-  // necessarily hard, I just haven't really looked at it yet.
-  assert(!isa<LandingPadInst>(I) && "Landing Pad is unimplemented");
-
-  if (isa<AtomicCmpXchgInst>(I))
-    // A CAS is effectively a atomic store and load combined under a
-    // predicate.  From the perspective of base pointers, we just treat it
-    // like a load.
-    return BaseDefiningValueResult(I, true);
-
-  assert(!isa<AtomicRMWInst>(I) && "Xchg handled above, all others are "
-                                   "binary ops which don't apply to pointers");
-
-  // The aggregate ops.  Aggregates can either be in the heap or on the
-  // stack, but in either case, this is simply a field load.  As a result,
-  // this is a defining definition of the base just like a load is.
-  if (isa<ExtractValueInst>(I))
-    return BaseDefiningValueResult(I, true);
-
-  // We should never see an insert vector since that would require we be
-  // tracing back a struct value not a pointer value.
-  assert(!isa<InsertValueInst>(I) &&
-         "Base pointer for a struct is meaningless");
-
-  // An extractelement produces a base result exactly when it's input does.
-  // We may need to insert a parallel instruction to extract the appropriate
-  // element out of the base vector corresponding to the input. Given this,
-  // it's analogous to the phi and select case even though it's not a merge.
-  if (isa<ExtractElementInst>(I))
-    // Note: There a lot of obvious peephole cases here.  This are deliberately
-    // handled after the main base pointer inference algorithm to make writing
-    // test cases to exercise that code easier.
-    return BaseDefiningValueResult(I, false);
-
-  // The last two cases here don't return a base pointer.  Instead, they
-  // return a value which dynamically selects from among several base
-  // derived pointers (each with it's own base potentially).  It's the job of
-  // the caller to resolve these.
-  assert((isa<SelectInst>(I) || isa<PHINode>(I)) &&
-         "missing instruction case in findBaseDefiningValing");
-  return BaseDefiningValueResult(I, false);
-}
-
-/// Returns the base defining value for this value.
-static Value *findBaseDefiningValueCached(Value *I, DefiningValueMapTy &Cache) {
-  Value *&Cached = Cache[I];
-  if (!Cached) {
-    Cached = findBaseDefiningValue(I).BDV;
-    DEBUG(dbgs() << "fBDV-cached: " << I->getName() << " -> "
-                 << Cached->getName() << "\n");
-  }
-  assert(Cache[I] != nullptr);
-  return Cached;
-}
-
-/// Return a base pointer for this value if known.  Otherwise, return it's
-/// base defining value.
-static Value *findBaseOrBDV(Value *I, DefiningValueMapTy &Cache) {
-  Value *Def = findBaseDefiningValueCached(I, Cache);
-  auto Found = Cache.find(Def);
-  if (Found != Cache.end()) {
-    // Either a base-of relation, or a self reference.  Caller must check.
-    return Found->second;
-  }
-  // Only a BDV available
-  return Def;
-}
-
-/// Given the result of a call to findBaseDefiningValue, or findBaseOrBDV,
-/// is it known to be a base pointer?  Or do we need to continue searching.
-static bool isKnownBaseResult(Value *V) {
-  if (!isa<PHINode>(V) && !isa<SelectInst>(V) &&
-      !isa<ExtractElementInst>(V) && !isa<InsertElementInst>(V) &&
-      !isa<ShuffleVectorInst>(V)) {
-    // no recursion possible
-    return true;
-  }
-  if (isa<Instruction>(V) &&
-      cast<Instruction>(V)->getMetadata("is_base_value")) {
-    // This is a previously inserted base phi or select.  We know
-    // that this is a base value.
-    return true;
-  }
-
-  // We need to keep searching
-  return false;
-}
-
-namespace {
-
-/// Models the state of a single base defining value in the findBasePointer
-/// algorithm for determining where a new instruction is needed to propagate
-/// the base of this BDV.
-class BDVState {
-public:
-  enum Status { Unknown, Base, Conflict };
-
-  BDVState() : BaseValue(nullptr) {}
-
-  explicit BDVState(Status Status, Value *BaseValue = nullptr)
-      : Status(Status), BaseValue(BaseValue) {
-    assert(Status != Base || BaseValue);
-  }
-
-  explicit BDVState(Value *BaseValue) : Status(Base), BaseValue(BaseValue) {}
-
-  Status getStatus() const { return Status; }
-  Value *getBaseValue() const { return BaseValue; }
-
-  bool isBase() const { return getStatus() == Base; }
-  bool isUnknown() const { return getStatus() == Unknown; }
-  bool isConflict() const { return getStatus() == Conflict; }
-
-  bool operator==(const BDVState &Other) const {
-    return BaseValue == Other.BaseValue && Status == Other.Status;
-  }
-
-  bool operator!=(const BDVState &other) const { return !(*this == other); }
-
-  LLVM_DUMP_METHOD
-  void dump() const {
-    print(dbgs());
-    dbgs() << '\n';
-  }
-
-  void print(raw_ostream &OS) const {
-    switch (getStatus()) {
-    case Unknown:
-      OS << "U";
-      break;
-    case Base:
-      OS << "B";
-      break;
-    case Conflict:
-      OS << "C";
-      break;
-    }
-    OS << " (" << getBaseValue() << " - "
-       << (getBaseValue() ? getBaseValue()->getName() : "nullptr") << "): ";
-  }
-
-private:
-  Status Status = Unknown;
-  AssertingVH<Value> BaseValue; // Non-null only if Status == Base.
-};
-
-} // end anonymous namespace
-
-#ifndef NDEBUG
-static raw_ostream &operator<<(raw_ostream &OS, const BDVState &State) {
-  State.print(OS);
-  return OS;
-}
-#endif
-
-static BDVState meetBDVStateImpl(const BDVState &LHS, const BDVState &RHS) {
-  switch (LHS.getStatus()) {
-  case BDVState::Unknown:
-    return RHS;
-
-  case BDVState::Base:
-    assert(LHS.getBaseValue() && "can't be null");
-    if (RHS.isUnknown())
-      return LHS;
-
-    if (RHS.isBase()) {
-      if (LHS.getBaseValue() == RHS.getBaseValue()) {
-        assert(LHS == RHS && "equality broken!");
-        return LHS;
-      }
-      return BDVState(BDVState::Conflict);
-    }
-    assert(RHS.isConflict() && "only three states!");
-    return BDVState(BDVState::Conflict);
-
-  case BDVState::Conflict:
-    return LHS;
-  }
-  llvm_unreachable("only three states!");
-}
-
-// Values of type BDVState form a lattice, and this function implements the meet
-// operation.
-static BDVState meetBDVState(const BDVState &LHS, const BDVState &RHS) {
-  BDVState Result = meetBDVStateImpl(LHS, RHS);
-  assert(Result == meetBDVStateImpl(RHS, LHS) &&
-         "Math is wrong: meet does not commute!");
-  return Result;
-}
-
-/// For a given value or instruction, figure out what base ptr its derived from.
-/// For gc objects, this is simply itself.  On success, returns a value which is
-/// the base pointer.  (This is reliable and can be used for relocation.)  On
-/// failure, returns nullptr.
-static Value *findBasePointer(Value *I, DefiningValueMapTy &Cache) {
-  Value *Def = findBaseOrBDV(I, Cache);
-
-  if (isKnownBaseResult(Def))
-    return Def;
-
-  // Here's the rough algorithm:
-  // - For every SSA value, construct a mapping to either an actual base
-  //   pointer or a PHI which obscures the base pointer.
-  // - Construct a mapping from PHI to unknown TOP state.  Use an
-  //   optimistic algorithm to propagate base pointer information.  Lattice
-  //   looks like:
-  //   UNKNOWN
-  //   b1 b2 b3 b4
-  //   CONFLICT
-  //   When algorithm terminates, all PHIs will either have a single concrete
-  //   base or be in a conflict state.
-  // - For every conflict, insert a dummy PHI node without arguments.  Add
-  //   these to the base[Instruction] = BasePtr mapping.  For every
-  //   non-conflict, add the actual base.
-  //  - For every conflict, add arguments for the base[a] of each input
-  //   arguments.
-  //
-  // Note: A simpler form of this would be to add the conflict form of all
-  // PHIs without running the optimistic algorithm.  This would be
-  // analogous to pessimistic data flow and would likely lead to an
-  // overall worse solution.
-
-#ifndef NDEBUG
-  auto isExpectedBDVType = [](Value *BDV) {
-    return isa<PHINode>(BDV) || isa<SelectInst>(BDV) ||
-           isa<ExtractElementInst>(BDV) || isa<InsertElementInst>(BDV) ||
-           isa<ShuffleVectorInst>(BDV);
-  };
-#endif
-
-  // Once populated, will contain a mapping from each potentially non-base BDV
-  // to a lattice value (described above) which corresponds to that BDV.
-  // We use the order of insertion (DFS over the def/use graph) to provide a
-  // stable deterministic ordering for visiting DenseMaps (which are unordered)
-  // below.  This is important for deterministic compilation.
-  MapVector<Value *, BDVState> States;
-
-  // Recursively fill in all base defining values reachable from the initial
-  // one for which we don't already know a definite base value for
-  /* scope */ {
-    SmallVector<Value*, 16> Worklist;
-    Worklist.push_back(Def);
-    States.insert({Def, BDVState()});
-    while (!Worklist.empty()) {
-      Value *Current = Worklist.pop_back_val();
-      assert(!isKnownBaseResult(Current) && "why did it get added?");
-
-      auto visitIncomingValue = [&](Value *InVal) {
-        Value *Base = findBaseOrBDV(InVal, Cache);
-        if (isKnownBaseResult(Base))
-          // Known bases won't need new instructions introduced and can be
-          // ignored safely
-          return;
-        assert(isExpectedBDVType(Base) && "the only non-base values "
-               "we see should be base defining values");
-        if (States.insert(std::make_pair(Base, BDVState())).second)
-          Worklist.push_back(Base);
-      };
-      if (PHINode *PN = dyn_cast<PHINode>(Current)) {
-        for (Value *InVal : PN->incoming_values())
-          visitIncomingValue(InVal);
-      } else if (SelectInst *SI = dyn_cast<SelectInst>(Current)) {
-        visitIncomingValue(SI->getTrueValue());
-        visitIncomingValue(SI->getFalseValue());
-      } else if (auto *EE = dyn_cast<ExtractElementInst>(Current)) {
-        visitIncomingValue(EE->getVectorOperand());
-      } else if (auto *IE = dyn_cast<InsertElementInst>(Current)) {
-        visitIncomingValue(IE->getOperand(0)); // vector operand
-        visitIncomingValue(IE->getOperand(1)); // scalar operand
-      } else if (auto *SV = dyn_cast<ShuffleVectorInst>(Current)) {
-        visitIncomingValue(SV->getOperand(0));
-        visitIncomingValue(SV->getOperand(1));
-      }
-      else {
-        llvm_unreachable("Unimplemented instruction case");
-      }
-    }
-  }
-
-#ifndef NDEBUG
-  DEBUG(dbgs() << "States after initialization:\n");
-  for (auto Pair : States) {
-    DEBUG(dbgs() << " " << Pair.second << " for " << *Pair.first << "\n");
-  }
-#endif
-
-  // Return a phi state for a base defining value.  We'll generate a new
-  // base state for known bases and expect to find a cached state otherwise.
-  auto getStateForBDV = [&](Value *baseValue) {
-    if (isKnownBaseResult(baseValue))
-      return BDVState(baseValue);
-    auto I = States.find(baseValue);
-    assert(I != States.end() && "lookup failed!");
-    return I->second;
-  };
-
-  bool Progress = true;
-  while (Progress) {
-#ifndef NDEBUG
-    const size_t OldSize = States.size();
-#endif
-    Progress = false;
-    // We're only changing values in this loop, thus safe to keep iterators.
-    // Since this is computing a fixed point, the order of visit does not
-    // effect the result.  TODO: We could use a worklist here and make this run
-    // much faster.
-    for (auto Pair : States) {
-      Value *BDV = Pair.first;
-      assert(!isKnownBaseResult(BDV) && "why did it get added?");
-
-      // Given an input value for the current instruction, return a BDVState
-      // instance which represents the BDV of that value.
-      auto getStateForInput = [&](Value *V) mutable {
-        Value *BDV = findBaseOrBDV(V, Cache);
-        return getStateForBDV(BDV);
-      };
-
-      BDVState NewState;
-      if (SelectInst *SI = dyn_cast<SelectInst>(BDV)) {
-        NewState = meetBDVState(NewState, getStateForInput(SI->getTrueValue()));
-        NewState =
-            meetBDVState(NewState, getStateForInput(SI->getFalseValue()));
-      } else if (PHINode *PN = dyn_cast<PHINode>(BDV)) {
-        for (Value *Val : PN->incoming_values())
-          NewState = meetBDVState(NewState, getStateForInput(Val));
-      } else if (auto *EE = dyn_cast<ExtractElementInst>(BDV)) {
-        // The 'meet' for an extractelement is slightly trivial, but it's still
-        // useful in that it drives us to conflict if our input is.
-        NewState =
-            meetBDVState(NewState, getStateForInput(EE->getVectorOperand()));
-      } else if (auto *IE = dyn_cast<InsertElementInst>(BDV)){
-        // Given there's a inherent type mismatch between the operands, will
-        // *always* produce Conflict.
-        NewState = meetBDVState(NewState, getStateForInput(IE->getOperand(0)));
-        NewState = meetBDVState(NewState, getStateForInput(IE->getOperand(1)));
-      } else {
-        // The only instance this does not return a Conflict is when both the
-        // vector operands are the same vector.
-        auto *SV = cast<ShuffleVectorInst>(BDV);
-        NewState = meetBDVState(NewState, getStateForInput(SV->getOperand(0)));
-        NewState = meetBDVState(NewState, getStateForInput(SV->getOperand(1)));
-      }
-
-      BDVState OldState = States[BDV];
-      if (OldState != NewState) {
-        Progress = true;
-        States[BDV] = NewState;
-      }
-    }
-
-    assert(OldSize == States.size() &&
-           "fixed point shouldn't be adding any new nodes to state");
-  }
-
-#ifndef NDEBUG
-  DEBUG(dbgs() << "States after meet iteration:\n");
-  for (auto Pair : States) {
-    DEBUG(dbgs() << " " << Pair.second << " for " << *Pair.first << "\n");
-  }
-#endif
-
-  // Insert Phis for all conflicts
-  // TODO: adjust naming patterns to avoid this order of iteration dependency
-  for (auto Pair : States) {
-    Instruction *I = cast<Instruction>(Pair.first);
-    BDVState State = Pair.second;
-    assert(!isKnownBaseResult(I) && "why did it get added?");
-    assert(!State.isUnknown() && "Optimistic algorithm didn't complete!");
-
-    // extractelement instructions are a bit special in that we may need to
-    // insert an extract even when we know an exact base for the instruction.
-    // The problem is that we need to convert from a vector base to a scalar
-    // base for the particular indice we're interested in.
-    if (State.isBase() && isa<ExtractElementInst>(I) &&
-        isa<VectorType>(State.getBaseValue()->getType())) {
-      auto *EE = cast<ExtractElementInst>(I);
-      // TODO: In many cases, the new instruction is just EE itself.  We should
-      // exploit this, but can't do it here since it would break the invariant
-      // about the BDV not being known to be a base.
-      auto *BaseInst = ExtractElementInst::Create(
-          State.getBaseValue(), EE->getIndexOperand(), "base_ee", EE);
-      BaseInst->setMetadata("is_base_value", MDNode::get(I->getContext(), {}));
-      States[I] = BDVState(BDVState::Base, BaseInst);
-    }
-
-    // Since we're joining a vector and scalar base, they can never be the
-    // same.  As a result, we should always see insert element having reached
-    // the conflict state.
-    assert(!isa<InsertElementInst>(I) || State.isConflict());
-
-    if (!State.isConflict())
-      continue;
-
-    /// Create and insert a new instruction which will represent the base of
-    /// the given instruction 'I'.
-    auto MakeBaseInstPlaceholder = [](Instruction *I) -> Instruction* {
-      if (isa<PHINode>(I)) {
-        BasicBlock *BB = I->getParent();
-        int NumPreds = std::distance(pred_begin(BB), pred_end(BB));
-        assert(NumPreds > 0 && "how did we reach here");
-        std::string Name = suffixed_name_or(I, ".base", "base_phi");
-        return PHINode::Create(I->getType(), NumPreds, Name, I);
-      } else if (SelectInst *SI = dyn_cast<SelectInst>(I)) {
-        // The undef will be replaced later
-        UndefValue *Undef = UndefValue::get(SI->getType());
-        std::string Name = suffixed_name_or(I, ".base", "base_select");
-        return SelectInst::Create(SI->getCondition(), Undef, Undef, Name, SI);
-      } else if (auto *EE = dyn_cast<ExtractElementInst>(I)) {
-        UndefValue *Undef = UndefValue::get(EE->getVectorOperand()->getType());
-        std::string Name = suffixed_name_or(I, ".base", "base_ee");
-        return ExtractElementInst::Create(Undef, EE->getIndexOperand(), Name,
-                                          EE);
-      } else if (auto *IE = dyn_cast<InsertElementInst>(I)) {
-        UndefValue *VecUndef = UndefValue::get(IE->getOperand(0)->getType());
-        UndefValue *ScalarUndef = UndefValue::get(IE->getOperand(1)->getType());
-        std::string Name = suffixed_name_or(I, ".base", "base_ie");
-        return InsertElementInst::Create(VecUndef, ScalarUndef,
-                                         IE->getOperand(2), Name, IE);
-      } else {
-        auto *SV = cast<ShuffleVectorInst>(I);
-        UndefValue *VecUndef = UndefValue::get(SV->getOperand(0)->getType());
-        std::string Name = suffixed_name_or(I, ".base", "base_sv");
-        return new ShuffleVectorInst(VecUndef, VecUndef, SV->getOperand(2),
-                                     Name, SV);
-      }
-    };
-    Instruction *BaseInst = MakeBaseInstPlaceholder(I);
-    // Add metadata marking this as a base value
-    BaseInst->setMetadata("is_base_value", MDNode::get(I->getContext(), {}));
-    States[I] = BDVState(BDVState::Conflict, BaseInst);
-  }
-
-  // Returns a instruction which produces the base pointer for a given
-  // instruction.  The instruction is assumed to be an input to one of the BDVs
-  // seen in the inference algorithm above.  As such, we must either already
-  // know it's base defining value is a base, or have inserted a new
-  // instruction to propagate the base of it's BDV and have entered that newly
-  // introduced instruction into the state table.  In either case, we are
-  // assured to be able to determine an instruction which produces it's base
-  // pointer.
-  auto getBaseForInput = [&](Value *Input, Instruction *InsertPt) {
-    Value *BDV = findBaseOrBDV(Input, Cache);
-    Value *Base = nullptr;
-    if (isKnownBaseResult(BDV)) {
-      Base = BDV;
-    } else {
-      // Either conflict or base.
-      assert(States.count(BDV));
-      Base = States[BDV].getBaseValue();
-    }
-    assert(Base && "Can't be null");
-    // The cast is needed since base traversal may strip away bitcasts
-    if (Base->getType() != Input->getType() && InsertPt) {
-      /*
-      if (!(cast<PointerType>(Base->getType())->getAddressSpace() ==
-            cast<PointerType>(Input->getType())->getAddressSpace())) {
-        Base = new AddressCastInst(Base, );
-      }
-      */
-      //Base = new BitCastInst(Base, Input->getType(), "cast", InsertPt);
-      Base = CastInst::CreatePointerBitCastOrAddrSpaceCast(
-                               Base, Input->getType(), "cast", InsertPt);
-    }
-      
-    return Base;
-  };
-
-  // Fixup all the inputs of the new PHIs.  Visit order needs to be
-  // deterministic and predictable because we're naming newly created
-  // instructions.
-  for (auto Pair : States) {
-    Instruction *BDV = cast<Instruction>(Pair.first);
-    BDVState State = Pair.second;
-
-    assert(!isKnownBaseResult(BDV) && "why did it get added?");
-    assert(!State.isUnknown() && "Optimistic algorithm didn't complete!");
-    if (!State.isConflict())
-      continue;
-
-    if (PHINode *BasePHI = dyn_cast<PHINode>(State.getBaseValue())) {
-      PHINode *PN = cast<PHINode>(BDV);
-      unsigned NumPHIValues = PN->getNumIncomingValues();
-      for (unsigned i = 0; i < NumPHIValues; i++) {
-        Value *InVal = PN->getIncomingValue(i);
-        BasicBlock *InBB = PN->getIncomingBlock(i);
-
-        // If we've already seen InBB, add the same incoming value
-        // we added for it earlier.  The IR verifier requires phi
-        // nodes with multiple entries from the same basic block
-        // to have the same incoming value for each of those
-        // entries.  If we don't do this check here and basephi
-        // has a different type than base, we'll end up adding two
-        // bitcasts (and hence two distinct values) as incoming
-        // values for the same basic block.
-
-        int BlockIndex = BasePHI->getBasicBlockIndex(InBB);
-        if (BlockIndex != -1) {
-          Value *OldBase = BasePHI->getIncomingValue(BlockIndex);
-          BasePHI->addIncoming(OldBase, InBB);
-
-#ifndef NDEBUG
-          Value *Base = getBaseForInput(InVal, nullptr);
-          // In essence this assert states: the only way two values
-          // incoming from the same basic block may be different is by
-          // being different bitcasts of the same value.  A cleanup
-          // that remains TODO is changing findBaseOrBDV to return an
-          // llvm::Value of the correct type (and still remain pure).
-          // This will remove the need to add bitcasts.
-          assert(Base->stripPointerCasts() == OldBase->stripPointerCasts() &&
-                 "Sanity -- findBaseOrBDV should be pure!");
-#endif
-          continue;
-        }
-
-        // Find the instruction which produces the base for each input.  We may
-        // need to insert a bitcast in the incoming block.
-        // TODO: Need to split critical edges if insertion is needed
-        Value *Base = getBaseForInput(InVal, InBB->getTerminator());
-        BasePHI->addIncoming(Base, InBB);
-      }
-      assert(BasePHI->getNumIncomingValues() == NumPHIValues);
-    } else if (SelectInst *BaseSI =
-                   dyn_cast<SelectInst>(State.getBaseValue())) {
-      SelectInst *SI = cast<SelectInst>(BDV);
-
-      // Find the instruction which produces the base for each input.
-      // We may need to insert a bitcast.
-      BaseSI->setTrueValue(getBaseForInput(SI->getTrueValue(), BaseSI));
-      BaseSI->setFalseValue(getBaseForInput(SI->getFalseValue(), BaseSI));
-    } else if (auto *BaseEE =
-                   dyn_cast<ExtractElementInst>(State.getBaseValue())) {
-      Value *InVal = cast<ExtractElementInst>(BDV)->getVectorOperand();
-      // Find the instruction which produces the base for each input.  We may
-      // need to insert a bitcast.
-      BaseEE->setOperand(0, getBaseForInput(InVal, BaseEE));
-    } else if (auto *BaseIE = dyn_cast<InsertElementInst>(State.getBaseValue())){
-      auto *BdvIE = cast<InsertElementInst>(BDV);
-      auto UpdateOperand = [&](int OperandIdx) {
-        Value *InVal = BdvIE->getOperand(OperandIdx);
-        Value *Base = getBaseForInput(InVal, BaseIE);
-        BaseIE->setOperand(OperandIdx, Base);
-      };
-      UpdateOperand(0); // vector operand
-      UpdateOperand(1); // scalar operand
-    } else {
-      auto *BaseSV = cast<ShuffleVectorInst>(State.getBaseValue());
-      auto *BdvSV = cast<ShuffleVectorInst>(BDV);
-      auto UpdateOperand = [&](int OperandIdx) {
-        Value *InVal = BdvSV->getOperand(OperandIdx);
-        Value *Base = getBaseForInput(InVal, BaseSV);
-        BaseSV->setOperand(OperandIdx, Base);
-      };
-      UpdateOperand(0); // vector operand
-      UpdateOperand(1); // vector operand
-    }
-  }
-
-  // Cache all of our results so we can cheaply reuse them
-  // NOTE: This is actually two caches: one of the base defining value
-  // relation and one of the base pointer relation!  FIXME
-  for (auto Pair : States) {
-    auto *BDV = Pair.first;
-    Value *Base = Pair.second.getBaseValue();
-    assert(BDV && Base);
-    assert(!isKnownBaseResult(BDV) && "why did it get added?");
-
-    DEBUG(dbgs() << "Updating base value cache"
-                 << " for: " << BDV->getName() << " from: "
-                 << (Cache.count(BDV) ? Cache[BDV]->getName().str() : "none")
-                 << " to: " << Base->getName() << "\n");
-
-    if (Cache.count(BDV)) {
-      assert(isKnownBaseResult(Base) &&
-             "must be something we 'know' is a base pointer");
-      // Once we transition from the BDV relation being store in the Cache to
-      // the base relation being stored, it must be stable
-      assert((!isKnownBaseResult(Cache[BDV]) || Cache[BDV] == Base) &&
-             "base relation should be stable");
-    }
-    Cache[BDV] = Base;
-  }
-  assert(Cache.count(Def));
-  return Cache[Def];
-}
-
-// For a set of live pointers (base and/or derived), identify the base
-// pointer of the object which they are derived from.  This routine will
-// mutate the IR graph as needed to make the 'base' pointer live at the
-// definition site of 'derived'.  This ensures that any use of 'derived' can
-// also use 'base'.  This may involve the insertion of a number of
-// additional PHI nodes.
-//
-// preconditions: live is a set of pointer type Values
-//
-// side effects: may insert PHI nodes into the existing CFG, will preserve
-// CFG, will not remove or mutate any existing nodes
-//
-// post condition: PointerToBase contains one (derived, base) pair for every
-// pointer in live.  Note that derived can be equal to base if the original
-// pointer was a base pointer.
-static void
-findBasePointers(const StatepointLiveSetTy &live,
-                 MapVector<Value *, Value *> &PointerToBase,
-                 DominatorTree *DT, DefiningValueMapTy &DVCache) {
-  for (Value *ptr : live) {
-    Value *base = findBasePointer(ptr, DVCache);
-    assert(base && "failed to find base pointer");
-    PointerToBase[ptr] = base;
-    assert((!isa<Instruction>(base) || !isa<Instruction>(ptr) ||
-            DT->dominates(cast<Instruction>(base)->getParent(),
-                          cast<Instruction>(ptr)->getParent())) &&
-           "The base we found better dominate the derived pointer");
-  }
-}
-
-/// Find the required based pointers (and adjust the live set) for the given
-/// parse point.
-static void findBasePointers(DominatorTree &DT, DefiningValueMapTy &DVCache,
-                             CallSite CS,
-                             PartiallyConstructedSafepointRecord &result) {
-  MapVector<Value *, Value *> PointerToBase;
-  findBasePointers(result.LiveSet, PointerToBase, &DT, DVCache);
-
-  if (PrintBasePointers) {
-    errs() << "Base Pairs (w/o Relocation):\n";
-    for (auto &Pair : PointerToBase) {
-      errs() << " derived ";
-      Pair.first->printAsOperand(errs(), false);
-      errs() << " base ";
-      Pair.second->printAsOperand(errs(), false);
-      errs() << "\n";;
-    }
-  }
-
-  result.PointerToBase = PointerToBase;
-}
-
-/// Given an updated version of the dataflow liveness results, update the
-/// liveset and base pointer maps for the call site CS.
-static void recomputeLiveInValues(GCPtrLivenessData &RevisedLivenessData,
-                                  CallSite CS,
-                                  PartiallyConstructedSafepointRecord &result);
-
-static void recomputeLiveInValues(
-    Function &F, DominatorTree &DT, ArrayRef<CallSite> toUpdate,
-    MutableArrayRef<struct PartiallyConstructedSafepointRecord> records) {
-  // TODO-PERF: reuse the original liveness, then simply run the dataflow
-  // again.  The old values are still live and will help it stabilize quickly.
-  GCPtrLivenessData RevisedLivenessData;
-  computeLiveInValues(DT, F, RevisedLivenessData);
-  for (size_t i = 0; i < records.size(); i++) {
-    struct PartiallyConstructedSafepointRecord &info = records[i];
-    recomputeLiveInValues(RevisedLivenessData, toUpdate[i], info);
-  }
-}
-
-// When inserting gc.relocate and gc.result calls, we need to ensure there are
-// no uses of the original value / return value between the gc.statepoint and
-// the gc.relocate / gc.result call.  One case which can arise is a phi node
-// starting one of the successor blocks.  We also need to be able to insert the
-// gc.relocates only on the path which goes through the statepoint.  We might
-// need to split an edge to make this possible.
-static BasicBlock *
-normalizeForInvokeSafepoint(BasicBlock *BB, BasicBlock *InvokeParent,
-                            DominatorTree &DT) {
-  BasicBlock *Ret = BB;
-  if (!BB->getUniquePredecessor())
-    Ret = SplitBlockPredecessors(BB, InvokeParent, "", &DT);
-
-  // Now that 'Ret' has unique predecessor we can safely remove all phi nodes
-  // from it
-  FoldSingleEntryPHINodes(Ret);
-  assert(!isa<PHINode>(Ret->begin()) &&
-         "All PHI nodes should have been removed!");
-
-  // At this point, we can safely insert a gc.relocate or gc.result as the first
-  // instruction in Ret if needed.
-  return Ret;
-}
-
-// Create new attribute set containing only attributes which can be transferred
-// from original call to the safepoint.
-static AttributeList legalizeCallAttributes(AttributeList AL) {
-  if (AL.isEmpty())
-    return AL;
-
-  // Remove the readonly, readnone, and statepoint function attributes.
-  AttrBuilder FnAttrs = AL.getFnAttributes();
-  FnAttrs.removeAttribute(Attribute::ReadNone);
-  FnAttrs.removeAttribute(Attribute::ReadOnly);
-  for (Attribute A : AL.getFnAttributes()) {
-    if (isStatepointDirectiveAttr(A))
-      FnAttrs.remove(A);
-  }
-
-  // Just skip parameter and return attributes for now
-  LLVMContext &Ctx = AL.getContext();
-  return AttributeList::get(Ctx, AttributeList::FunctionIndex,
-                            AttributeSet::get(Ctx, FnAttrs));
-}
-
-/// Helper function to place all gc relocates necessary for the given
-/// statepoint.
-/// Inputs:
-///   liveVariables - list of variables to be relocated.
-///   liveStart - index of the first live variable.
-///   basePtrs - base pointers.
-///   statepointToken - statepoint instruction to which relocates should be
-///   bound.
-///   Builder - Llvm IR builder to be used to construct new calls.
-static void CreateGCRelocates(ArrayRef<Value *> LiveVariables,
-                              const int LiveStart,
-                              ArrayRef<Value *> BasePtrs,
-                              Instruction *StatepointToken,
-                              IRBuilder<> Builder) {
-  if (LiveVariables.empty())
-    return;
-
-  auto FindIndex = [](ArrayRef<Value *> LiveVec, Value *Val) {
-    auto ValIt = llvm::find(LiveVec, Val);
-    assert(ValIt != LiveVec.end() && "Val not found in LiveVec!");
-    size_t Index = std::distance(LiveVec.begin(), ValIt);
-    assert(Index < LiveVec.size() && "Bug in std::find?");
-    return Index;
-  };
-  Module *M = StatepointToken->getModule();
-  
-  // All gc_relocate are generated as i8 addrspace(1)* (or a vector type whose
-  // element type is i8 addrspace(1)*). We originally generated unique
-  // declarations for each pointer type, but this proved problematic because
-  // the intrinsic mangling code is incomplete and fragile.  Since we're moving
-  // towards a single unified pointer type anyways, we can just cast everything
-  // to an i8* of the right address space.  A bitcast is added later to convert
-  // gc_relocate to the actual value's type.  
-  auto getGCRelocateDecl = [&] (Type *Ty) {
-    assert(isHandledGCPointerType(Ty));
-    auto AS = Ty->getScalarType()->getPointerAddressSpace();
-    Type *NewTy = Type::getInt8PtrTy(M->getContext(), AS);
-    if (auto *VT = dyn_cast<VectorType>(Ty))
-      NewTy = VectorType::get(NewTy, VT->getNumElements());
-    return Intrinsic::getDeclaration(M, Intrinsic::experimental_gc_relocate,
-                                     {NewTy});
-  };
-
-  // Lazily populated map from input types to the canonicalized form mentioned
-  // in the comment above.  This should probably be cached somewhere more
-  // broadly.
-  DenseMap<Type*, Value*> TypeToDeclMap;
-
-  for (unsigned i = 0; i < LiveVariables.size(); i++) {
-    // Generate the gc.relocate call and save the result
-    //Value *BaseIdx =
-    //  Builder.getInt32(LiveStart + FindIndex(LiveVariables, BasePtrs[i]));
-    Value* BaseIdx = Builder.getInt32(LiveStart + i);
-    Value *LiveIdx = Builder.getInt32(LiveStart + i);
-
-    Type *Ty = LiveVariables[i]->getType();
-    if (!TypeToDeclMap.count(Ty))
-      TypeToDeclMap[Ty] = getGCRelocateDecl(Ty);
-    Value *GCRelocateDecl = TypeToDeclMap[Ty];
-
-    // only specify a debug name if we can give a useful one
-    CallInst *Reloc = Builder.CreateCall(
-        GCRelocateDecl, {StatepointToken, BaseIdx, LiveIdx},
-        suffixed_name_or(LiveVariables[i], ".relocated", ""));
-    // Trick CodeGen into thinking there are lots of free registers at this
-    // fake call.
-    Reloc->setCallingConv(CallingConv::Cold);
-  }
-}
-
-namespace {
-
-/// This struct is used to defer RAUWs and `eraseFromParent` s.  Using this
-/// avoids having to worry about keeping around dangling pointers to Values.
-class DeferredReplacement {
-  AssertingVH<Instruction> Old;
-  AssertingVH<Instruction> New;
-  bool IsDeoptimize = false;
-
-  DeferredReplacement() = default;
-
-public:
-  static DeferredReplacement createRAUW(Instruction *Old, Instruction *New) {
-    assert(Old != New && Old && New &&
-           "Cannot RAUW equal values or to / from null!");
-
-    DeferredReplacement D;
-    D.Old = Old;
-    D.New = New;
-    return D;
-  }
-
-  static DeferredReplacement createDelete(Instruction *ToErase) {
-    DeferredReplacement D;
-    D.Old = ToErase;
-    return D;
-  }
-
-  static DeferredReplacement createDeoptimizeReplacement(Instruction *Old) {
-#ifndef NDEBUG
-    auto *F = cast<CallInst>(Old)->getCalledFunction();
-    assert(F && F->getIntrinsicID() == Intrinsic::experimental_deoptimize &&
-           "Only way to construct a deoptimize deferred replacement");
-#endif
-    DeferredReplacement D;
-    D.Old = Old;
-    D.IsDeoptimize = true;
-    return D;
-  }
-
-  /// Does the task represented by this instance.
-  void doReplacement() {
-    Instruction *OldI = Old;
-    Instruction *NewI = New;
-
-    assert(OldI != NewI && "Disallowed at construction?!");
-    assert((!IsDeoptimize || !New) &&
-           "Deoptimize instrinsics are not replaced!");
-
-    Old = nullptr;
-    New = nullptr;
-
-    if (NewI)
-      OldI->replaceAllUsesWith(NewI);
-
-    if (IsDeoptimize) {
-      // Note: we've inserted instructions, so the call to llvm.deoptimize may
-      // not necessarilly be followed by the matching return.
-      auto *RI = cast<ReturnInst>(OldI->getParent()->getTerminator());
-      new UnreachableInst(RI->getContext(), RI);
-      RI->eraseFromParent();
-    }
-
-    OldI->eraseFromParent();
-  }
-};
-
-} // end anonymous namespace
-
-static StringRef getDeoptLowering(CallSite CS) {
-  const char *DeoptLowering = "deopt-lowering";
-  if (CS.hasFnAttr(DeoptLowering)) {
-    // FIXME: CallSite has a *really* confusing interface around attributes
-    // with values.
-    const AttributeList &CSAS = CS.getAttributes();
-    if (CSAS.hasAttribute(AttributeList::FunctionIndex, DeoptLowering))
-      return CSAS.getAttribute(AttributeList::FunctionIndex, DeoptLowering)
-          .getValueAsString();
-    Function *F = CS.getCalledFunction();
-    assert(F && F->hasFnAttribute(DeoptLowering));
-    return F->getFnAttribute(DeoptLowering).getValueAsString();
-  }
-  return "live-through";
-}
-    
-static void
-makeStatepointExplicitImpl(const CallSite CS, /* to replace */
-                           const SmallVectorImpl<Value *> &BasePtrs,
-                           const SmallVectorImpl<Value *> &LiveVariables,
-                           PartiallyConstructedSafepointRecord &Result,
-                           std::vector<DeferredReplacement> &Replacements) {
-  assert(BasePtrs.size() == LiveVariables.size());
-
-  // Then go ahead and use the builder do actually do the inserts.  We insert
-  // immediately before the previous instruction under the assumption that all
-  // arguments will be available here.  We can't insert afterwards since we may
-  // be replacing a terminator.
-  Instruction *InsertBefore = CS.getInstruction();
-  IRBuilder<> Builder(InsertBefore);
-
-  ArrayRef<Value *> GCArgs(LiveVariables);
-  uint64_t StatepointID = StatepointDirectives::DefaultStatepointID;
-  uint32_t NumPatchBytes = 0;
-  uint32_t Flags = uint32_t(StatepointFlags::None);
-
-  ArrayRef<Use> CallArgs(CS.arg_begin(), CS.arg_end());
-  ArrayRef<Use> DeoptArgs = GetDeoptBundleOperands(CS);
-  ArrayRef<Use> TransitionArgs;
-  if (auto TransitionBundle =
-      CS.getOperandBundle(LLVMContext::OB_gc_transition)) {
-    Flags |= uint32_t(StatepointFlags::GCTransition);
-    TransitionArgs = TransitionBundle->Inputs;
-  }
-
-  // Instead of lowering calls to @llvm.experimental.deoptimize as normal calls
-  // with a return value, we lower then as never returning calls to
-  // __llvm_deoptimize that are followed by unreachable to get better codegen.
-  bool IsDeoptimize = false;
-
-  StatepointDirectives SD =
-      parseStatepointDirectivesFromAttrs(CS.getAttributes());
-  if (SD.NumPatchBytes)
-    NumPatchBytes = *SD.NumPatchBytes;
-  if (SD.StatepointID)
-    StatepointID = *SD.StatepointID;
-
-  // Pass through the requested lowering if any.  The default is live-through.
-  StringRef DeoptLowering = getDeoptLowering(CS);
-  if (DeoptLowering.equals("live-in"))
-    Flags |= uint32_t(StatepointFlags::DeoptLiveIn);
-  else {
-    assert(DeoptLowering.equals("live-through") && "Unsupported value!");
-  }
-
-  Value *CallTarget = CS.getCalledValue();
-  if (Function *F = dyn_cast<Function>(CallTarget)) {
-    if (F->getIntrinsicID() == Intrinsic::experimental_deoptimize) {
-      // Calls to llvm.experimental.deoptimize are lowered to calls to the
-      // __llvm_deoptimize symbol.  We want to resolve this now, since the
-      // verifier does not allow taking the address of an intrinsic function.
-
-      SmallVector<Type *, 8> DomainTy;
-      for (Value *Arg : CallArgs)
-        DomainTy.push_back(Arg->getType());
-      auto *FTy = FunctionType::get(Type::getVoidTy(F->getContext()), DomainTy,
-                                    /* isVarArg = */ false);
-
-      // Note: CallTarget can be a bitcast instruction of a symbol if there are
-      // calls to @llvm.experimental.deoptimize with different argument types in
-      // the same module.  This is fine -- we assume the frontend knew what it
-      // was doing when generating this kind of IR.
-      CallTarget =
-          F->getParent()->getOrInsertFunction("__llvm_deoptimize", FTy);
-
-      IsDeoptimize = true;
-    }
-  }
-
-  // Create the statepoint given all the arguments
-  Instruction *Token = nullptr;
-  if (CS.isCall()) {
-    CallInst *ToReplace = cast<CallInst>(CS.getInstruction());
-    CallInst *Call = Builder.CreateGCStatepointCall(
-        StatepointID, NumPatchBytes, CallTarget, Flags, CallArgs,
-        TransitionArgs, DeoptArgs, GCArgs, "safepoint_token");
-
-    Call->setTailCallKind(ToReplace->getTailCallKind());
-    Call->setCallingConv(ToReplace->getCallingConv());
-
-    // Currently we will fail on parameter attributes and on certain
-    // function attributes.  In case if we can handle this set of attributes -
-    // set up function attrs directly on statepoint and return attrs later for
-    // gc_result intrinsic.
-    Call->setAttributes(legalizeCallAttributes(ToReplace->getAttributes()));
-
-    Token = Call;
-
-    // Put the following gc_result and gc_relocate calls immediately after the
-    // the old call (which we're about to delete)
-    assert(ToReplace->getNextNode() && "Not a terminator, must have next!");
-    Builder.SetInsertPoint(ToReplace->getNextNode());
-    Builder.SetCurrentDebugLocation(ToReplace->getNextNode()->getDebugLoc());
-  } else {
-    InvokeInst *ToReplace = cast<InvokeInst>(CS.getInstruction());
-
-    // Insert the new invoke into the old block.  We'll remove the old one in a
-    // moment at which point this will become the new terminator for the
-    // original block.
-    InvokeInst *Invoke = Builder.CreateGCStatepointInvoke(
-        StatepointID, NumPatchBytes, CallTarget, ToReplace->getNormalDest(),
-        ToReplace->getUnwindDest(), Flags, CallArgs, TransitionArgs, DeoptArgs,
-        GCArgs, "statepoint_token");
-
-    Invoke->setCallingConv(ToReplace->getCallingConv());
-
-    // Currently we will fail on parameter attributes and on certain
-    // function attributes.  In case if we can handle this set of attributes -
-    // set up function attrs directly on statepoint and return attrs later for
-    // gc_result intrinsic.
-    Invoke->setAttributes(legalizeCallAttributes(ToReplace->getAttributes()));
-
-    Token = Invoke;
-
-    // Generate gc relocates in exceptional path
-    BasicBlock *UnwindBlock = ToReplace->getUnwindDest();
-    assert(!isa<PHINode>(UnwindBlock->begin()) &&
-           UnwindBlock->getUniquePredecessor() &&
-           "can't safely insert in this block!");
-
-    Builder.SetInsertPoint(&*UnwindBlock->getFirstInsertionPt());
-    Builder.SetCurrentDebugLocation(ToReplace->getDebugLoc());
-
-    // Attach exceptional gc relocates to the landingpad.
-    Instruction *ExceptionalToken = UnwindBlock->getLandingPadInst();
-    Result.UnwindToken = ExceptionalToken;
-
-    const unsigned LiveStartIdx = Statepoint(Token).gcArgsStartIdx();
-    CreateGCRelocates(LiveVariables, LiveStartIdx, BasePtrs, ExceptionalToken,
-                      Builder);
-
-    // Generate gc relocates and returns for normal block
-    BasicBlock *NormalDest = ToReplace->getNormalDest();
-    assert(!isa<PHINode>(NormalDest->begin()) &&
-           NormalDest->getUniquePredecessor() &&
-           "can't safely insert in this block!");
-
-    Builder.SetInsertPoint(&*NormalDest->getFirstInsertionPt());
-
-    // gc relocates will be generated later as if it were regular call
-    // statepoint
-  }
-  assert(Token && "Should be set in one of the above branches!");
-
-  if (IsDeoptimize) {
-    // If we're wrapping an @llvm.experimental.deoptimize in a statepoint, we
-    // transform the tail-call like structure to a call to a void function
-    // followed by unreachable to get better codegen.
-    Replacements.push_back(
-        DeferredReplacement::createDeoptimizeReplacement(CS.getInstruction()));
-  } else {
-    Token->setName("statepoint_token");
-    if (!CS.getType()->isVoidTy() && !CS.getInstruction()->use_empty()) {
-      StringRef Name =
-          CS.getInstruction()->hasName() ? CS.getInstruction()->getName() : "";
-      CallInst *GCResult = Builder.CreateGCResult(Token, CS.getType(), Name);
-      GCResult->setAttributes(
-          AttributeList::get(GCResult->getContext(), AttributeList::ReturnIndex,
-                             CS.getAttributes().getRetAttributes()));
-
-      // We cannot RAUW or delete CS.getInstruction() because it could be in the
-      // live set of some other safepoint, in which case that safepoint's
-      // PartiallyConstructedSafepointRecord will hold a raw pointer to this
-      // llvm::Instruction.  Instead, we defer the replacement and deletion to
-      // after the live sets have been made explicit in the IR, and we no longer
-      // have raw pointers to worry about.
-      Replacements.emplace_back(
-          DeferredReplacement::createRAUW(CS.getInstruction(), GCResult));
-    } else {
-      Replacements.emplace_back(
-          DeferredReplacement::createDelete(CS.getInstruction()));
-    }
-  }
-
-  Result.StatepointToken = Token;
-
-  // Second, create a gc.relocate for every live variable
-  const unsigned LiveStartIdx = Statepoint(Token).gcArgsStartIdx();
-  CreateGCRelocates(LiveVariables, LiveStartIdx, BasePtrs, Token, Builder);
-}
-
-// Replace an existing gc.statepoint with a new one and a set of gc.relocates
-// which make the relocations happening at this safepoint explicit.
-//
-// WARNING: Does not do any fixup to adjust users of the original live
-// values.  That's the callers responsibility.
-static void
-makeStatepointExplicit(DominatorTree &DT, CallSite CS,
-                       PartiallyConstructedSafepointRecord &Result,
-                       std::vector<DeferredReplacement> &Replacements) {
-  const auto &LiveSet = Result.LiveSet;
-  const auto &PointerToBase = Result.PointerToBase;
-
-  // Convert to vector for efficient cross referencing.
-  SmallVector<Value *, 64> BaseVec, LiveVec;
-  LiveVec.reserve(LiveSet.size());
-  BaseVec.reserve(LiveSet.size());
-  for (Value *L : LiveSet) {
-    LiveVec.push_back(L);
-    assert(PointerToBase.count(L));
-    Value *Base = PointerToBase.find(L)->second;
-    BaseVec.push_back(Base);
-  }
-  assert(LiveVec.size() == BaseVec.size());
-
-  // Do the actual rewriting and delete the old statepoint
-  makeStatepointExplicitImpl(CS, BaseVec, LiveVec, Result, Replacements);
-}
-
-// Helper function for the relocationViaAlloca.
-//
-// It receives iterator to the statepoint gc relocates and emits a store to the
-// assigned location (via allocaMap) for the each one of them.  It adds the
-// visited values into the visitedLiveValues set, which we will later use them
-// for sanity checking.
-static void
-insertRelocationStores(iterator_range<Value::user_iterator> GCRelocs,
-                       DenseMap<Value *, Value *> &AllocaMap,
-                       DenseSet<Value *> &VisitedLiveValues) {
-  for (User *U : GCRelocs) {
-    GCRelocateInst *Relocate = dyn_cast<GCRelocateInst>(U);
-    if (!Relocate)
-      continue;
-
-    Value *OriginalValue = Relocate->getDerivedPtr();
-    assert(AllocaMap.count(OriginalValue));
-    Value *Alloca = AllocaMap[OriginalValue];
-
-    // Emit store into the related alloca
-    // All gc_relocates are i8 addrspace(1)* typed, and it must be bitcasted to
-    // the correct type according to alloca.
-    assert(Relocate->getNextNode() &&
-           "Should always have one since it's not a terminator");
-    IRBuilder<> Builder(Relocate->getNextNode());
-    Value *CastedRelocatedValue =
-      Builder.CreateBitCast(Relocate,
-                            cast<AllocaInst>(Alloca)->getAllocatedType(),
-                            suffixed_name_or(Relocate, ".casted", ""));
-
-    StoreInst *Store = new StoreInst(CastedRelocatedValue, Alloca);
-    Store->insertAfter(cast<Instruction>(CastedRelocatedValue));
-
-#ifndef NDEBUG
-    VisitedLiveValues.insert(OriginalValue);
-#endif
-  }
-}
-
-// Helper function for the "relocationViaAlloca". Similar to the
-// "insertRelocationStores" but works for rematerialized values.
-static void insertRematerializationStores(
-    const RematerializedValueMapTy &RematerializedValues,
-    DenseMap<Value *, Value *> &AllocaMap,
-    DenseSet<Value *> &VisitedLiveValues) {
-  for (auto RematerializedValuePair: RematerializedValues) {
-    Instruction *RematerializedValue = RematerializedValuePair.first;
-    Value *OriginalValue = RematerializedValuePair.second;
-
-    assert(AllocaMap.count(OriginalValue) &&
-           "Can not find alloca for rematerialized value");
-    Value *Alloca = AllocaMap[OriginalValue];
-
-    StoreInst *Store = new StoreInst(RematerializedValue, Alloca);
-    Store->insertAfter(RematerializedValue);
-
-#ifndef NDEBUG
-    VisitedLiveValues.insert(OriginalValue);
-#endif
-  }
-}
-
-/// Do all the relocation update via allocas and mem2reg
-static void relocationViaAlloca(
-    Function &F, DominatorTree &DT, ArrayRef<Value *> Live,
-    ArrayRef<PartiallyConstructedSafepointRecord> Records) {
-#ifndef NDEBUG
-  // record initial number of (static) allocas; we'll check we have the same
-  // number when we get done.
-  int InitialAllocaNum = 0;
-  for (Instruction &I : F.getEntryBlock())
-    if (isa<AllocaInst>(I))
-      InitialAllocaNum++;
-#endif
-
-  // TODO-PERF: change data structures, reserve
-  DenseMap<Value *, Value *> AllocaMap;
-  SmallVector<AllocaInst *, 200> PromotableAllocas;
-  // Used later to chack that we have enough allocas to store all values
-  std::size_t NumRematerializedValues = 0;
-  PromotableAllocas.reserve(Live.size());
-
-  // Emit alloca for "LiveValue" and record it in "allocaMap" and
-  // "PromotableAllocas"
-  const DataLayout &DL = F.getParent()->getDataLayout();
-  auto emitAllocaFor = [&](Value *LiveValue) {
-    AllocaInst *Alloca = new AllocaInst(LiveValue->getType(),
-                                        DL.getAllocaAddrSpace(), "",
-                                        F.getEntryBlock().getFirstNonPHI());
-    AllocaMap[LiveValue] = Alloca;
-    PromotableAllocas.push_back(Alloca);
-  };
-
-  // Emit alloca for each live gc pointer
-  for (Value *V : Live)
-    emitAllocaFor(V);
-
-  // Emit allocas for rematerialized values
-  for (const auto &Info : Records)
-    for (auto RematerializedValuePair : Info.RematerializedValues) {
-      Value *OriginalValue = RematerializedValuePair.second;
-      if (AllocaMap.count(OriginalValue) != 0)
-        continue;
-
-      emitAllocaFor(OriginalValue);
-      ++NumRematerializedValues;
-    }
-
-  // The next two loops are part of the same conceptual operation.  We need to
-  // insert a store to the alloca after the original def and at each
-  // redefinition.  We need to insert a load before each use.  These are split
-  // into distinct loops for performance reasons.
-
-  // Update gc pointer after each statepoint: either store a relocated value or
-  // null (if no relocated value was found for this gc pointer and it is not a
-  // gc_result).  This must happen before we update the statepoint with load of
-  // alloca otherwise we lose the link between statepoint and old def.
-  for (const auto &Info : Records) {
-    Value *Statepoint = Info.StatepointToken;
-
-    // This will be used for consistency check
-    DenseSet<Value *> VisitedLiveValues;
-
-    // Insert stores for normal statepoint gc relocates
-    insertRelocationStores(Statepoint->users(), AllocaMap, VisitedLiveValues);
-
-    // In case if it was invoke statepoint
-    // we will insert stores for exceptional path gc relocates.
-    if (isa<InvokeInst>(Statepoint)) {
-      insertRelocationStores(Info.UnwindToken->users(), AllocaMap,
-                             VisitedLiveValues);
-    }
-
-    // Do similar thing with rematerialized values
-    insertRematerializationStores(Info.RematerializedValues, AllocaMap,
-                                  VisitedLiveValues);
-
-    if (ClobberNonLive) {
-      // As a debugging aid, pretend that an unrelocated pointer becomes null at
-      // the gc.statepoint.  This will turn some subtle GC problems into
-      // slightly easier to debug SEGVs.  Note that on large IR files with
-      // lots of gc.statepoints this is extremely costly both memory and time
-      // wise.
-      SmallVector<AllocaInst *, 64> ToClobber;
-      for (auto Pair : AllocaMap) {
-        Value *Def = Pair.first;
-        AllocaInst *Alloca = cast<AllocaInst>(Pair.second);
-
-        // This value was relocated
-        if (VisitedLiveValues.count(Def)) {
-          continue;
-        }
-        ToClobber.push_back(Alloca);
-      }
-
-      auto InsertClobbersAt = [&](Instruction *IP) {
-        for (auto *AI : ToClobber) {
-          auto PT = cast<PointerType>(AI->getAllocatedType());
-          Constant *CPN = ConstantPointerNull::get(PT);
-          StoreInst *Store = new StoreInst(CPN, AI);
-          Store->insertBefore(IP);
-        }
-      };
-
-      // Insert the clobbering stores.  These may get intermixed with the
-      // gc.results and gc.relocates, but that's fine.
-      if (auto II = dyn_cast<InvokeInst>(Statepoint)) {
-        InsertClobbersAt(&*II->getNormalDest()->getFirstInsertionPt());
-        InsertClobbersAt(&*II->getUnwindDest()->getFirstInsertionPt());
-      } else {
-        InsertClobbersAt(cast<Instruction>(Statepoint)->getNextNode());
-      }
-    }
-  }
-
-  // Update use with load allocas and add store for gc_relocated.
-  for (auto Pair : AllocaMap) {
-    Value *Def = Pair.first;
-    Value *Alloca = Pair.second;
-
-    // We pre-record the uses of allocas so that we dont have to worry about
-    // later update that changes the user information..
-
-    SmallVector<Instruction *, 20> Uses;
-    // PERF: trade a linear scan for repeated reallocation
-    Uses.reserve(std::distance(Def->user_begin(), Def->user_end()));
-    for (User *U : Def->users()) {
-      if (!isa<ConstantExpr>(U)) {
-        // If the def has a ConstantExpr use, then the def is either a
-        // ConstantExpr use itself or null.  In either case
-        // (recursively in the first, directly in the second), the oop
-        // it is ultimately dependent on is null and this particular
-        // use does not need to be fixed up.
-        Uses.push_back(cast<Instruction>(U));
-      }
-    }
-
-    std::sort(Uses.begin(), Uses.end());
-    auto Last = std::unique(Uses.begin(), Uses.end());
-    Uses.erase(Last, Uses.end());
-
-    for (Instruction *Use : Uses) {
-      if (isa<PHINode>(Use)) {
-        PHINode *Phi = cast<PHINode>(Use);
-        for (unsigned i = 0; i < Phi->getNumIncomingValues(); i++) {
-          if (Def == Phi->getIncomingValue(i)) {
-            LoadInst *Load = new LoadInst(
-                Alloca, "", Phi->getIncomingBlock(i)->getTerminator());
-            Phi->setIncomingValue(i, Load);
-          }
-        }
-      } else {
-        LoadInst *Load = new LoadInst(Alloca, "", Use);
-        Use->replaceUsesOfWith(Def, Load);
-      }
-    }
-
-    // Emit store for the initial gc value.  Store must be inserted after load,
-    // otherwise store will be in alloca's use list and an extra load will be
-    // inserted before it.
-    StoreInst *Store = new StoreInst(Def, Alloca);
-    if (Instruction *Inst = dyn_cast<Instruction>(Def)) {
-      if (InvokeInst *Invoke = dyn_cast<InvokeInst>(Inst)) {
-        // InvokeInst is a TerminatorInst so the store need to be inserted
-        // into its normal destination block.
-        BasicBlock *NormalDest = Invoke->getNormalDest();
-        Store->insertBefore(NormalDest->getFirstNonPHI());
-      } else {
-        assert(!Inst->isTerminator() &&
-               "The only TerminatorInst that can produce a value is "
-               "InvokeInst which is handled above.");
-        Store->insertAfter(Inst);
-      }
-    } else {
-      assert(isa<Argument>(Def));
-      Store->insertAfter(cast<Instruction>(Alloca));
-    }
-  }
-
-  assert(PromotableAllocas.size() == Live.size() + NumRematerializedValues &&
-         "we must have the same allocas with lives");
-  if (!PromotableAllocas.empty()) {
-    // Apply mem2reg to promote alloca to SSA
-    PromoteMemToReg(PromotableAllocas, DT);
-  }
-
-#ifndef NDEBUG
-  for (auto &I : F.getEntryBlock())
-    if (isa<AllocaInst>(I))
-      InitialAllocaNum--;
-  assert(InitialAllocaNum == 0 && "We must not introduce any extra allocas");
-#endif
-}
-
-/// Implement a unique function which doesn't require we sort the input
-/// vector.  Doing so has the effect of changing the output of a couple of
-/// tests in ways which make them less useful in testing fused safepoints.
-template <typename T> static void unique_unsorted(SmallVectorImpl<T> &Vec) {
-  SmallSet<T, 8> Seen;
-  Vec.erase(remove_if(Vec, [&](const T &V) { return !Seen.insert(V).second; }),
-            Vec.end());
-}
-
-/// Insert holders so that each Value is obviously live through the entire
-/// lifetime of the call.
-static void insertUseHolderAfter(CallSite &CS, const ArrayRef<Value *> Values,
-                                 SmallVectorImpl<CallInst *> &Holders) {
-  if (Values.empty())
-    // No values to hold live, might as well not insert the empty holder
-    return;
-
-  Module *M = CS.getInstruction()->getModule();
-  // Use a dummy vararg function to actually hold the values live
-  Function *Func = cast<Function>(M->getOrInsertFunction(
-      "__tmp_use", FunctionType::get(Type::getVoidTy(M->getContext()), true)));
-  if (CS.isCall()) {
-    // For call safepoints insert dummy calls right after safepoint
-    Holders.push_back(CallInst::Create(Func, Values, "",
-                                       &*++CS.getInstruction()->getIterator()));
-    return;
-  }
-  // For invoke safepooints insert dummy calls both in normal and
-  // exceptional destination blocks
-  auto *II = cast<InvokeInst>(CS.getInstruction());
-  Holders.push_back(CallInst::Create(
-      Func, Values, "", &*II->getNormalDest()->getFirstInsertionPt()));
-  Holders.push_back(CallInst::Create(
-      Func, Values, "", &*II->getUnwindDest()->getFirstInsertionPt()));
-}
-
-static void findLiveReferences(
-    Function &F, DominatorTree &DT, ArrayRef<CallSite> toUpdate,
-    MutableArrayRef<struct PartiallyConstructedSafepointRecord> records) {
-  GCPtrLivenessData OriginalLivenessData;
-  computeLiveInValues(DT, F, OriginalLivenessData);
-  for (size_t i = 0; i < records.size(); i++) {
-    struct PartiallyConstructedSafepointRecord &info = records[i];
-    analyzeParsePointLiveness(DT, OriginalLivenessData, toUpdate[i], info);
-  }
-}
-
-// Helper function for the "rematerializeLiveValues". It walks use chain
-// starting from the "CurrentValue" until it reaches the root of the chain, i.e.
-// the base or a value it cannot process. Only "simple" values are processed
-// (currently it is GEP's and casts). The returned root is  examined by the
-// callers of findRematerializableChainToBasePointer.  Fills "ChainToBase" array
-// with all visited values.
-static Value* findRematerializableChainToBasePointer(
-  SmallVectorImpl<Instruction*> &ChainToBase,
-  Value *CurrentValue) {
-  if (GetElementPtrInst *GEP = dyn_cast<GetElementPtrInst>(CurrentValue)) {
-    ChainToBase.push_back(GEP);
-    return findRematerializableChainToBasePointer(ChainToBase,
-                                                  GEP->getPointerOperand());
-  }
-
-  if (CastInst *CI = dyn_cast<CastInst>(CurrentValue)) {
-    if (!CI->isNoopCast(CI->getModule()->getDataLayout()))
-      return CI;
-
-    ChainToBase.push_back(CI);
-    return findRematerializableChainToBasePointer(ChainToBase,
-                                                  CI->getOperand(0));
-  }
-
-  // We have reached the root of the chain, which is either equal to the base or
-  // is the first unsupported value along the use chain.
-  return CurrentValue;
-}
-
-// Helper function for the "rematerializeLiveValues". Compute cost of the use
-// chain we are going to rematerialize.
-static unsigned
-chainToBasePointerCost(SmallVectorImpl<Instruction*> &Chain,
-                       TargetTransformInfo &TTI) {
-  unsigned Cost = 0;
-
-  for (Instruction *Instr : Chain) {
-    if (CastInst *CI = dyn_cast<CastInst>(Instr)) {
-      assert(CI->isNoopCast(CI->getModule()->getDataLayout()) &&
-             "non noop cast is found during rematerialization");
-
-      Type *SrcTy = CI->getOperand(0)->getType();
-      Cost += TTI.getCastInstrCost(CI->getOpcode(), CI->getType(), SrcTy, CI);
-
-    } else if (GetElementPtrInst *GEP = dyn_cast<GetElementPtrInst>(Instr)) {
-      // Cost of the address calculation
-      Type *ValTy = GEP->getSourceElementType();
-      Cost += TTI.getAddressComputationCost(ValTy);
-
-      // And cost of the GEP itself
-      // TODO: Use TTI->getGEPCost here (it exists, but appears to be not
-      //       allowed for the external usage)
-      if (!GEP->hasAllConstantIndices())
-        Cost += 2;
-
-    } else {
-      llvm_unreachable("unsupported instruciton type during rematerialization");
-    }
-  }
-
-  return Cost;
-}
-
-static bool AreEquivalentPhiNodes(PHINode &OrigRootPhi, PHINode &AlternateRootPhi) {
-  unsigned PhiNum = OrigRootPhi.getNumIncomingValues();
-  if (PhiNum != AlternateRootPhi.getNumIncomingValues() ||
-      OrigRootPhi.getParent() != AlternateRootPhi.getParent())
-    return false;
-  // Map of incoming values and their corresponding basic blocks of
-  // OrigRootPhi.
-  SmallDenseMap<Value *, BasicBlock *, 8> CurrentIncomingValues;
-  for (unsigned i = 0; i < PhiNum; i++)
-    CurrentIncomingValues[OrigRootPhi.getIncomingValue(i)] =
-        OrigRootPhi.getIncomingBlock(i);
-
-  // Both current and base PHIs should have same incoming values and
-  // the same basic blocks corresponding to the incoming values.
-  for (unsigned i = 0; i < PhiNum; i++) {
-    auto CIVI =
-        CurrentIncomingValues.find(AlternateRootPhi.getIncomingValue(i));
-    if (CIVI == CurrentIncomingValues.end())
-      return false;
-    BasicBlock *CurrentIncomingBB = CIVI->second;
-    if (CurrentIncomingBB != AlternateRootPhi.getIncomingBlock(i))
-      return false;
-  }
-  return true;
-}
-
-// From the statepoint live set pick values that are cheaper to recompute then
-// to relocate. Remove this values from the live set, rematerialize them after
-// statepoint and record them in "Info" structure. Note that similar to
-// relocated values we don't do any user adjustments here.
-static void rematerializeLiveValues(CallSite CS,
-                                    PartiallyConstructedSafepointRecord &Info,
-                                    TargetTransformInfo &TTI) {
-  const unsigned int ChainLengthThreshold = 10;
-
-  // Record values we are going to delete from this statepoint live set.
-  // We can not di this in following loop due to iterator invalidation.
-  SmallVector<Value *, 32> LiveValuesToBeDeleted;
-
-  for (Value *LiveValue: Info.LiveSet) {
-    // For each live pointer find it's defining chain
-    SmallVector<Instruction *, 3> ChainToBase;
-    assert(Info.PointerToBase.count(LiveValue));
-    Value *RootOfChain =
-      findRematerializableChainToBasePointer(ChainToBase,
-                                             LiveValue);
-
-    // Nothing to do, or chain is too long
-    if ( ChainToBase.size() == 0 ||
-        ChainToBase.size() > ChainLengthThreshold)
-      continue;
-
-    // Handle the scenario where the RootOfChain is not equal to the
-    // Base Value, but they are essentially the same phi values.
-    if (RootOfChain != Info.PointerToBase[LiveValue]) {
-      PHINode *OrigRootPhi = dyn_cast<PHINode>(RootOfChain);
-      PHINode *AlternateRootPhi = dyn_cast<PHINode>(Info.PointerToBase[LiveValue]);
-      if (!OrigRootPhi || !AlternateRootPhi)
-        continue;
-      // PHI nodes that have the same incoming values, and belonging to the same
-      // basic blocks are essentially the same SSA value.  When the original phi
-      // has incoming values with different base pointers, the original phi is
-      // marked as conflict, and an additional `AlternateRootPhi` with the same
-      // incoming values get generated by the findBasePointer function. We need
-      // to identify the newly generated AlternateRootPhi (.base version of phi)
-      // and RootOfChain (the original phi node itself) are the same, so that we
-      // can rematerialize the gep and casts. This is a workaround for the
-      // deficiency in the findBasePointer algorithm.
-      if (!AreEquivalentPhiNodes(*OrigRootPhi, *AlternateRootPhi))
-        continue;
-      // Now that the phi nodes are proved to be the same, assert that
-      // findBasePointer's newly generated AlternateRootPhi is present in the
-      // liveset of the call.
-      assert(Info.LiveSet.count(AlternateRootPhi));
-    }
-    // Compute cost of this chain
-    unsigned Cost = chainToBasePointerCost(ChainToBase, TTI);
-    // TODO: We can also account for cases when we will be able to remove some
-    //       of the rematerialized values by later optimization passes. I.e if
-    //       we rematerialized several intersecting chains. Or if original values
-    //       don't have any uses besides this statepoint.
-
-    // For invokes we need to rematerialize each chain twice - for normal and
-    // for unwind basic blocks. Model this by multiplying cost by two.
-    if (CS.isInvoke()) {
-      Cost *= 2;
-    }
-    // If it's too expensive - skip it
-    if (Cost >= RematerializationThreshold)
-      continue;
-
-    // Remove value from the live set
-    LiveValuesToBeDeleted.push_back(LiveValue);
-
-    // Clone instructions and record them inside "Info" structure
-
-    // Walk backwards to visit top-most instructions first
-    std::reverse(ChainToBase.begin(), ChainToBase.end());
-
-    // Utility function which clones all instructions from "ChainToBase"
-    // and inserts them before "InsertBefore". Returns rematerialized value
-    // which should be used after statepoint.
-    auto rematerializeChain = [&ChainToBase](
-        Instruction *InsertBefore, Value *RootOfChain, Value *AlternateLiveBase) {
-      Instruction *LastClonedValue = nullptr;
-      Instruction *LastValue = nullptr;
-      for (Instruction *Instr: ChainToBase) {
-        // Only GEP's and casts are supported as we need to be careful to not
-        // introduce any new uses of pointers not in the liveset.
-        // Note that it's fine to introduce new uses of pointers which were
-        // otherwise not used after this statepoint.
-        assert(isa<GetElementPtrInst>(Instr) || isa<CastInst>(Instr));
-
-        Instruction *ClonedValue = Instr->clone();
-        ClonedValue->insertBefore(InsertBefore);
-        ClonedValue->setName(Instr->getName() + ".remat");
-
-        // If it is not first instruction in the chain then it uses previously
-        // cloned value. We should update it to use cloned value.
-        if (LastClonedValue) {
-          assert(LastValue);
-          ClonedValue->replaceUsesOfWith(LastValue, LastClonedValue);
-#ifndef NDEBUG
-          for (auto OpValue : ClonedValue->operand_values()) {
-            // Assert that cloned instruction does not use any instructions from
-            // this chain other than LastClonedValue
-            assert(!is_contained(ChainToBase, OpValue) &&
-                   "incorrect use in rematerialization chain");
-            // Assert that the cloned instruction does not use the RootOfChain
-            // or the AlternateLiveBase.
-            assert(OpValue != RootOfChain && OpValue != AlternateLiveBase);
-          }
-#endif
-        } else {
-          // For the first instruction, replace the use of unrelocated base i.e.
-          // RootOfChain/OrigRootPhi, with the corresponding PHI present in the
-          // live set. They have been proved to be the same PHI nodes.  Note
-          // that the *only* use of the RootOfChain in the ChainToBase list is
-          // the first Value in the list.
-          if (RootOfChain != AlternateLiveBase)
-            ClonedValue->replaceUsesOfWith(RootOfChain, AlternateLiveBase);
-        }
-
-        LastClonedValue = ClonedValue;
-        LastValue = Instr;
-      }
-      assert(LastClonedValue);
-      return LastClonedValue;
-    };
-
-    // Different cases for calls and invokes. For invokes we need to clone
-    // instructions both on normal and unwind path.
-    if (CS.isCall()) {
-      Instruction *InsertBefore = CS.getInstruction()->getNextNode();
-      assert(InsertBefore);
-      Instruction *RematerializedValue = rematerializeChain(
-          InsertBefore, RootOfChain, Info.PointerToBase[LiveValue]);
-      Info.RematerializedValues[RematerializedValue] = LiveValue;
-    } else {
-      InvokeInst *Invoke = cast<InvokeInst>(CS.getInstruction());
-
-      Instruction *NormalInsertBefore =
-          &*Invoke->getNormalDest()->getFirstInsertionPt();
-      Instruction *UnwindInsertBefore =
-          &*Invoke->getUnwindDest()->getFirstInsertionPt();
-
-      Instruction *NormalRematerializedValue = rematerializeChain(
-          NormalInsertBefore, RootOfChain, Info.PointerToBase[LiveValue]);
-      Instruction *UnwindRematerializedValue = rematerializeChain(
-          UnwindInsertBefore, RootOfChain, Info.PointerToBase[LiveValue]);
-
-      Info.RematerializedValues[NormalRematerializedValue] = LiveValue;
-      Info.RematerializedValues[UnwindRematerializedValue] = LiveValue;
-    }
-  }
-
-  // Remove rematerializaed values from the live set
-  for (auto LiveValue: LiveValuesToBeDeleted) {
-    Info.LiveSet.remove(LiveValue);
-  }
-}
-
-static bool insertParsePoints(Function &F, DominatorTree &DT,
-                              TargetTransformInfo &TTI,
-                              SmallVectorImpl<CallSite> &ToUpdate) {
-#ifndef NDEBUG
-  // sanity check the input
-  std::set<CallSite> Uniqued;
-  Uniqued.insert(ToUpdate.begin(), ToUpdate.end());
-  assert(Uniqued.size() == ToUpdate.size() && "no duplicates please!");
-
-  for (CallSite CS : ToUpdate)
-    assert(CS.getInstruction()->getFunction() == &F);
-#endif
-
-  // When inserting gc.relocates for invokes, we need to be able to insert at
-  // the top of the successor blocks.  See the comment on
-  // normalForInvokeSafepoint on exactly what is needed.  Note that this step
-  // may restructure the CFG.
-  for (CallSite CS : ToUpdate) {
-    if (!CS.isInvoke())
-      continue;
-    auto *II = cast<InvokeInst>(CS.getInstruction());
-    normalizeForInvokeSafepoint(II->getNormalDest(), II->getParent(), DT);
-    normalizeForInvokeSafepoint(II->getUnwindDest(), II->getParent(), DT);
-  }
-
-  // A list of dummy calls added to the IR to keep various values obviously
-  // live in the IR.  We'll remove all of these when done.
-  SmallVector<CallInst *, 64> Holders;
-
-  // Insert a dummy call with all of the deopt operands we'll need for the
-  // actual safepoint insertion as arguments.  This ensures reference operands
-  // in the deopt argument list are considered live through the safepoint (and
-  // thus makes sure they get relocated.)
-  for (CallSite CS : ToUpdate) {
-    SmallVector<Value *, 64> DeoptValues;
-
-    for (Value *Arg : GetDeoptBundleOperands(CS)) {
-      assert(!isUnhandledGCPointerType(Arg->getType()) &&
-             "support for FCA unimplemented");
-      if (isHandledGCPointerType(Arg->getType()))
-        DeoptValues.push_back(Arg);
-    }
-
-    insertUseHolderAfter(CS, DeoptValues, Holders);
-  }
-
-  SmallVector<PartiallyConstructedSafepointRecord, 64> Records(ToUpdate.size());
-
-  // A) Identify all gc pointers which are statically live at the given call
-  // site.
-  findLiveReferences(F, DT, ToUpdate, Records);
-
-  // B) Find the base pointers for each live pointer
-  /* scope for caching */ {
-    // Cache the 'defining value' relation used in the computation and
-    // insertion of base phis and selects.  This ensures that we don't insert
-    // large numbers of duplicate base_phis.
-    DefiningValueMapTy DVCache;
-
-    for (size_t i = 0; i < Records.size(); i++) {
-      PartiallyConstructedSafepointRecord &info = Records[i];
-      findBasePointers(DT, DVCache, ToUpdate[i], info);
-    }
-  } // end of cache scope
-
-  // The base phi insertion logic (for any safepoint) may have inserted new
-  // instructions which are now live at some safepoint.  The simplest such
-  // example is:
-  // loop:
-  //   phi a  <-- will be a new base_phi here
-  //   safepoint 1 <-- that needs to be live here
-  //   gep a + 1
-  //   safepoint 2
-  //   br loop
-  // We insert some dummy calls after each safepoint to definitely hold live
-  // the base pointers which were identified for that safepoint.  We'll then
-  // ask liveness for _every_ base inserted to see what is now live.  Then we
-  // remove the dummy calls.
-  Holders.reserve(Holders.size() + Records.size());
-  for (size_t i = 0; i < Records.size(); i++) {
-    PartiallyConstructedSafepointRecord &Info = Records[i];
-
-    SmallVector<Value *, 128> Bases;
-    for (auto Pair : Info.PointerToBase)
-      Bases.push_back(Pair.second);
-
-    insertUseHolderAfter(ToUpdate[i], Bases, Holders);
-  }
-
-  // By selecting base pointers, we've effectively inserted new uses. Thus, we
-  // need to rerun liveness.  We may *also* have inserted new defs, but that's
-  // not the key issue.
-  recomputeLiveInValues(F, DT, ToUpdate, Records);
-
-  if (PrintBasePointers) {
-    for (auto &Info : Records) {
-      errs() << "Base Pairs: (w/Relocation)\n";
-      for (auto Pair : Info.PointerToBase) {
-        errs() << " derived ";
-        Pair.first->printAsOperand(errs(), false);
-        errs() << " base ";
-        Pair.second->printAsOperand(errs(), false);
-        errs() << "\n";
-      }
-    }
-  }
-
-  // It is possible that non-constant live variables have a constant base.  For
-  // example, a GEP with a variable offset from a global.  In this case we can
-  // remove it from the liveset.  We already don't add constants to the liveset
-  // because we assume they won't move at runtime and the GC doesn't need to be
-  // informed about them.  The same reasoning applies if the base is constant.
-  // Note that the relocation placement code relies on this filtering for
-  // correctness as it expects the base to be in the liveset, which isn't true
-  // if the base is constant.
-  for (auto &Info : Records)
-    for (auto &BasePair : Info.PointerToBase)
-      if (isa<Constant>(BasePair.second))
-        Info.LiveSet.remove(BasePair.first);
-
-  for (CallInst *CI : Holders)
-    CI->eraseFromParent();
-
-  Holders.clear();
-
-  // In order to reduce live set of statepoint we might choose to rematerialize
-  // some values instead of relocating them. This is purely an optimization and
-  // does not influence correctness.
-  for (size_t i = 0; i < Records.size(); i++)
-    rematerializeLiveValues(ToUpdate[i], Records[i], TTI);
-
-  // We need this to safely RAUW and delete call or invoke return values that
-  // may themselves be live over a statepoint.  For details, please see usage in
-  // makeStatepointExplicitImpl.
-  std::vector<DeferredReplacement> Replacements;
-
-  // Now run through and replace the existing statepoints with new ones with
-  // the live variables listed.  We do not yet update uses of the values being
-  // relocated. We have references to live variables that need to
-  // survive to the last iteration of this loop.  (By construction, the
-  // previous statepoint can not be a live variable, thus we can and remove
-  // the old statepoint calls as we go.)
-  for (size_t i = 0; i < Records.size(); i++)
-    makeStatepointExplicit(DT, ToUpdate[i], Records[i], Replacements);
-
-  ToUpdate.clear(); // prevent accident use of invalid CallSites
-
-  for (auto &PR : Replacements)
-    PR.doReplacement();
-
-  Replacements.clear();
-
-  for (auto &Info : Records) {
-    // These live sets may contain state Value pointers, since we replaced calls
-    // with operand bundles with calls wrapped in gc.statepoint, and some of
-    // those calls may have been def'ing live gc pointers.  Clear these out to
-    // avoid accidentally using them.
-    //
-    // TODO: We should create a separate data structure that does not contain
-    // these live sets, and migrate to using that data structure from this point
-    // onward.
-    Info.LiveSet.clear();
-    Info.PointerToBase.clear();
-  }
-
-  // Do all the fixups of the original live variables to their relocated selves
-  SmallVector<Value *, 128> Live;
-  for (size_t i = 0; i < Records.size(); i++) {
-    PartiallyConstructedSafepointRecord &Info = Records[i];
-
-    // We can't simply save the live set from the original insertion.  One of
-    // the live values might be the result of a call which needs a safepoint.
-    // That Value* no longer exists and we need to use the new gc_result.
-    // Thankfully, the live set is embedded in the statepoint (and updated), so
-    // we just grab that.
-    Statepoint Statepoint(Info.StatepointToken);
-    Live.insert(Live.end(), Statepoint.gc_args_begin(),
-                Statepoint.gc_args_end());
-#ifndef NDEBUG
-    // Do some basic sanity checks on our liveness results before performing
-    // relocation.  Relocation can and will turn mistakes in liveness results
-    // into non-sensical code which is must harder to debug.
-    // TODO: It would be nice to test consistency as well
-    assert(DT.isReachableFromEntry(Info.StatepointToken->getParent()) &&
-           "statepoint must be reachable or liveness is meaningless");
-    for (Value *V : Statepoint.gc_args()) {
-      if (!isa<Instruction>(V))
-        // Non-instruction values trivial dominate all possible uses
-        continue;
-      auto *LiveInst = cast<Instruction>(V);
-      assert(DT.isReachableFromEntry(LiveInst->getParent()) &&
-             "unreachable values should never be live");
-      assert(DT.dominates(LiveInst, Info.StatepointToken) &&
-             "basic SSA liveness expectation violated by liveness analysis");
-    }
-#endif
-  }
-  unique_unsorted(Live);
-
-#ifndef NDEBUG
-  // sanity check
-  for (auto *Ptr : Live)
-    assert(isHandledGCPointerType(Ptr->getType()) &&
-           "must be a gc pointer type");
-#endif
-
-  relocationViaAlloca(F, DT, Live, Records);
-  return !Records.empty();
-}
-
-// Handles both return values and arguments for Functions and CallSites.
-template <typename AttrHolder>
-static void RemoveNonValidAttrAtIndex(LLVMContext &Ctx, AttrHolder &AH,
-                                      unsigned Index) {
-  AttrBuilder R;
-  if (AH.getDereferenceableBytes(Index))
-    R.addAttribute(Attribute::get(Ctx, Attribute::Dereferenceable,
-                                  AH.getDereferenceableBytes(Index)));
-  if (AH.getDereferenceableOrNullBytes(Index))
-    R.addAttribute(Attribute::get(Ctx, Attribute::DereferenceableOrNull,
-                                  AH.getDereferenceableOrNullBytes(Index)));
-  if (AH.getAttributes().hasAttribute(Index, Attribute::NoAlias))
-    R.addAttribute(Attribute::NoAlias);
-
-  if (!R.empty())
-    AH.setAttributes(AH.getAttributes().removeAttributes(Ctx, Index, R));
-}
-
-static void stripNonValidAttributesFromPrototype(Function &F) {
-  LLVMContext &Ctx = F.getContext();
-
-  for (Argument &A : F.args())
-    if (isa<PointerType>(A.getType()))
-      RemoveNonValidAttrAtIndex(Ctx, F,
-                                A.getArgNo() + AttributeList::FirstArgIndex);
-
-  if (isa<PointerType>(F.getReturnType()))
-    RemoveNonValidAttrAtIndex(Ctx, F, AttributeList::ReturnIndex);
-}
-
-/// Certain metadata on instructions are invalid after running RS4GC.
-/// Optimizations that run after RS4GC can incorrectly use this metadata to
-/// optimize functions. We drop such metadata on the instruction.
-static void stripInvalidMetadataFromInstruction(Instruction &I) {
-  if (!isa<LoadInst>(I) && !isa<StoreInst>(I))
-    return;
-  // These are the attributes that are still valid on loads and stores after
-  // RS4GC.
-  // The metadata implying dereferenceability and noalias are (conservatively)
-  // dropped.  This is because semantically, after RewriteStatepointsForGC runs,
-  // all calls to gc.statepoint "free" the entire heap. Also, gc.statepoint can
-  // touch the entire heap including noalias objects. Note: The reasoning is
-  // same as stripping the dereferenceability and noalias attributes that are
-  // analogous to the metadata counterparts.
-  // We also drop the invariant.load metadata on the load because that metadata
-  // implies the address operand to the load points to memory that is never
-  // changed once it became dereferenceable. This is no longer true after RS4GC.
-  // Similar reasoning applies to invariant.group metadata, which applies to
-  // loads within a group.
-  unsigned ValidMetadataAfterRS4GC[] = {LLVMContext::MD_tbaa,
-                         LLVMContext::MD_range,
-                         LLVMContext::MD_alias_scope,
-                         LLVMContext::MD_nontemporal,
-                         LLVMContext::MD_nonnull,
-                         LLVMContext::MD_align,
-                         LLVMContext::MD_type};
-
-  // Drops all metadata on the instruction other than ValidMetadataAfterRS4GC.
-  I.dropUnknownNonDebugMetadata(ValidMetadataAfterRS4GC);
-}
-
-static void stripNonValidDataFromBody(Function &F) {
-  if (F.empty())
-    return;
-
-  LLVMContext &Ctx = F.getContext();
-  MDBuilder Builder(Ctx);
-
-  // Set of invariantstart instructions that we need to remove.
-  // Use this to avoid invalidating the instruction iterator.
-  SmallVector<IntrinsicInst*, 12> InvariantStartInstructions;
-
-  for (Instruction &I : instructions(F)) {
-    // invariant.start on memory location implies that the referenced memory
-    // location is constant and unchanging. This is no longer true after
-    // RewriteStatepointsForGC runs because there can be calls to gc.statepoint
-    // which frees the entire heap and the presence of invariant.start allows
-    // the optimizer to sink the load of a memory location past a statepoint,
-    // which is incorrect.
-    if (auto *II = dyn_cast<IntrinsicInst>(&I))
-      if (II->getIntrinsicID() == Intrinsic::invariant_start) {
-        InvariantStartInstructions.push_back(II);
-        continue;
-      }
-
-    if (const MDNode *MD = I.getMetadata(LLVMContext::MD_tbaa)) {
-      assert(MD->getNumOperands() < 5 && "unrecognized metadata shape!");
-      bool IsImmutableTBAA =
-          MD->getNumOperands() == 4 &&
-          mdconst::extract<ConstantInt>(MD->getOperand(3))->getValue() == 1;
-
-      if (!IsImmutableTBAA)
-        continue; // no work to do, MD_tbaa is already marked mutable
-
-      MDNode *Base = cast<MDNode>(MD->getOperand(0));
-      MDNode *Access = cast<MDNode>(MD->getOperand(1));
-      uint64_t Offset =
-          mdconst::extract<ConstantInt>(MD->getOperand(2))->getZExtValue();
-
-      MDNode *MutableTBAA =
-          Builder.createTBAAStructTagNode(Base, Access, Offset);
-      I.setMetadata(LLVMContext::MD_tbaa, MutableTBAA);
-    }
-
-    stripInvalidMetadataFromInstruction(I);
-
-    if (CallSite CS = CallSite(&I)) {
-      for (int i = 0, e = CS.arg_size(); i != e; i++)
-        if (isa<PointerType>(CS.getArgument(i)->getType()))
-          RemoveNonValidAttrAtIndex(Ctx, CS, i + AttributeList::FirstArgIndex);
-      if (isa<PointerType>(CS.getType()))
-        RemoveNonValidAttrAtIndex(Ctx, CS, AttributeList::ReturnIndex);
-    }
-  }
-
-  // Delete the invariant.start instructions and RAUW undef.
-  for (auto *II : InvariantStartInstructions) {
-    II->replaceAllUsesWith(UndefValue::get(II->getType()));
-    II->eraseFromParent();
-  }
-}
-
-/// Returns true if this function should be rewritten by this pass.  The main
-/// point of this function is as an extension point for custom logic.
-static bool shouldRewriteStatepointsIn(Function &F) {
-  // TODO: This should check the GCStrategy
-  if (F.hasGC()) {
-    const auto &FunctionGCName = F.getGC();
-    const StringRef StatepointExampleName("statepoint-example");
-    const StringRef CoreCLRName("coreclr");
-    return (StatepointExampleName == FunctionGCName) ||
-           (CoreCLRName == FunctionGCName);
-  } else
-    return false;
-}
-
-static void stripNonValidData(Module &M) {
-#ifndef NDEBUG
-  assert(llvm::any_of(M, shouldRewriteStatepointsIn) && "precondition!");
-#endif
-
-  for (Function &F : M)
-    stripNonValidAttributesFromPrototype(F);
-
-  for (Function &F : M)
-    stripNonValidDataFromBody(F);
-}
-
-bool RewriteStatepointsForGC::runOnFunction(Function &F, DominatorTree &DT,
-                                            TargetTransformInfo &TTI,
-                                            const TargetLibraryInfo &TLI) {
-  assert(!F.isDeclaration() && !F.empty() &&
-         "need function body to rewrite statepoints in");
-  assert(shouldRewriteStatepointsIn(F) && "mismatch in rewrite decision");
-
-  auto NeedsRewrite = [&TLI](Instruction &I) {
-    if (ImmutableCallSite CS = ImmutableCallSite(&I))
-      return !callsGCLeafFunction(CS, TLI) && !isStatepoint(CS);
-    return false;
-  };
-
-  // Gather all the statepoints which need rewritten.  Be careful to only
-  // consider those in reachable code since we need to ask dominance queries
-  // when rewriting.  We'll delete the unreachable ones in a moment.
-  SmallVector<CallSite, 64> ParsePointNeeded;
-  bool HasUnreachableStatepoint = false;
-  for (Instruction &I : instructions(F)) {
-    // TODO: only the ones with the flag set!
-    if (NeedsRewrite(I)) {
-      if (DT.isReachableFromEntry(I.getParent()))
-        ParsePointNeeded.push_back(CallSite(&I));
-      else
-        HasUnreachableStatepoint = true;
-    }
-  }
-
-  bool MadeChange = false;
-
-  // Delete any unreachable statepoints so that we don't have unrewritten
-  // statepoints surviving this pass.  This makes testing easier and the
-  // resulting IR less confusing to human readers.  Rather than be fancy, we
-  // just reuse a utility function which removes the unreachable blocks.
-  if (HasUnreachableStatepoint)
-    MadeChange |= removeUnreachableBlocks(F);
-
-  // Return early if no work to do.
-  if (ParsePointNeeded.empty())
-    return MadeChange;
-
-  // As a prepass, go ahead and aggressively destroy single entry phi nodes.
-  // These are created by LCSSA.  They have the effect of increasing the size
-  // of liveness sets for no good reason.  It may be harder to do this post
-  // insertion since relocations and base phis can confuse things.
-  for (BasicBlock &BB : F)
-    if (BB.getUniquePredecessor()) {
-      MadeChange = true;
-      FoldSingleEntryPHINodes(&BB);
-    }
-
-  // Before we start introducing relocations, we want to tweak the IR a bit to
-  // avoid unfortunate code generation effects.  The main example is that we 
-  // want to try to make sure the comparison feeding a branch is after any
-  // safepoints.  Otherwise, we end up with a comparison of pre-relocation
-  // values feeding a branch after relocation.  This is semantically correct,
-  // but results in extra register pressure since both the pre-relocation and
-  // post-relocation copies must be available in registers.  For code without
-  // relocations this is handled elsewhere, but teaching the scheduler to
-  // reverse the transform we're about to do would be slightly complex.
-  // Note: This may extend the live range of the inputs to the icmp and thus
-  // increase the liveset of any statepoint we move over.  This is profitable
-  // as long as all statepoints are in rare blocks.  If we had in-register
-  // lowering for live values this would be a much safer transform.
-  auto getConditionInst = [](TerminatorInst *TI) -> Instruction* {
-    if (auto *BI = dyn_cast<BranchInst>(TI))
-      if (BI->isConditional())
-        return dyn_cast<Instruction>(BI->getCondition());
-    // TODO: Extend this to handle switches
-    return nullptr;
-  };
-  for (BasicBlock &BB : F) {
-    TerminatorInst *TI = BB.getTerminator();
-    if (auto *Cond = getConditionInst(TI))
-      // TODO: Handle more than just ICmps here.  We should be able to move
-      // most instructions without side effects or memory access.  
-      if (isa<ICmpInst>(Cond) && Cond->hasOneUse()) {
-        MadeChange = true;
-        Cond->moveBefore(TI);
-      }
-  }
-
-  MadeChange |= insertParsePoints(F, DT, TTI, ParsePointNeeded);
-  return MadeChange;
-}
-
-// liveness computation via standard dataflow
-// -------------------------------------------------------------------
-
-// TODO: Consider using bitvectors for liveness, the set of potentially
-// interesting values should be small and easy to pre-compute.
-
-/// Compute the live-in set for the location rbegin starting from
-/// the live-out set of the basic block
-static void computeLiveInValues(BasicBlock::reverse_iterator Begin,
-                                BasicBlock::reverse_iterator End,
-                                SetVector<Value *> &LiveTmp) {
-  for (auto &I : make_range(Begin, End)) {
-    // KILL/Def - Remove this definition from LiveIn
-    LiveTmp.remove(&I);
-
-    // Don't consider *uses* in PHI nodes, we handle their contribution to
-    // predecessor blocks when we seed the LiveOut sets
-    if (isa<PHINode>(I))
-      continue;
-
-    // USE - Add to the LiveIn set for this instruction
-    for (Value *V : I.operands()) {
-      assert(!isUnhandledGCPointerType(V->getType()) &&
-             "support for FCA unimplemented");
-      if (isHandledGCPointerType(V->getType()) && !isa<Constant>(V)) {
-        // The choice to exclude all things constant here is slightly subtle.
-        // There are two independent reasons:
-        // - We assume that things which are constant (from LLVM's definition)
-        // do not move at runtime.  For example, the address of a global
-        // variable is fixed, even though it's contents may not be.
-        // - Second, we can't disallow arbitrary inttoptr constants even
-        // if the language frontend does.  Optimization passes are free to
-        // locally exploit facts without respect to global reachability.  This
-        // can create sections of code which are dynamically unreachable and
-        // contain just about anything.  (see constants.ll in tests)
-        LiveTmp.insert(V);
-      }
-    }
-  }
-}
-
-static void computeLiveOutSeed(BasicBlock *BB, SetVector<Value *> &LiveTmp) {
-  for (BasicBlock *Succ : successors(BB)) {
-    for (auto &I : *Succ) {
-      PHINode *PN = dyn_cast<PHINode>(&I);
-      if (!PN)
-        break;
-
-      Value *V = PN->getIncomingValueForBlock(BB);
-      assert(!isUnhandledGCPointerType(V->getType()) &&
-             "support for FCA unimplemented");
-      if (isHandledGCPointerType(V->getType()) && !isa<Constant>(V))
-        LiveTmp.insert(V);
-    }
-  }
-}
-
-static SetVector<Value *> computeKillSet(BasicBlock *BB) {
-  SetVector<Value *> KillSet;
-  for (Instruction &I : *BB)
-    if (isHandledGCPointerType(I.getType()))
-      KillSet.insert(&I);
-  return KillSet;
-}
-
-#ifndef NDEBUG
-/// Check that the items in 'Live' dominate 'TI'.  This is used as a basic
-/// sanity check for the liveness computation.
-static void checkBasicSSA(DominatorTree &DT, SetVector<Value *> &Live,
-                          TerminatorInst *TI, bool TermOkay = false) {
-  for (Value *V : Live) {
-    if (auto *I = dyn_cast<Instruction>(V)) {
-      // The terminator can be a member of the LiveOut set.  LLVM's definition
-      // of instruction dominance states that V does not dominate itself.  As
-      // such, we need to special case this to allow it.
-      if (TermOkay && TI == I)
-        continue;
-      assert(DT.dominates(I, TI) &&
-             "basic SSA liveness expectation violated by liveness analysis");
-    }
-  }
-}
-
-/// Check that all the liveness sets used during the computation of liveness
-/// obey basic SSA properties.  This is useful for finding cases where we miss
-/// a def.
-static void checkBasicSSA(DominatorTree &DT, GCPtrLivenessData &Data,
-                          BasicBlock &BB) {
-  checkBasicSSA(DT, Data.LiveSet[&BB], BB.getTerminator());
-  checkBasicSSA(DT, Data.LiveOut[&BB], BB.getTerminator(), true);
-  checkBasicSSA(DT, Data.LiveIn[&BB], BB.getTerminator());
-}
-#endif
-
-static void computeLiveInValues(DominatorTree &DT, Function &F,
-                                GCPtrLivenessData &Data) {
-  SmallSetVector<BasicBlock *, 32> Worklist;
-
-  // Seed the liveness for each individual block
-  for (BasicBlock &BB : F) {
-    Data.KillSet[&BB] = computeKillSet(&BB);
-    Data.LiveSet[&BB].clear();
-    computeLiveInValues(BB.rbegin(), BB.rend(), Data.LiveSet[&BB]);
-
-#ifndef NDEBUG
-    for (Value *Kill : Data.KillSet[&BB])
-      assert(!Data.LiveSet[&BB].count(Kill) && "live set contains kill");
-#endif
-
-    Data.LiveOut[&BB] = SetVector<Value *>();
-    computeLiveOutSeed(&BB, Data.LiveOut[&BB]);
-    Data.LiveIn[&BB] = Data.LiveSet[&BB];
-    Data.LiveIn[&BB].set_union(Data.LiveOut[&BB]);
-    Data.LiveIn[&BB].set_subtract(Data.KillSet[&BB]);
-    if (!Data.LiveIn[&BB].empty())
-      Worklist.insert(pred_begin(&BB), pred_end(&BB));
-  }
-
-  // Propagate that liveness until stable
-  while (!Worklist.empty()) {
-    BasicBlock *BB = Worklist.pop_back_val();
-
-    // Compute our new liveout set, then exit early if it hasn't changed despite
-    // the contribution of our successor.
-    SetVector<Value *> LiveOut = Data.LiveOut[BB];
-    const auto OldLiveOutSize = LiveOut.size();
-    for (BasicBlock *Succ : successors(BB)) {
-      assert(Data.LiveIn.count(Succ));
-      LiveOut.set_union(Data.LiveIn[Succ]);
-    }
-    // assert OutLiveOut is a subset of LiveOut
-    if (OldLiveOutSize == LiveOut.size()) {
-      // If the sets are the same size, then we didn't actually add anything
-      // when unioning our successors LiveIn.  Thus, the LiveIn of this block
-      // hasn't changed.
-      continue;
-    }
-    Data.LiveOut[BB] = LiveOut;
-
-    // Apply the effects of this basic block
-    SetVector<Value *> LiveTmp = LiveOut;
-    LiveTmp.set_union(Data.LiveSet[BB]);
-    LiveTmp.set_subtract(Data.KillSet[BB]);
-
-    assert(Data.LiveIn.count(BB));
-    const SetVector<Value *> &OldLiveIn = Data.LiveIn[BB];
-    // assert: OldLiveIn is a subset of LiveTmp
-    if (OldLiveIn.size() != LiveTmp.size()) {
-      Data.LiveIn[BB] = LiveTmp;
-      Worklist.insert(pred_begin(BB), pred_end(BB));
-    }
-  } // while (!Worklist.empty())
-
-#ifndef NDEBUG
-  // Sanity check our output against SSA properties.  This helps catch any
-  // missing kills during the above iteration.
-  for (BasicBlock &BB : F)
-    checkBasicSSA(DT, Data, BB);
-#endif
-}
-
-static void findLiveSetAtInst(Instruction *Inst, GCPtrLivenessData &Data,
-                              StatepointLiveSetTy &Out) {
-  BasicBlock *BB = Inst->getParent();
-
-  // Note: The copy is intentional and required
-  assert(Data.LiveOut.count(BB));
-  SetVector<Value *> LiveOut = Data.LiveOut[BB];
-
-  // We want to handle the statepoint itself oddly.  It's
-  // call result is not live (normal), nor are it's arguments
-  // (unless they're used again later).  This adjustment is
-  // specifically what we need to relocate
-  computeLiveInValues(BB->rbegin(), ++Inst->getIterator().getReverse(),
-                      LiveOut);
-  LiveOut.remove(Inst);
-  Out.insert(LiveOut.begin(), LiveOut.end());
-}
-
-static void recomputeLiveInValues(GCPtrLivenessData &RevisedLivenessData,
-                                  CallSite CS,
-                                  PartiallyConstructedSafepointRecord &Info) {
-  Instruction *Inst = CS.getInstruction();
-  StatepointLiveSetTy Updated;
-  findLiveSetAtInst(Inst, RevisedLivenessData, Updated);
-
-  // We may have base pointers which are now live that weren't before.  We need
-  // to update the PointerToBase structure to reflect this.
-  for (auto V : Updated)
-    if (Info.PointerToBase.insert({V, V}).second) {
-      assert(isKnownBaseResult(V) &&
-             "Can't find base for unexpected live value!");
-      continue;
-    }
-
-#ifndef NDEBUG
-  for (auto V : Updated)
-    assert(Info.PointerToBase.count(V) &&
-           "Must be able to find base for live value!");
-#endif
-
-  // Remove any stale base mappings - this can happen since our liveness is
-  // more precise then the one inherent in the base pointer analysis.
-  DenseSet<Value *> ToErase;
-  for (auto KVPair : Info.PointerToBase)
-    if (!Updated.count(KVPair.first))
-      ToErase.insert(KVPair.first);
-
-  for (auto *V : ToErase)
-    Info.PointerToBase.erase(V);
-
-#ifndef NDEBUG
-  for (auto KVPair : Info.PointerToBase)
-    assert(Updated.count(KVPair.first) && "record for non-live value");
-#endif
-
-  Info.LiveSet = Updated;
-}
\ No newline at end of file
diff --git a/compiler/me/src/Foster/Base.hs b/compiler/me/src/Foster/Base.hs
--- a/compiler/me/src/Foster/Base.hs
+++ b/compiler/me/src/Foster/Base.hs
@@ -226,7 +226,6 @@
                             , dataCtorDTTyF :: [TypeFormal]
                             , dataCtorTypes :: [ty]
                             , dataCtorRepr  :: Maybe CtorRepr
-                            , dataCtorLone  :: Bool
                             , dataCtorRange :: SourceRange
                             }
 
@@ -249,7 +248,6 @@
 data LLCtorInfo ty = LLCtorInfo { ctorLLInfoId   :: CtorId
                                 , ctorLLInfoRepr :: CtorRepr
                                 , ctorLLInfoTys  :: [ty]
-                                , ctorLLInfoLone :: Bool -- Only one ctor?
                                 }
                      deriving (Show, Functor)
 
@@ -380,10 +378,6 @@
      }
 
 data ToplevelBinding ty = TopBindArray Ident ty [Literal]
-                        | TopBindLiteral Ident ty Literal
-                        | TopBindTuple   Ident ty [Ident]
-                        | TopBindAppCtor Ident ty (CtorId, CtorRepr) [Ident]
-
 -- }}}||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||
 -- ||||||||||||||||||||||| Source Ranges ||||||||||||||||||||||||{{{
 
@@ -572,7 +566,6 @@
 
 concatMapM f vs = do vs' <- mapM f vs ; return $ concat vs'
 
-mapMaybeM :: (Monad m) => (a -> m b) -> Maybe a -> m (Maybe b)
 mapMaybeM _ Nothing = return Nothing
 mapMaybeM f (Just x) = f x >>= return . Just
 
@@ -719,7 +712,6 @@
 prettyIdent i = text (show i)
 
 prettyId (TypedId _ i) = prettyIdent i
-prettyTypedId (TypedId t i) = prettyIdent i <> text " :: " <> pretty t
 
 -- Handler expressions pre-allocate the ids that will be bound for the `resume` functions
 -- during typechecking; they must be kept around to be used during handler compilation.
@@ -1116,8 +1108,7 @@
 instance Ord (LLCtorInfo ty) where
   compare = compareLLCtorInfo
 
-compareLLCtorInfo (LLCtorInfo c1 r1 _ o1) (LLCtorInfo c2 r2 _ o2) =
-  compare (c1, r1, o1) (c2, r2, o2)
+compareLLCtorInfo (LLCtorInfo c1 r1 _) (LLCtorInfo c2 r2 _) = compare (c1, r1) (c2, r2)
 
 instance Eq  (LLCtorInfo ty) where
   c1 == c2 = compare c1 c2 == EQ
diff --git a/compiler/me/src/Foster/CapnpIL.hs b/compiler/me/src/Foster/CapnpIL.hs
--- a/compiler/me/src/Foster/CapnpIL.hs
+++ b/compiler/me/src/Foster/CapnpIL.hs
@@ -447,7 +447,7 @@
 -- }}}||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||
 
 -- ||||||||||||||||||||| Other Expressions ||||||||||||||||||||||{{{
-dumpCtorInfo (LLCtorInfo cid repr tys _) =
+dumpCtorInfo (LLCtorInfo cid repr tys) =
     PbCtorInfo {
           ctorid_of_PbCtorInfo = dumpCtorIdWithRepr "dumpCtorInfo" (cid, repr)
         , ctorstructty_of_PbCtorInfo =  if null tys
@@ -549,12 +549,6 @@
                              then StrictlyJust $ dumpType t
                              else StrictlyNone
     }
-dumpVarIdent i =
-    TermVar {
-          tag_of_TermVar  = Ilvar
-        , name_of_TermVar = dumpIdent i
-        , typ_of_TermVar  = StrictlyNone
-    }
 -- }}}||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||
 
 dumpILProgramToCapnp :: ILProgram -> FilePath -> IO ()
@@ -592,43 +586,12 @@
     dumpItem (TopBindArray id _ty@(LLArrayType ety) lits) =
       PbToplevelItem {
           name_of_PbToplevelItem = dumpIdent id
-        , arr_of_PbToplevelItem = StrictlyJust $ pbArrayLit ety (map Left lits)
-        , lit_of_PbToplevelItem = StrictlyNone
+        , arr_of_PbToplevelItem = pbArrayLit ety (map Left lits)
         }
 
     dumpItem (TopBindArray _id otherty _lits) =
         error $ "dumpItem saw top-level array with non-array type " ++ show otherty
 
-    dumpItem (TopBindLiteral id ty lit) =
-      PbToplevelItem {
-          name_of_PbToplevelItem = dumpIdent id
-        , lit_of_PbToplevelItem = StrictlyJust $ dumpLiteral ty lit
-        , arr_of_PbToplevelItem = StrictlyNone
-        }
-
-    dumpItem (TopBindTuple id refty@(LLPtrType unboxedty) ids) =
-      PbToplevelItem {
-          name_of_PbToplevelItem = dumpIdent id
-        , lit_of_PbToplevelItem = StrictlyJust $
-              (defaultLetable unboxedty Ilunboxedtuple) { parts_of_Letable = map dumpVarIdent ids }
-              -- hacky detail: we use the presence of the type field to mean "static/global"
-        , arr_of_PbToplevelItem = StrictlyNone
-        }
-
-    dumpItem (TopBindAppCtor id ty (cid, repr) ids) =
-      PbToplevelItem {
-          name_of_PbToplevelItem = dumpIdent id
-        , lit_of_PbToplevelItem = StrictlyJust $
-              (defaultLetable ty Ilglobalappctor) { parts_of_Letable = map dumpVarIdent ids
-                                                  , ctorinfo_of_Letable = StrictlyJust $ 
-                        PbCtorInfo {
-                              ctorid_of_PbCtorInfo = dumpCtorIdWithRepr "dumpCtorInfo" (cid, repr)
-                            , ctorstructty_of_PbCtorInfo = StrictlyNone
-                        }
-              }
-        , arr_of_PbToplevelItem = StrictlyNone
-      }
-
     dumpDataTypeDecl :: DataType TypeLL -> Decl
     dumpDataTypeDecl datatype =
         let formal = dataTypeName datatype in
@@ -669,15 +632,15 @@
             , ctor_of_Type_ = map dumpDataCtor ctors
         }
      where
-        dumpDataCtor (DataCtor ctorName _tyformals types _repr _lone _range) =
+        dumpDataCtor (DataCtor ctorName _tyformals types _repr _range) =
           PbDataCtor { name_of_PbDataCtor = u8fromText ctorName
                      , type_of_PbDataCtor = map dumpType types
                      }
 
-    dumpDataType (TypeFormal _dtName _sr KindAnySizeType) [DataCtor _nm [] [ty] _repr _lone _range] =
+    dumpDataType (TypeFormal _dtName _sr KindAnySizeType) [DataCtor _nm [] [ty] _repr _range] =
         dumpType ty
 
-    dumpDataType (TypeFormal _dtName _sr KindAnySizeType) [DataCtor _nm []  tys _repr _lone _range] =
+    dumpDataType (TypeFormal _dtName _sr KindAnySizeType) [DataCtor _nm []  tys _repr _range] =
         dumpType (LLStructType tys)
 
     dumpDataType (TypeFormal dtName _sr kind) ctors =
diff --git a/compiler/me/src/Foster/CloConv.hs b/compiler/me/src/Foster/CloConv.hs
--- a/compiler/me/src/Foster/CloConv.hs
+++ b/compiler/me/src/Foster/CloConv.hs
@@ -122,16 +122,8 @@
 
 closureConvertToplevel :: PreCloConv -> ILM ()
 closureConvertToplevel (PreCloConv (cffns, topbinds)) = do
-  let recordTopItem (TopBindArray id ty lits) =
-          recordGlobalVal (TopBindArray id (monoToLL ty) lits)
-      recordTopItem (TopBindLiteral id ty lit) =
-          recordGlobalVal (TopBindLiteral id (monoToLL ty) lit)
-      recordTopItem (TopBindTuple id ty ids) =
-          recordGlobalVal (TopBindTuple id (monoToLL ty) ids)
-      recordTopItem (TopBindAppCtor id ty cidrep ids) =
-          recordGlobalVal (TopBindAppCtor id (monoToLL ty) cidrep ids)
-
-  mapM_ recordTopItem topbinds
+  mapM_ (\(TopBindArray id ty lits) ->
+      recordGlobalVal (TopBindArray id (monoToLL ty) lits)) topbinds
   mapM_ (lambdaLift []) cffns
 
 recordGlobalVal thing = do
diff --git a/compiler/me/src/Foster/Context.hs b/compiler/me/src/Foster/Context.hs
--- a/compiler/me/src/Foster/Context.hs
+++ b/compiler/me/src/Foster/Context.hs
@@ -109,8 +109,8 @@
   liftM (\cs' ->DataType nm formals cs'   isForeign srcrange) (mapM (liftDataCtor f) ctors)
 
 liftDataCtor :: Monad m => (t1 -> m t2) -> DataCtor t1 -> m (DataCtor t2)
-liftDataCtor f (DataCtor dataCtorName formals types repr lone range) = do
-  liftM (\tys-> DataCtor dataCtorName formals tys   repr lone range) (mapM f types)
+liftDataCtor f (DataCtor dataCtorName formals types repr range) = do
+  liftM (\tys-> DataCtor dataCtorName formals tys   repr range) (mapM f types)
 
 liftPrimOp f primop =
   case primop of
diff --git a/compiler/me/src/Foster/ConvertExprAST.hs b/compiler/me/src/Foster/ConvertExprAST.hs
--- a/compiler/me/src/Foster/ConvertExprAST.hs
+++ b/compiler/me/src/Foster/ConvertExprAST.hs
@@ -37,9 +37,9 @@
   cts <- mapM (convertDataCtor f) ctors
   return $ DataType dtName tyformals cts isForeign range
 
-convertDataCtor f (DataCtor dataCtorName formals types repr lone range) = do
+convertDataCtor f (DataCtor dataCtorName formals types repr range) = do
   tys <- mapM f types
-  return $ DataCtor dataCtorName formals tys repr lone range
+  return $ DataCtor dataCtorName formals tys repr range
 
 convertEffect :: (Show a, Show b) =>
                    (a -> Tc b) -> EffectDecl a -> Tc (EffectDecl b)
diff --git a/compiler/me/src/Foster/FromHaskell.hs b/compiler/me/src/Foster/FromHaskell.hs
--- a/compiler/me/src/Foster/FromHaskell.hs
+++ b/compiler/me/src/Foster/FromHaskell.hs
@@ -272,7 +272,7 @@
         case d of
           H.TypeDecl _dh _ty ->
             appendFile fosterpath ("/* TODO(dumpDecl.TypeDecl):\n" ++ prettyPrint d ++ "\n*/\n\n")
-          H.DataDecl H.DataType Nothing dh qcdecls ders -> do
+          H.DataDecl H.DataType Nothing dh qcdecls mb_der -> do
             let (name, _tvbs) = parseDeclHead dh
             appendFile fosterpath $ "type case " ++ prettyPrint name ++ {- tyvars -} "\n"
             forM_ qcdecls $ \(H.QualConDecl _mb_tvbs _mb_ctx condecl) -> do
@@ -290,9 +290,9 @@
                    appendFile fosterpath "\n/* TODO: generate record accessor functions */\n"
                appendFile fosterpath "\n"
             appendFile fosterpath ";\n"
-            case ders of
-              [] -> return ()
-              _  -> appendFile fosterpath ("/* # derivings: " ++ show (length ders) ++ "*/\n")
+            case mb_der of
+              Nothing -> return ()
+              Just der -> appendFile fosterpath ("/* " ++ prettyPrint der ++ "*/\n")
 
           H.DataDecl dor mb_ctx dh qcdecls mb_der ->
             appendFile fosterpath ("/* TODO(dumpDecl.DataDecl):\n" ++ prettyPrint d ++ "*/\n")
diff --git a/compiler/me/src/Foster/KNExpr.hs b/compiler/me/src/Foster/KNExpr.hs
--- a/compiler/me/src/Foster/KNExpr.hs
+++ b/compiler/me/src/Foster/KNExpr.hs
@@ -424,12 +424,11 @@
             P_Or    rng ty pats -> do pats' <- mapM qp pats
                                       ty'   <- qt ty
                                       return $ PR_Or rng ty' pats'
-            P_Ctor  rng ty pats (CtorInfo cid dc) -> do
+            P_Ctor  rng ty pats (CtorInfo cid _dc) -> do
                         pats' <- mapM qp pats
                         ty'   <- qt ty
                         let patTys = map typeOf pats'
-                        let cinfo' = LLCtorInfo cid (lookupCtorRepr (lookupCtor cid))
-                                                patTys (dataCtorLone dc)
+                        let cinfo' = LLCtorInfo cid (lookupCtorRepr (lookupCtor cid)) patTys
                         return $ PR_Ctor rng ty' pats' cinfo'
 
         ilPrim :: FosterPrim TypeTC -> KN (FosterPrim TypeIL)
@@ -508,9 +507,8 @@
         case mty of
           Unbound _-> if mtvIsEffect m
                        then return (TyAppIL (TyConIL "effect.Empty") [])
-                       else do {-tcWarn [text $ "Found un-unified unification variable "
-                                ++ show (mtvUniq m) ++ "(" ++ mtvDesc m ++ ")!"]-}
-                               return $ TupleTypeIL KindPointerSized []
+                       else tcFails [text $ "Found un-unified unification variable "
+                                ++ show (mtvUniq m) ++ "(" ++ mtvDesc m ++ ")!"]
           BoundTo t -> let t' = shallowStripRefinedTypeTC t in
                        -- TODO: strip refinements deeply 
                        q t'
@@ -543,9 +541,9 @@
       where mapping = [(BoundTyVar nm rng, ty)
                       | (ty, TypeFormal nm rng _kind) <- zip tys formals]
 
-convertDataCtor f (DataCtor dataCtorName tyformals types repr lone range) = do
+convertDataCtor f (DataCtor dataCtorName tyformals types repr range) = do
   tys <- mapM f types
-  return $ DataCtor dataCtorName tyformals tys repr lone range
+  return $ DataCtor dataCtorName tyformals tys repr range
 
 convertED :: KNState -> EffectDecl TypeTC -> KN (EffectDecl TypeIL)
 convertED st (EffectDecl name formals effctors range) = do
@@ -987,9 +985,9 @@
   where
     kNormalCtor :: DataType TypeIL -> DataCtor TypeIL
                 -> KN (FnExprIL)
-    kNormalCtor _datatype (DataCtor _cname _tyformals _tys Nothing _lone _range) = do
+    kNormalCtor _datatype (DataCtor _cname _tyformals _tys Nothing _range) = do
       error "Cannot wrap a data constructor with no representation information."
-    kNormalCtor datatype (DataCtor cname _tyformals tys (Just repr) _lone range) = do
+    kNormalCtor datatype (DataCtor cname _tyformals tys (Just repr) range) = do
       let dname = dataTypeName datatype
       let arity = Prelude.length tys
       let cid   = CtorId (typeFormalName dname) (T.unpack cname) arity
@@ -1041,7 +1039,7 @@
 kNormalEffectWrappers st ed = map kNormalEffectWrapper (zip [0..] (effectDeclCtors ed))
   where
     kNormalEffectWrapper :: (Int, EffectCtor TypeIL) -> KN FnExprIL
-    kNormalEffectWrapper (n, EffectCtor (DataCtor cname tyformals tys _repr _lone range) outty) = do
+    kNormalEffectWrapper (n, EffectCtor (DataCtor cname tyformals tys _repr range) outty) = do
       let dname = effectDeclName ed
       let arity = Prelude.length tys
       let cid   = CtorId (typeFormalName dname) (T.unpack cname) arity
diff --git a/compiler/me/src/Foster/KSmallstep.hs b/compiler/me/src/Foster/KSmallstep.hs
--- a/compiler/me/src/Foster/KSmallstep.hs
+++ b/compiler/me/src/Foster/KSmallstep.hs
@@ -594,7 +594,7 @@
     (SSBool b1, PR_Atom (P_Bool _ _ b2)) -> matchIf $ b1 == b2
     (_        , PR_Atom (P_Bool _ _ _ )) -> matchFailure
 
-    (SSCtorVal vid vals, PR_Ctor _ _ pats (LLCtorInfo cid _ _ _)) -> do
+    (SSCtorVal vid vals, PR_Ctor _ _ pats (LLCtorInfo cid _ _)) -> do
                                             _ <- matchIf $ vid == cid
                                             matchPatterns pats vals
     (_                 , PR_Ctor _ _ _ _) -> matchFailure
diff --git a/compiler/me/src/Foster/MKNExpr.hs b/compiler/me/src/Foster/MKNExpr.hs
--- a/compiler/me/src/Foster/MKNExpr.hs
+++ b/compiler/me/src/Foster/MKNExpr.hs
@@ -18,7 +18,7 @@
 import Foster.Kind
 
 import Control.Monad(liftM)
-import Control.Monad.State(gets, get, put, lift, liftIO, forM_,
+import Control.Monad.State(gets, get, put, lift, liftIO,
                            StateT, evalStateT, execStateT, runStateT)
 import Data.IORef(IORef, readIORef, newIORef, writeIORef)
 import Data.UnionFind.IO
@@ -28,8 +28,8 @@
 import qualified Data.Set as Set(toList, fromList)
 import qualified Data.Map as Map
 import Data.Map(Map)
-import qualified Data.List as List(foldl', reverse, all, any)
-import Data.Maybe(catMaybes, isJust, isNothing, fromJust)
+import qualified Data.List as List(foldl', reverse)
+import Data.Maybe(catMaybes, isJust, isNothing)
 import Data.Either(partitionEithers)
 
 import Compiler.Hoopl(UniqueMonad(..), C, O, freshLabel, intToUnique,
@@ -38,6 +38,8 @@
 import Prelude hiding ((<$>))
 import Text.PrettyPrint.ANSI.Leijen
 
+import Debug.Trace(trace)
+
 -- Binding occurrences of variables, with link to a free occurrence (if not dead).
 data MKBound x = MKBound (TypedId x) (OrdRef (Maybe (FreeOcc x)))
 
@@ -80,15 +82,12 @@
 boundOcc :: MKBound t -> Compiled (Maybe (FreeOcc t))
 boundOcc (MKBound _ r) = readOrdRef r
 
-boundUniq :: MKBound t -> Uniq
-boundUniq (MKBound _ r) = ordRefUniq r
-
 {- Given a graph like this:
       b1 ----> f1       x1 <---- b2
               / \       |
             f2---f3     x2
 
-   substVarForBound f3 b2  will produce
+   substVarForBound f3 b  will produce
 
       b1 ----> f1------x1    \--- b2
               /        |
@@ -112,14 +111,11 @@
 substVarForVar' :: Point (MKBound t) -> Point (MKBound t) -> Compiled ()
 substVarForVar' px py | px == py = do return ()
 substVarForVar' px py = do
-  bx <- liftIO $ descriptor px
-  by <- liftIO $ descriptor py
-  substBinders bx by
-  py `nowPointsTo` px where nowPointsTo x y = liftIO $ union x y
-
-substBinders (MKBound _ b'x) (MKBound _ b'y) = do
+  MKBound _ b'x <- liftIO $ descriptor px
+  MKBound _ b'y <- liftIO $ descriptor py
   mergeFreeLists b'x b'y
   writeOrdRef b'y Nothing
+  py `nowPointsTo` px where nowPointsTo x y = liftIO $ union x y
 
 substVarForVar'' :: Show t => MKBound t -> MKBound t -> Compiled ()
 substVarForVar'' bx by = do
@@ -130,13 +126,9 @@
       ccWhen ccVerbose $ do
         dbgDoc $ text $ "substVarForVar'' " ++ show (boundVar bx) ++ "  " ++ show (boundVar by)
       substVarForVar fx fy
-
-    (Just _x, Nothing)               -> do substBinders bx by
-    (Nothing, Just (DLCNode py _ _)) -> do substBinders bx by; liftIO $ setDescriptor (fopPoint py) bx
-
-    (Nothing, Nothing) -> do
+    _ -> do
       ccWhen ccVerbose $ do
-        dbgDoc $ text $ "substVarForVar'' doing nothing; both binders are dead"
+        dbgDoc $ text $ "substVarForVar'' doing nothing; one or both binders are dead"
       return ()
 
 mergeFreeLists :: OrdRef (Maybe (FreeOcc t)) -> OrdRef (Maybe (FreeOcc t)) -> Compiled ()
@@ -298,45 +290,40 @@
 -- we'll examine the uplink, which points to the MKLetCont; from there,
 -- we'll harvest linkCF and linkCC. We can do simple identity comparisons
 -- of the uplinks in the linked terms. Thus from linkCB we update linkCC.
---
--- Inlining can revisit dead terms. One example of how this can happen is
--- that (1) a singleton fn is optimized, passing a statically known argument.
--- After substitution, uses of the fn's formal are (potentially) redexes.
--- Thus we enqueue the uses for further processing; note specifically that
--- this involves the ``freeLink`` of the formal's occurrence. In step (3),
--- before visiting the occurrence's link, the same call using the formal is
--- inlined, but via a different link -- the one from the call's parent term.
--- In step (4), we finally re-visit the call enqueued in step 2. At this point,
--- the call is dead, meaning its parent now points to the inlined code instead.
 
-data ActiveLinkStatus ty = ActiveSubterm (Subterm ty)
-                         | TermIsDead
-
-getActiveLinkFor :: Pretty ty => MKTerm ty -> Compiled (ActiveLinkStatus ty)
+getActiveLinkFor :: MKTerm ty -> Compiled (Subterm ty)
 getActiveLinkFor term = do
   let isLinkToOurTerm link = do
         mb_term' <- readOrdRef link
         case mb_term' of
-          Nothing -> do
-            liftIO $ putStrLn $ "no term for link #" ++ show (ordRefUniq link)
-            return False
-          Just term' -> do
-            return $ parentLinkT term' == parentLinkT term
+          Nothing -> return False
+          Just term' -> return $ parentLinkT term' == parentLinkT term
 
   parent <- readLink "linkFor" (parentLinkT term)
   case parent of
     ParentFn fn -> do
       good <- isLinkToOurTerm (mkfnBody fn)
       if good
-        then return $ ActiveSubterm $ mkfnBody fn
-        else return $ TermIsDead
+        then return $ mkfnBody fn
+        else error $ "linkFor: body of parent fn wasn't equal to our term!"
     ParentTerm p -> do
       siblings <- subtermsOf p
       goods <- mapM isLinkToOurTerm siblings
       case [sib | (True, sib) <- zip goods siblings] of
-        [] -> return TermIsDead
-        [x] -> return $ ActiveSubterm x
-        _ -> error $ "linkFor found multiple candidates among the siblings!"
+        [] -> error "linkFor didn't find our candidate among the siblings"
+        [x] -> return x
+        candidates -> do
+          knT <- knOfMK NoCont term
+          knP <- knOfMK NoCont p
+          cands <- mapM (\s -> do tm <- readLink "sib" s ; knOfMK NoCont tm) candidates
+          compiledThrowE [text "linkFor found multiple candidates among the siblings!",
+                                 text "parent:",
+                                 indent 10 $ pretty knP,
+                                 text "term:",
+                                 indent 10 $ pretty knT,
+                                 text "candidates:",
+                                 indent 10 $ vsep (map (\s -> text "***" <> pretty s) cands)]
+                                 
 
 subtermsOf :: MKTerm t -> Compiled [Subterm t]
 subtermsOf term =
@@ -351,7 +338,7 @@
       MKCase        _u _ _v arms   -> do return $ map mkcaseArmBody arms
       MKCont {} -> return []
       MKCall {} -> return []
-      MKRelocDoms   _u _vs k -> return $ [k]
+      MKRelocDoms   _u _ids k -> return $ [k]
 
 type Uplink ty = Link (Parent ty)
 data Parent ty = ParentTerm (MKTerm ty)
@@ -384,7 +371,7 @@
         | MKLetRec      (Uplink ty) [Known ty   (Subterm ty)] (Subterm ty)
         | MKLetFuns     (Uplink ty) [Known ty (Link (MKFn (Subterm ty) ty))] (Subterm ty)
         | MKLetCont     (Uplink ty) [Known ty (Link (MKFn (Subterm ty) ty))] (Subterm ty)
-        | MKRelocDoms   (Uplink ty) [FreeVar ty] (Subterm ty)
+        | MKRelocDoms   (Uplink ty) [Ident] (Subterm ty)
 
         -- Control flow
         | MKCase        (Uplink ty) ty (FreeVar ty) [MKCaseArm (Subterm ty) ty]
@@ -415,7 +402,7 @@
 -- In the course of processing, each subterm gets an empty uplink.
 -- Finally, backpatch the result rv into the subterms' uplinks.
 
-mkBackpatch' :: (ty ~ MonoType) => --(CanMakeFun ty, Pretty ty) =>
+mkBackpatch' :: (CanMakeFun ty, Pretty ty) =>
                 [KNExpr' RecStatus ty]
              -> ContinuationContext ty
              -> ([Subterm ty] -> WithBinders ty (MKTerm ty))
@@ -454,12 +441,10 @@
                       ++ "\n; m = " ++ show [(k, tidIdent (boundVar v)) | (k,v) <- Map.toList m]
 
 mkFreeOcc :: TypedId ty -> WithBinders ty (FreeVar ty)
-mkFreeOcc tid = mkFreeOcc' (tidIdent tid)
-
-mkFreeOcc' :: Ident -> WithBinders ty (FreeVar ty)
-mkFreeOcc' xid = do
+mkFreeOcc tid = do
     m <- get
-    let binder = findBinder xid m
+    let xid = tidIdent tid
+    let binder = findBinder ({-trace ("mkFreeOcc looking up " ++ show xid)-} xid) m
     lift $ mkFreeOccForBinder binder
 
 mkFreeOccForBinder :: MKBoundVar t -> Compiled (FreeOcc t)
@@ -511,36 +496,24 @@
   mk <- readLink "backpatchFn" (mkfnBody f)
   writeOrdRef (parentLinkT mk) (Just (ParentFn f))
 
-unsafeForceCont id = T.pack "forcecont_" `T.isPrefixOf` (identPrefix id)
-
-mkOfKNFn :: (ty ~ MonoType) =>
-            Maybe (ContinuationContext ty)
-         -> (Ident, Fn RecStatus (KNExpr' RecStatus ty) ty)
-         -> WithBinders ty (Bool, MKFn (Subterm ty) ty)
-
-mkOfKNFn mb_k (localname, Fn v vs expr isrec annot) = do
+mkOfKNFn :: (CanMakeFun ty, Pretty ty) =>
+            Fn RecStatus (KNExpr' RecStatus ty) ty
+         -> WithBinders ty (MKFn (Subterm ty) ty)
+         
+mkOfKNFn (Fn v vs expr isrec annot) = do
     m <- get
     v' <- mkBinder v
     vs' <- mapM mkBinder vs
-
-    case (mb_k, unsafeForceCont localname) of
-      (Just k, True) -> do
-        expr' <- mkOfKN_Base expr k -- TODO maybe need to separately track ret k?
-        put m
-        let f' = MKFn v' vs' Nothing expr' isrec annot
-        lift $ backpatchFn f'
-        return (False, f')
+    
+    jb  <- genBinder ".fret" (tidType v)
+    lift $ ccWhen ccVerbose $ do
+      dbgDoc $ text "Generated return continuation " <> pretty (tidIdent $ boundVar jb) <> text " for fn " <> pretty (tidIdent v)
 
-      _ -> do
-        jb  <- genBinder ".fret" (TupleType []) -- type is ignored
-        lift $ ccWhen ccVerbose $ do
-          dbgDoc $ text "Generated return continuation " <> pretty (tidIdent $ boundVar jb) <> text " for fn " <> pretty (tidIdent v)
-
-        expr' <- mkOfKN_Base expr (CC_Tail jb)
-        put m
-        let f' = MKFn v' vs' (Just jb) expr' isrec annot
-        lift $ backpatchFn f'
-        return (True, f')
+    expr' <- mkOfKN_Base expr (CC_Tail jb)
+    put m
+    let f' = MKFn v' vs' (Just jb) expr' isrec annot
+    lift $ backpatchFn f'
+    return f'
 
 data ContinuationContext ty =
       CC_Tail (MKBoundVar ty)
@@ -555,17 +528,17 @@
         selfLink2   <- lift $ newOrdRef $ Just tm
         lift $ setFreeLink v' tm
         lift $ setFreeLink cv tm
-
+        
         return selfLink2
 contApply (CC_Base (fn, _)) v' = fn v'
 
 mkOfKNMod kn mainBinder = do
   lift $ whenDumpIR "mono-structure" $ do
-    liftIO $ putDocLn $ pretty kn
-    liftIO $ putDocLn $ showStructure kn
+    dbgDoc $ pretty kn
+    dbgDoc $ showStructure kn
   mkOfKN_Base kn (CC_Tail mainBinder)
 
-mkOfKN_Base :: (ty ~ MonoType) =>
+mkOfKN_Base :: (CanMakeFun ty, Pretty ty) =>
                KNExpr' RecStatus ty ->
                ContinuationContext ty ->
                 WithBinders ty (Subterm ty)
@@ -609,13 +582,13 @@
             let rv = MKLetVal nu (mkKnownE xb selfLink2) subterm
             lift $ backpatchE rv [selfLink2]
             lift $ backpatchT rv [subterm]
-
+        
         KNLetVal      x e1 e2 -> do
             -- The 'let val' case from CwCC figure 8.
             -- Generate the continuation variable 'j'.
             jb  <- genBinder ".cont" (mkFunType [typeKN e1] (typeKN e2))
             jbx <- genBinder ".cntx" (mkFunType [typeKN e1] (typeKN e2))
-
+            
             -- Generate the continuation's bound parameter, 'x'
             xb <- mkBinder $ TypedId (typeKN e1) x
 
@@ -631,10 +604,9 @@
             let rv = MKLetCont nu [known] rest'
             lift $ backpatchT rv [rest']
 
-        KNRelocDoms ids e -> do
-          vs <- mapM mkFreeOcc' ids
+        KNRelocDoms ids e ->
           mkBackpatch' [e] k (\[e'] -> do
-            return $ MKRelocDoms nu vs e')
+            return $ MKRelocDoms nu ids e')
 
         KNCompiles (KNCompilesResult r) ty _expr -> do 
             genMKLetVal ".cpi" ty $ \nu' -> do
@@ -667,15 +639,10 @@
             (v':vs') <- qvs (v:vs)
             case k of
                 CC_Tail jb -> do
-                  if unsafeForceCont (tidIdent v)
-                    then do
-                      return $ MKCont nu  ty v' vs'
-                    else do
-                      kv <- lift $ mkFreeOccForBinder jb
-                      return $ MKCall nu  ty v' vs' kv
+                  kv <- lift $ mkFreeOccForBinder jb
+                  return $ MKCall nu  ty v' vs' kv
 
                 CC_Base kf -> do
-                  liftIO $ putDocLn $ text "saw non-tail call of " <> pretty v
                   genContinuation ".clco" ".clcx" ty kf nu $ \nu' jb -> do
                       kv <- lift $ mkFreeOccForBinder jb
                       return $ MKCall nu'  ty v' vs' kv
@@ -683,10 +650,10 @@
         KNLetRec  xs es rest -> do 
             let vs = map (\(x,e) -> (TypedId (typeKN e) x)) (zip xs es)
             m1 <- get
-            do dbgDoc $ text "m1: " <> pretty (Map.toList m1)
+            dbgDoc $ text "m1: " <> pretty (Map.toList m1)
             xs' <- mapM mkBinder vs
-            do m2 <- get
-               dbgDoc $ text "m2: " <> pretty (Map.toList m2)
+            m2 <- get
+            dbgDoc $ text "m2: " <> pretty (Map.toList m2)
             --put $ extend m (map tidIdent vs) xs'
             -- TODO reconsider k
             ts <- mapM (\e -> mkOfKN_Base e k) es
@@ -700,20 +667,11 @@
             let vs = map (\(x,fn) -> (TypedId (fnType fn) x)) (zip ids fns)
             m <- get
             binders <- mapM mkBinder vs
-            fcfs' <- mapM (mkOfKNFn (Just k)) (zip ids fns)
+            fs'   <- mapM mkOfKNFn fns
             rest' <- mkOfKN_Base st k
-            fknowns <- lift $ mapM (uncurry mkKnown') (zip binders [f | (True,  f) <- fcfs'])
-            cknowns <- lift $ mapM (uncurry mkKnown') (zip binders [f | (False, f) <- fcfs'])
-
-            crest <- do
-              if null cknowns then return rest'
-                else do
-                  nu'  <- lift $ newOrdRef Nothing
-                  cfres <- lift $ backpatchT (MKLetCont nu' cknowns rest') [rest']
-                  lift $ do selfLink <- newOrdRef Nothing
-                            installLinks selfLink cfres
+            knowns <- lift $ mapM (uncurry mkKnown') (zip binders fs')
             put m
-            lift $ backpatchT (MKLetFuns nu fknowns crest) [crest]
+            lift $ backpatchT (MKLetFuns nu knowns rest') [rest']
 
         e | Just (bindName, gen) <- isExprNotTerm e -> do
             genMKLetVal bindName (typeKN e) gen
@@ -825,24 +783,6 @@
     let rv = MKLetCont nu [known] rest'
     lift $ backpatchT rv [rest']
 
-canRemoveIfDead :: MKExpr ty -> Bool
-canRemoveIfDead expr =
-  case expr of
-    MKLiteral     {} -> True
-    MKTuple       {} -> True
-    MKKillProcess {} -> False
-    MKCallPrim    {} -> False -- TODO refine
-    MKAppCtor     {} -> True
-    MKAlloc       {} -> True
-    MKDeref       {} -> True
-    MKStore       {} -> False
-    MKAllocArray  {} -> True
-    MKArrayRead   {} -> True
-    MKArrayPoke   {} -> False
-    MKArrayLit    {} -> True
-    MKCompiles    {} -> True
-    MKTyApp       {} -> True
-    
 parentLinkE :: MKExpr ty -> Uplink ty
 parentLinkE expr =
   case expr of
@@ -868,7 +808,7 @@
     MKLetRec      u   _knowns _k -> u
     MKLetFuns     u   _knowns _k -> u
     MKLetCont     u   _knowns _k -> u
-    MKRelocDoms   u _vs _k      -> u
+    MKRelocDoms   u _ids _k      -> u
     MKCase        u  _ty _ _arms  -> u
     MKIf          u  _ty _ _e1 _e2 -> u
     MKCall        u     _ty _ _s _   -> u
@@ -983,7 +923,7 @@
   let qf = knOfMKFn mb_retCont
 
   case term0 of
-    MKRelocDoms   _u _vs k -> q k
+    MKRelocDoms   _u _ids k -> q k
     MKIf          _u  ty v e1 e2  -> do e1' <- q e1
                                         e2' <- q e2
                                         v'  <- qv v
@@ -1073,9 +1013,10 @@
     case mb_term of
       Nothing -> return ()
       Just term -> do
+        let markRedex = liftIO $ modIORef' ref (\w -> worklistAdd w subterm)
         case term of
-          MKCall _ _ fo _ _ -> whenNotM (isMainFn fo) (markRedex subterm)
-          MKCont {}         -> markRedex subterm
+          MKCall _ _ fo _ _ -> whenNotM (isMainFn fo) markRedex
+          MKCont {}         -> markRedex
           _ -> markAndFindSubtermsOf term >>= mapM_ go
           where markAndFindSubtermsOf term =
                     case term of
@@ -1085,21 +1026,18 @@
                                                          return [k]
                       MKLetRec      _u   knowns k  -> do mapM_ markValBind knowns
                                                          return $ k : (map snd knowns)
-                      MKLetFuns     _u   knowns k  -> do markRedex subterm
+                      MKLetFuns     _u   knowns k  -> do liftIO $ modIORef' ref (\w -> worklistAdd w subterm) -- markRedex
                                                          mapM_ (markFunBind subterm) knowns
                                                          fns <- knownActuals knowns
                                                          return $ k : map mkfnBody fns
                       MKLetCont     _u   knowns k  -> do mapM_ markCntBind knowns
                                                          fns <- knownActuals knowns
                                                          return $ k : map mkfnBody fns
-                      MKCase        _u _ _v arms -> do markRedex subterm
-                                                       return $ map mkcaseArmBody arms
-                      MKRelocDoms _u  vs k -> do bvs <- mapM freeBinder vs
-                                                 let ids = map (tidIdent . boundVar) bvs
-                                                 liftIO $ modIORef' relocdomsref (\m -> Map.insert ids (term,k) m)
+                      MKCase        _u _ _v arms -> return $ map mkcaseArmBody arms
+                      MKRelocDoms _u ids k -> do liftIO $ modIORef' relocdomsref (\m -> Map.insert ids (term,k) m)
                                                  return [k]
                       _ -> return []
-   markRedex subterm  = liftIO $ modIORef' ref     (\w -> worklistAdd w subterm)
+
    markValBind (x,tm) = liftIO $ modIORef' valbindsref (\m -> Map.insert x tm m)
    markCntBind (x,fn) = liftIO $ modIORef' funbindsref (\m -> Map.insert x fn m)
    markExpBind (x,exlink) = do
@@ -1122,14 +1060,13 @@
                         bc <- mkbCount x
                         fc <- dlcCount (mkfnVar mkfn)
                         ccWhen ccVerbose $ do
-                          dbgDoc $ text $ "markFnBind: x  = (" ++ show xc ++ " vs " ++ show bc ++ ") " ++ show (tidIdent $ boundVar x)
+                          dbgDoc $ text $ "markFnBind: x  = (" ++ show xc ++ " vs " ++ show bc ++ ") " ++ show (tidIdent $ boundVar x)                
                           dbgDoc $ text $ "            fv = (" ++ show fc ++ ") " ++ show (tidIdent $ boundVar (mkfnVar mkfn))
                         if xc == 0 && not (isTextPrim (tidIdent $ boundVar x))
                           then do
-                            dbgDoc $ text $ "markFunBind killing dead fn binding " ++ show (tidIdent $ boundVar x)
+                            -- dbgDoc $ text $ "killing dead fn binding " ++ show (tidIdent $ boundVar x)
                             writeOrdRef fn Nothing
                           else do
-                            dbgDoc $ text "adding fn ordref # " <> pretty (ordRefUniq fn) <+> text " :: " <> pretty (boundVar (mkfnVar mkfn))
                             liftIO $ modIORef' funbindsref (\m -> Map.insert x fn m)
                             liftIO $ modIORef' fundefsref  (\m -> Map.insert x subterm m)
 
@@ -1144,36 +1081,30 @@
   (T.pack "noinline_" `T.isInfixOf` identPrefix id
    && not (T.pack "." `T.isInfixOf` identPrefix id))
 
-flattenMaybe :: Maybe (Maybe a) -> Maybe a
-flattenMaybe Nothing = Nothing
-flattenMaybe (Just x) = x
 
 data RedexSituation t =
        CallOfUnknownFunction
-     | CallOfNonInlineableFunction (MKFn (Subterm t) t) (Link (MKFn (Subterm t) t))
      | CallOfSingletonFunction (MKFn (Subterm t) t)
      | CallOfDonatableFunction (MKFn (Subterm t) t)
-     | SomethingElse           (MKFn (Subterm t) t) (Link (MKFn (Subterm t) t))
+     | SomethingElse           (MKFn (Subterm t) t)
 
 classifyRedex :: (Pretty t)
               => FreeOcc t -> [FreeOcc t]
               -> Map (MKBoundVar t) (Link (MKFn (Subterm t) t))
               -> Map (MKBoundVar t) (MKBoundVar t)
-              -> Compiled (Bool, RedexSituation t)
+              -> Compiled (RedexSituation t)
 classifyRedex callee args knownFns aliases = do
   bv <- freeBinder callee
   let bv' = case Map.lookup bv aliases of
               Nothing -> bv
               Just z  -> z
-  let mb_link = Map.lookup bv' knownFns
-  mb_fn <- mapMaybeM readOrdRef mb_link >>= return . flattenMaybe
-  situation <- classifyRedex' bv' mb_fn mb_link args knownFns
-  return (bv /= bv', situation)
+  mb_fn <- lookupBinding' bv' knownFns
+  classifyRedex' bv' mb_fn args knownFns
 
-classifyRedex' _ Nothing _ _ _ = do
+classifyRedex' _ Nothing _ _ =
   return CallOfUnknownFunction
 
-classifyRedex' fnbinder (Just fn) (Just fnlink) args knownFns = do
+classifyRedex' fnbinder (Just fn) args knownFns = do
   callee_singleton <- binderIsSingletonOrDead fnbinder
   {-
   count <- mkbCount binder
@@ -1183,8 +1114,8 @@
 
   case (callee_singleton, mkfnIsRec fn) of
     _ | shouldNotInlineFn fn
-                   -> do return $ CallOfNonInlineableFunction fn fnlink
-    (True, NotRec) -> do return $ CallOfSingletonFunction fn
+                   -> return $ CallOfUnknownFunction
+    (True, NotRec) -> return $ CallOfSingletonFunction fn
     _ -> do
       donationss <- mapM (\(arg, binder) -> do
                          argsingle <- freeOccIsSingleton arg
@@ -1207,7 +1138,7 @@
                          ) (zip args (mkfnVars fn))
       let donations = concat donationss
       if null donations
-        then return $ SomethingElse fn fnlink
+        then return $ SomethingElse fn
         else return $ CallOfDonatableFunction fn
 -- }}}
 
@@ -1230,14 +1161,14 @@
   lift $ ccWhen ccVerbose $ do
     dbgDoc $ text $ "copied binder " ++ show (prettyIdent $ tidIdent $ boundVar b) ++ " (" ++ msg ++ ") into " ++ show newid
   return binder
-
-ccRefresh :: Ident -> Compiled Ident
-ccRefresh (Ident t _) = do
-    u <- ccUniq
-    return $ Ident t u
-ccRefresh (GlobalSymbol t alt) = do
-    u <- ccUniq
-    return $ GlobalSymbol (t `T.append` T.pack (show u)) alt
+ where
+    ccRefresh :: Ident -> Compiled Ident
+    ccRefresh (Ident t _) = do
+        u <- ccUniq
+        return $ Ident t u
+    ccRefresh (GlobalSymbol t alt) = do
+        u <- ccUniq
+        return $ GlobalSymbol (t `T.append` T.pack (show u)) alt
 
 copyFreeOcc :: FreeVar t -> WithBinders t (FreeVar t)
 copyFreeOcc fv = do
@@ -1362,10 +1293,9 @@
 
   -- TODO maybe have withLinkT use subtermsOf ?
   (link, newterm) <- case term of
-    MKRelocDoms   _u   vs     k   -> do k' <- q k
-                                        vs' <- mapM qv vs
+    MKRelocDoms   _u   ids    k   -> do k' <- q k
                                         withLinkT $ \u -> lift $ do
-                                          let rv = MKRelocDoms u vs' k'
+                                          let rv = MKRelocDoms u ids k'
                                           backpatchT rv [k']
     MKLetVal      _u   known  k   -> do x' <- qk qe known
                                         k' <- q k
@@ -1415,7 +1345,6 @@
     fr <- liftIO $ newIORef Map.empty
     fd <- liftIO $ newIORef Map.empty
     ar <- liftIO $ newIORef Map.empty
-    bindingWorklistRef <- liftIO $ newIORef worklistEmpty
     relocDomMarkers <- liftIO $ newIORef Map.empty
     --term <- readLink "mknInline" subterm
     collectRedexes wr kr er fr fd ar relocDomMarkers subterm
@@ -1430,28 +1359,6 @@
        dbgDoc $ text $ "collected " ++ show (length (Map.toList k0)) ++ " valbinds..."
        dbgDoc $ text $ "collected " ++ show (length (Map.toList e0)) ++ " expbinds..."
        dbgDoc $ text $ "collected " ++ show (length (Map.toList f0)) ++ " funbinds..."
-       
-    let processDeadBindings = do
-          w0 <- liftIO $ readIORef bindingWorklistRef
-          case worklistGet w0 of
-            Nothing -> do return ()
-            Just (bv, w') -> do
-              liftIO $ writeIORef bindingWorklistRef w'
-              expBindMap <- liftIO $ readIORef er
-              case Map.lookup bv expBindMap of
-                Nothing -> do dbgDoc $ text "unable to find expr for dead bound var " <> pretty bv
-                              return ()
-                Just exprLink -> do
-                    e <- readLink "processDeadBindings" exprLink
-                    let freeOccs = freeVarsE e
-                    if canRemoveIfDead e && not (null freeOccs)
-                      then do dbgDoc $ yellow (text "killing " <> pretty (length freeOccs) <>
-                                               text " occurrences in dead expr bound to ") <> red (pretty bv)
-                              kn <- knOfMKExpr NoCont e
-                              dbgDoc $ indent 8 (pretty kn)
-                              mapM_ (killOccurrence bindingWorklistRef) freeOccs
-                      else return ()
-              processDeadBindings
 
     let worklistGet' = do
           w0 <- liftIO $ readIORef wr
@@ -1465,22 +1372,26 @@
                 Just mredex -> do
                   parent <- readOrdRef (parentLinkT mredex)
                   return $ Just (subterm, mredex, parent)
-
-    origGas <- case mb_gas of
-                    Nothing -> return 42000
-                    Just gas -> do liftIO $ putStrLn $ "using gas: " ++ show gas
-                                   return gas
+              {-
+              parent <- readOrdRef (parentLinkT mredex)
+              mb_altself <- readOrdRef (selfLinkT mredex)
+              case mb_altself of
+                Nothing ->
+                  error $ "item in worklist had null self-link...?"
+                Just altself ->
+                  if selfLinkT altself /= selfLinkT mredex
+                    then
+                      error $ "altself not the same as mredex?!?"
+                    else
+                      return $ Just (mredex, parent)
+                      -}
 
     let go 0 = dbgDoc $ text "... ran outta gas"
 
         go gas = do
-           ccWhen ccVerbose $ do
-             liftIO $ putStrLn $ "gas: " ++ show gas ++ "; step: " ++ show (origGas - gas)
-
-           processDeadBindings
            mb_mredex_parent <- worklistGet'
            case mb_mredex_parent of
-             Nothing -> ccWhen ccVerbose $ do liftIO $ putDocLn $ text "... ran outta work"
+             Nothing -> dbgDoc $ text "... ran outta work"
              Just (_subterm, mredex, Nothing) -> do
                 case mredex of
                   MKLetFuns _u [(bv,_)] _ | tidIdent (boundVar bv) == GlobalSymbol (T.pack "TextFragment") NoRename ->
@@ -1490,459 +1401,299 @@
                        dbgDoc $ red (text "skipping parentless redex: ") <+> pretty redex
                   
                 go gas
-
-             Just (subterm, mredex, Just _parent) -> do
-              linkResult <- getActiveLinkFor mredex
-              case linkResult of
-                TermIsDead -> go (gas - 1)
-
-                ActiveSubterm link -> do
-                  let replaceActiveSubtermWith newthing =
-                        replaceWith bindingWorklistRef link subterm newthing
-
-                  case mredex of
-                    MKCall _up _ty callee args kv -> do
-                      knownFns   <- liftIO $ readIORef fr
-                      aliases    <- liftIO $ readIORef ar
-                      (peekedThroughBitcast, situation) <- classifyRedex callee args knownFns aliases
-                      case situation of
-                        CallOfNonInlineableFunction fn fnlink -> do
-                          if peekedThroughBitcast
-                            then return ()
-                            else considerFunctionForArityRaising er bindingWorklistRef fn fnlink callee
-
-                        CallOfUnknownFunction -> do
-                          do  redex <- knOfMK NoCont mredex
-                              dbgDoc $ text "CallOfUnknownFunction: " <+> pretty redex
-                          return ()
+             Just (subterm, mredex, Just _parent) -> case mredex of
+               MKCall _up _ty callee args kv -> do
+                 knownFns   <- liftIO $ readIORef fr
+                 aliases    <- liftIO $ readIORef ar
+                 situation <- classifyRedex callee args knownFns aliases
+                 case situation of
+                   CallOfUnknownFunction -> do
+                     do redex <- knOfMK NoCont mredex
+                        dbgDoc $ text "CallOfUnknownFunction: " <+> pretty redex
+                     return ()
+                   CallOfSingletonFunction fn -> do
+                     ccWhen ccVerbose $ do
+                        redex <- knOfMK (mbContOf $ mkfnCont fn) mredex
+                        dbgDoc $ text "CallOfSingletonFunction starting with: " <+> align (pretty redex)
+                        knfn <- knOfMKFn NoCont fn
+                        dbgDoc $ text "CallOfSingletonFunction fn is " <+> align (pretty knfn)
 
-                        CallOfSingletonFunction fn -> do
-                          ccWhen ccVerbose $ do
-                              redex <- knOfMK (mbContOf $ mkfnCont fn) mredex
-                              dbgDoc $ text "CallOfSingletonFunction starting with: " <+> align (pretty redex)
-
-                          ccWhen ccVerbose $ do
-                              v <- freeBinder callee
-                              dbgDoc $ green (text "inlining without copying ") <> pretty (tidIdent $ boundVar v)
-
-                          newbody <- betaReduceOnlyCall fn args kv     wr fd
+                     ccWhen ccVerbose $ do
+                        v <- freeBinder callee
+                        dbgDoc $ green (text "inlining without copying ") <> pretty (tidIdent $ boundVar v)
 
-                          --do nubody <- readLink "kninline-sf" newbody
-                          --   newbody' <- knOfMK NoCont nubody
-                          --   dbgDoc $ text "CallOfSingletonFunction generated: " <+> pretty newbody'
+                     newbody <- betaReduceOnlyCall fn args kv     wr fd
 
-                          dbgDoc $ text "Invoking `replaceWith` for CallOfSingletonFunction"
-                          replaceActiveSubtermWith newbody
-                          dbgDoc $ text "Killing callee for CallOfSingletonFunction"
-                          killBinding callee knownFns aliases
-                          -- No need to collect redexes, since the body wasn't duplicated.
+                     --do nubody <- readLink "kninline-sf" newbody
+                     --   newbody' <- knOfMK NoCont nubody
+                     --   dbgDoc $ text "CallOfSingletonFunction generated: " <+> pretty newbody'
 
-      {-
-                          case _parent of
-                                  ParentTerm pt -> do
-                                    kn <- knOfMK   (mbContOf $ mkfnCont fn) pt
-                                    dbgDoc $ text "CallOfSingletonFunction parent tm became: " <+> pretty kn
-                                  ParentFn   pf -> do
-                                    kn <- knOfMKFn NoCont pf
-                                    dbgDoc $ text "CallOfSingletonFunction parent fn became: " <+> pretty kn
+                     replaceWith subterm newbody
+                     killBinding callee knownFns aliases
+                     -- No need to collect redexes, since the body wasn't duplicated.
+
+{-
+                     case _parent of
+                            ParentTerm pt -> do
+                              kn <- knOfMK   (mbContOf $ mkfnCont fn) pt
+                              dbgDoc $ text "CallOfSingletonFunction parent tm became: " <+> pretty kn
+                            ParentFn   pf -> do
+                              kn <- knOfMKFn NoCont pf
+                              dbgDoc $ text "CallOfSingletonFunction parent fn became: " <+> pretty kn
+-}
+
+                   CallOfDonatableFunction fn -> do
+                     do redex <- knOfMK (mbContOf $ mkfnCont fn) mredex
+                        dbgDoc $ text "CallOfDonatableFunction: " <+> pretty redex
+                     flags <- gets ccFlagVals
+                     if getInliningDonate flags
+                       then do
+                         ccWhen ccVerbose $ do
+                            v <- freeBinder callee
+                            dbgDoc $ green (text "copying and inlining DF ") <+> pretty (tidIdent $ boundVar v)
+                            --kn1 <- knOfMKFn (mbContOf $ mkfnCont fn) fn
+                            --dbgDoc $ text $ "pre-copy fn is " ++ show (pretty kn1)
+                            return ()
+                         fn' <- runCopyMKFn fn Map.empty
+                         ccWhen ccVerbose $ do
+                            kn1 <- knOfMKFn (mbContOf $ mkfnCont fn) fn'
+                            dbgDoc $ text $ "post-copy fn is " ++ show (pretty kn1)
+                         -- TODO Recursive-but-not-tail-recursive functions (RBNTRF)
+                         --      will have a recursive call in the body, so we can't
+                         --      simply use betaReduceOnlyCall as theres more than 1 call.
+                         --
+                         --      Most functions will be given loop headers in KNExpr,
+                         --      but an un-eliminated loop header within a RBNTRF
+                         --      might change the allocation behavior of a program.
+                         --      
+                         --      If the generated fn' isn't singleton/dead, it should
+                         --      be inserted next to the original fn. (TODO)
+
+                         rbntr <- isRecursiveButNotTailRecursive fn'
+                         newbody <- if rbntr then do
+                           -- We don't modify the known function list, so the recursive call in
+                           -- the copied body will bottom out and not do any loop unrolling.
+                           
+                           dbgDoc $ red $ text "isRecursiveButNotTailRecursive!"
+                           -- We must disable recursive inlining or else we'd infinitely regress!
+                           
+                           knfn <- knOfMKFn NoCont $ fn'
+
+                           kn' <- knLoopHeaders' (KNLetFuns [tidIdent $ fnVar knfn] [knfn] (KNVar $ fnVar knfn))
+                                                 True
+                           let (KNLetFuns _ [knfn'] _) = kn'
+                           dbgDoc $ text $ "loop-headered fn is " ++ show (pretty knfn')
+
+                           fn'' <- evalStateT (mkOfKNFn knfn') $
+                            Map.fromList [(tidIdent $ boundVar b, b) | (b,_) <- Map.toList knownFns]
+
+                           -- We reuse the pieces of the original MKCall because it's now dead.
+                           createLetFunAndCall fn'' (mkfnVar fn'') _ty _up args kv
+                           
+                          else do betaReduceOnlyCall fn' args kv     wr fd
+                          
+                         replaceWith subterm newbody
+                         -- No need to kill the old binding, since the body was duplicated.
+
+                         collectRedexes wr kr er fr fd ar relocDomMarkers newbody
+
+                       else return ()
+
+                   SomethingElse _fn -> do
+                     do redex <- knOfMK (mbContOf $ mkfnCont _fn) mredex
+                        dbgDoc $ text "SomethingElse (inlineNorF): " <+> align (pretty redex)
+                     if shouldInlineRedex mredex _fn
+                       then do
+                             do v <- freeBinder callee
+                                dbgDoc $ green (text "copying and inlining SE ") <+> pretty (tidIdent $ boundVar v)
+                                --kn1 <- knOfMK (YesCont mainCont) term
+                                --dbgDoc $ text $ "knOfMK, term is " ++ show (pretty kn1)
+                             fn' <- runCopyMKFn _fn Map.empty
+                             newbody <- betaReduceOnlyCall fn' args kv    wr fd
+                             replaceWith subterm newbody
+                             killOccurrence callee
+                             collectRedexes wr kr er fr fd ar relocDomMarkers newbody
+                       else return ()
+                 go (gas - 1)
+              
+               MKCont _up _ty callee args -> do
+                 knownFns   <- liftIO $ readIORef fr
+                 aliases    <- liftIO $ readIORef ar
+                 situation <- classifyRedex callee args knownFns aliases
+                 case situation of
+                   CallOfUnknownFunction -> do
+                     do cb <- freeBinder callee
+                        if T.pack ".fret" `T.isPrefixOf` (identPrefix $ tidIdent $ boundVar cb) 
+                          then return ()
+                          else do redex <- knOfMK (YesCont mainCont) mredex
+                                  dbgDoc $ red (text "CallOfUnknownCont: ") <+> pretty redex
+                     return ()
+
+                   CallOfSingletonFunction fn -> do
+                     do redex <- knOfMK (mbContOf $ mkfnCont fn) mredex
+                        dbgDoc $ text "CallOfSingletonCont: " <+> pretty redex
+                    
+                        mapM_ (\arg -> do b <- freeBinder arg
+                                          c <- mkbCount b
+                                          dbgDoc $ text "      pre-beta occ count: " <> pretty c) args
+                                          {-
+      fob <- freeBinder fo
+      fo_c <- mkbCount fob
+      fx_c <- mkbCount b
+      dbgDoc $ text "substituting var " <> pretty (boundVar b) <> text " for " <> pretty (boundVar fob)
+      dbgDoc $ text "    occ lengths " <> pretty fx_c <> text " and " <> pretty fo_c
+
+      fo_c <- mkbCount fob
+      fx_c <- mkbCount b
+      fa <- freeBinder fox
+      dbgDoc $ text "    afteward, lengths " <> pretty fx_c <> text " and " <> pretty fo_c <> text "; fox -> " <> pretty (boundVar fa)
       -}
 
-                        CallOfDonatableFunction fn -> do
-                          do  redex <- knOfMK (mbContOf $ mkfnCont fn) mredex
-                              dbgDoc $ text "CallOfDonatableFunction: " <+> pretty redex
-                          flags <- gets ccFlagVals
-                          if getInliningDonate flags
-                            then do
-                              ccWhen ccVerbose $ do
-                                  v <- freeBinder callee
-                                  dbgDoc $ green (text "copying and inlining DF ") <+> pretty (tidIdent $ boundVar v)
-                                  --kn1 <- knOfMKFn (mbContOf $ mkfnCont fn) fn
-                                  --dbgDoc $ text $ "pre-copy fn is " ++ show (pretty kn1)
-                                  return ()
-                              fn' <- runCopyMKFn fn Map.empty
-                              ccWhen ccVerbose $ do
-                                  kn1 <- knOfMKFn (mbContOf $ mkfnCont fn) fn'
-                                  dbgDoc $ text $ "post-copy fn is " ++ show (pretty kn1)
-                              -- TODO Recursive-but-not-tail-recursive functions (RBNTRF)
-                              --      will have a recursive call in the body, so we can't
-                              --      simply use betaReduceOnlyCall as theres more than 1 call.
-                              --
-                              --      Most functions will be given loop headers in KNExpr,
-                              --      but an un-eliminated loop header within a RBNTRF
-                              --      might change the allocation behavior of a program.
-                              --      
-                              --      If the generated fn' isn't singleton/dead, it should
-                              --      be inserted next to the original fn. (TODO)
-
-                              rbntr <- isRecursiveButNotTailRecursive fn'
-                              newbody <- if rbntr then do
-                                            -- We don't modify the known function list, so the recursive call in
-                                            -- the copied body will bottom out and not do any loop unrolling.
-                                            
-                                            dbgDoc $ red $ text "isRecursiveButNotTailRecursive!"
-                                            -- We must disable recursive inlining or else we'd infinitely regress!
-                                            
-                                            knfn <- knOfMKFn NoCont $ fn'
-
-                                            kn' <- knLoopHeaders' (KNLetFuns [tidIdent $ fnVar knfn] [knfn] (KNVar $ fnVar knfn))
-                                                                  True
-                                            let (KNLetFuns [id'] [knfn'] _) = kn'
-                                            dbgDoc $ text $ "loop-headered fn is " ++ show (pretty knfn')
-
-                                            (_, fn'') <- evalStateT (mkOfKNFn Nothing (id' , knfn')) $
-                                              Map.fromList [(tidIdent $ boundVar b, b) | (b,_) <- Map.toList knownFns]
-
-                                            -- We reuse the pieces of the original MKCall because it's now dead.
-                                            createLetFunAndCall fn'' (mkfnVar fn'') _ty _up args kv
-                                          
-                                          else do betaReduceOnlyCall fn' args kv     wr fd
-                                
-                              replaceActiveSubtermWith newbody
-                              -- No need to kill the old binding, since the body was duplicated.
-
-                              collectRedexes wr kr er fr fd ar relocDomMarkers newbody
-
-                            else return ()
+                     do v <- freeBinder callee
+                        dbgDoc $ green (text "      beta reducing (inlining) singleton cont ") <> pretty (tidIdent $ boundVar v)
 
-                        SomethingElse _fn fnlink -> do
-                          do  redex <- knOfMK (mbContOf $ mkfnCont _fn) mredex
-                              dbgDoc $ text "SomethingElse (inlineNorF): " <+> align (pretty redex)
-                          if shouldInlineRedex mredex _fn
-                            then do
-                                  do  v <- freeBinder callee
-                                      dbgDoc $ green (text "copying and inlining SE ") <+> pretty (tidIdent $ boundVar v)
-                                      --kn1 <- knOfMK (YesCont mainCont) term
-                                      --dbgDoc $ text $ "knOfMK, term is " ++ show (pretty kn1)
-                                  fn' <- runCopyMKFn _fn Map.empty
-                                  newbody <- betaReduceOnlyCall fn' args kv    wr fd
-                                  replaceActiveSubtermWith newbody
-                                  killOccurrence bindingWorklistRef callee
-                                  collectRedexes wr kr er fr fd ar relocDomMarkers newbody
-                            else do
-                              if peekedThroughBitcast
-                                then return ()
-                                else considerFunctionForArityRaising er bindingWorklistRef _fn fnlink callee
-                      go (gas - 1)
-                    
-                    MKCont _up _ty callee args -> do
-                      knownFns   <- liftIO $ readIORef fr
-                      aliases    <- liftIO $ readIORef ar
-                      (peekedThroughBitcast, situation) <- classifyRedex callee args knownFns aliases
-                      case situation of
-                        CallOfUnknownFunction -> do
-                          do  cb <- freeBinder callee
-                              if T.pack ".fret" `T.isPrefixOf` (identPrefix $ tidIdent $ boundVar cb) 
-                                then return ()
-                                else do redex <- knOfMK (YesCont mainCont) mredex
-                                        dbgDoc $ red (text "CallOfUnknownCont: ") <+> pretty redex
-                          return ()
+                     newbody <- betaReduceOnlyCall fn args callee         wr fd
+                     
+                    --  do newbody' <- knOfMK (mbContOf $ mkfnCont fn) newbody
+                    --     dbgDoc $ text "CallOfSingletonCont: new: " <+> pretty newbody'
 
-                        CallOfSingletonFunction fn -> do
-                          do  redex <- knOfMK (mbContOf $ mkfnCont fn) mredex
-                              dbgDoc $ text "CallOfSingletonCont: " <+> pretty redex
-                          
-                              mapM_ (\arg -> do b <- freeBinder arg
-                                                c <- mkbCount b
-                                                dbgDoc $ text "      pre-beta occ count: " <> pretty c) args
-
-                          do  v <- freeBinder callee
-                              dbgDoc $ green (text "      beta reducing (inlining) singleton cont ") <> pretty (tidIdent $ boundVar v)
+                     mapM_ (\arg -> do b <- freeBinder arg
+                                       c <- mkbCount b
+                                       dbgDoc $ text "      pre-kill occ count: " <> pretty c) args
 
-                          -- If we are substituting a known argument, we should re-examine the
-                          -- substituted occurrences, which might have been made reducible.
-                          expBindMap <- liftIO $ readIORef er
-                          forM_ (zipSameLength args (mkfnVars fn)) $ \(arg, fnbv) -> do
-                            bv <- freeBinder arg
-                            if Map.member bv expBindMap
-                              then do occs <- collectOccurrences fnbv
-                                      liftIO $ modIORef' wr (\w -> worklistAddList w $ map freeLink occs)
-                              else return ()
-
+                     replaceWith subterm newbody
+                     killBinding callee knownFns aliases
 
-                          newbody <- betaReduceOnlyCall fn args callee         wr fd
-                        
-                        --  do newbody' <- knOfMK (mbContOf $ mkfnCont fn) newbody
-                        --     dbgDoc $ text "CallOfSingletonCont: new: " <+> pretty newbody'
-
-                          mapM_ (\arg -> do b <- freeBinder arg
-                                            c <- mkbCount b
-                                            dbgDoc $ text "      pre-kill occ count: " <> pretty c) args
-
-                          replaceActiveSubtermWith newbody
-                          killBinding callee knownFns aliases
-
-                          mapM_ (\arg -> do b <- freeBinder arg
-                                            c <- mkbCount b
-                                            dbgDoc $ text "      post-kill occ count: " <> pretty c) args
+                     mapM_ (\arg -> do b <- freeBinder arg
+                                       c <- mkbCount b
+                                       dbgDoc $ text "      post-kill occ count: " <> pretty c) args
 
 
-                        CallOfDonatableFunction fn -> do
-                          do  redex <- knOfMK (mbContOf $ mkfnCont fn) mredex
-                              dbgDoc $ text "CallOfDonatableCont: " <+> pretty redex
-                          return ()
-                          {-
-                          flags <- gets ccFlagVals
-                          if getInliningDonate flags
-                            then do
-                              fn' <- runCopyMKFn fn
-                              newbody <- do mk <- betaReduceOnlyCall fn' args kv   wr fd
-                                            readLink "CallOfDonatableC" mk
-                              replaceActiveSubtermWith newbody
-                              killOccurrence bindingWorklistRef callee
-                              collectRedexes wr kr er fr fd ar relocDomMarkers newbody
-                            else return ()
-      -}
-                        SomethingElse _fn _fnlink -> do
-                          do  redex <- knOfMK (mbContOf $ mkfnCont _fn) mredex
-                              dbgIf dbgCont $ text "SomethingElseC: " <+> pretty redex
-                          if shouldInlineRedex mredex _fn
-                            then do
-                                  dbgIf dbgCont $ text "skipping inlining continuation redex...?"
-                                  {-
-                                  fn' <- runCopyMKFn _fn
-                                  newbody <- betaReduceOnlyCall fn' args kv   wr fd  >>= readLink "CallOfDonatable"
-                                  replaceActiveSubtermWith newbody
-                                  killOccurrence bindingWorklistRef callee
-                                  collectRedexes wr kr er fr fd ar relocDomMarkers newbody
-                                  -}
-                                  return ()
-                            else return ()
-                      go (gas - 1)
+                   CallOfDonatableFunction fn -> do
+                     do redex <- knOfMK (mbContOf $ mkfnCont fn) mredex
+                        dbgDoc $ text "CallOfDonatableCont: " <+> pretty redex
+                     return ()
+                     {-
+                     flags <- gets ccFlagVals
+                     if getInliningDonate flags
+                       then do
+                         fn' <- runCopyMKFn fn
+                         newbody <- do mk <- betaReduceOnlyCall fn' args kv   wr fd
+                                       readLink "CallOfDonatableC" mk
+                         replaceWith mredex newbody
+                         killOccurrence callee
+                         collectRedexes wr kr er fr fd ar relocDomMarkers newbody
+                       else return ()
+-}
+                   SomethingElse _fn -> do
+                     do redex <- knOfMK (mbContOf $ mkfnCont _fn) mredex
+                        dbgDoc $ text "SomethingElseC: " <+> pretty redex
+                     if shouldInlineRedex mredex _fn
+                       then do
+                             dbgDoc $ text "skipping inlining continuation redex...?"
+                             {-
+                             fn' <- runCopyMKFn _fn
+                             newbody <- betaReduceOnlyCall fn' args kv   wr fd  >>= readLink "CallOfDonatable"
+                             replaceWith mredex newbody
+                             killOccurrence callee
+                             collectRedexes wr kr er fr fd ar relocDomMarkers newbody
+                             -}
+                             return ()
+                       else return ()
+                 go (gas - 1)
 
-                    MKLetFuns _u knowns fnrest -> do
-                      dbgIf dbgCont $ (text "analyzing for contifiability:")
-                          <+> align (vsep (map (pretty.tidIdent.boundVar.fst) knowns))
-                      knownFns   <- liftIO $ readIORef fr
-                      contifiability <- analyzeContifiability knowns knownFns
-                      case contifiability of
-                        GlobalsArentContifiable -> return ()
-                        CantContifyWithNoFn -> do dbgIf dbgCont $ yellow (text "       can't contify with no fn...")
+               MKLetFuns _u knowns fnrest -> do
+                 contifiability <- analyzeContifiability knowns
+                 case contifiability of
+                   GlobalsArentContifiable -> return ()
+                   CantContifyWithNoFn -> do dbgDoc $ yellow (text "       can't contify with no fn...")
+                                             return ()
+                   NoNeedToContifySingleton -> do dbgDoc $ yellow (text "       singleton usage, no need to contify")
                                                   return ()
-                        NoNeedToContifySingleton -> do  dbgIf dbgCont $ yellow (text "       singleton usage, no need to contify")
-                                                        return ()
-                        HadUnknownContinuations -> do dbgIf dbgCont $ red (text "       had one or more unknown continuations")
+                   HadUnknownContinuations -> do dbgDoc $ red (text "       had one or more unknown continuations")
+                                                 return ()
+                   HadMultipleContinuations -> do dbgDoc $ red (text "       had too many continuations")
+                                                  return ()
+                   NoSupportForMultiBindingsYet -> do dbgDoc $ red (text "skipping considering " <> pretty (map (tidIdent.boundVar.fst) knowns) <> text " for contification")
                                                       return ()
-                        HadMultipleContinuations (tailconts, nontailconts) -> do
-                            dbgIf dbgCont $ red (text "       had too many continuations")
-                            dbgIf dbgCont $ red (text "       " <> pretty (tailconts, nontailconts))
-                            return ()
-                        NoSupportForMultiBindingsYet -> do  dbgIf dbgCont $ red (text "skipping considering " <> pretty (map (tidIdent.boundVar.fst) knowns) <> text " for contification")
-                                                            return ()
-                        CantContifyNestedTailCalls -> do  dbgIf dbgCont $ red (text "can't contify with nested tail call...")
-                                                          return ()
-                        ContifyWith cont bv fn occs -> do
-                            doContifyWith_part1 cont bv fn occs wr fd bindingWorklistRef
-                            doContifyWith_part2 cont [bv] [fn] bindingWorklistRef relocDomMarkers mredex fnrest replaceActiveSubtermWith
+                   CantContifyNestedTailCalls -> do dbgDoc $ red (text "can't contify with nested tail call...")
+                                                    return ()
+                   ContifyWith cont bv fn occs -> do
+                      dbgDoc $ green (text "       should contify!")
+                      
+                      -- Replace uses of return continuation with common cont target.
+                      let Just oldret = mkfnCont fn
+                      -- This may result in additional functions becoming contifiable,
+                      -- so we collect the uses of the old ret cont first.
+                      collectRedexesUsingFnRetCont oldret   wr fd
+                      substVarForVar'' cont oldret
 
-                        ContifyWithMulti cont bvs_occs_fns -> do
-                            mapM_ (\(bv, occs, fn) -> do
-                               doContifyWith_part1 cont bv fn occs wr fd bindingWorklistRef
-                               ) bvs_occs_fns
-                            let (bvs, _, fns) = unzip3 bvs_occs_fns
-                            doContifyWith_part2 cont bvs fns bindingWorklistRef relocDomMarkers mredex fnrest replaceActiveSubtermWith
-
-                      go (gas - 1)
+                      -- Replacing the Call with a Cont will kill the old cont occurrences.
+                      mapM_ (\occ -> do
+                        mb_tm <- readOrdRef (freeLink occ)
+                        case mb_tm of
+                          Nothing -> error $ "asdfasdf"
+                          Just tm@(MKCall uplink ty v vs _cont) -> do
+                            let newterm = MKCont uplink ty v vs
+                            replaceTermWith tm newterm
+                            writeOrdRef (freeLink occ) (Just newterm)) occs
 
-                    MKCase _up ty v arms -> do
-                      x <- freeBinder v
-                      expBindsMap <- liftIO $ readIORef er
-                      case Map.lookup x expBindsMap of
-                         Nothing -> do
-                           dbgDoc $ text "skipping case expression because scrutinee is unknown for " <> pretty x
-                           go gas
-                         Just _scrutLink -> do
-                           findMatchingArm replaceActiveSubtermWith ty v arms (\v -> do
-                                       x <- freeBinder v
-                                       case Map.lookup x expBindsMap of
-                                         Nothing -> return Nothing
-                                         Just link -> readLink "case.scrut" link >>= return . Just)
-                           go gas
+                      rdm <- liftIO $ readIORef relocDomMarkers
+                      let ids = [tidIdent $ boundVar bv]
+                      (target, targetrest) <-
+                          case Map.lookup ids rdm of
+                            Nothing -> do
+                              -- We have    fun f = F in fR
+                              -- and want to end up with
+                              --           cont f = F in fR
+                              -- Replace the function with a continuation; be sure to
+                              -- replace the fn's global ident with a local version!
+                              return (mredex, fnrest)
 
-                    _ -> do
-                      ccWhen ccVerbose $ do
-                          kn <- knOfMK (YesCont mainCont) mredex
-                          dbgDoc $ text $ "skipping non-call/cont redex: " ++ show (pretty kn)
+                            Just targetandrest -> do
+                              -- We have fun f = F in fR   and somewhere else,   domreloc f in dR
+                              -- and want to end up with
+                              --                      fR                         cont f = F in dR
+                              --
+                              -- Remove the contified function explicitly
+                              replaceWith subterm fnrest
+                              return targetandrest
+
+                      contfn <- mkKnown' bv $ fn { mkfnCont = Nothing }
+                      let letcont = MKLetCont (parentLinkT target) [contfn] targetrest
+                      replaceTermWith target letcont
+
+                 go (gas - 1)
+
+               MKCase _up ty v arms -> do
+                 x <- freeBinder v
+                 expBindsMap <- liftIO $ readIORef er
+                 case Map.lookup x expBindsMap of
+                    Nothing -> do
+                      dbgDoc $ text "skipping case expression because scrutinee is unknown for " <> pretty x
                       go gas
 
-    go origGas
+                    Just _scrutLink -> do
+                      findMatchingArm bindingWorklistRef subterm ty v arms (\v -> do
+                                  x <- freeBinder v
+                                  case Map.lookup x expBindsMap of
+                                    Nothing -> return Nothing
+                                    Just link -> readLink "case.scrut" link >>= return . Just)
+                      go gas
+
+               _ -> do
+                 ccWhen ccVerbose $ do
+                    kn <- knOfMK (YesCont mainCont) mredex
+                    liftIO $ putDocLn $ text "skipping non-call/cont redex: " <> pretty kn
+                 go gas
+
+    let gas = case mb_gas of
+                Nothing -> 42000
+                Just gas -> gas
+    go gas
 
     return ()
 
-
-
-doContifyWith_part1 cont bv fn occs wr fd bindingWorklistRef = do
-  dbgIf dbgCont $ green (text "       should contify!")
-
-  -- Replace uses of return continuation with common cont target.
-  let Just oldret = mkfnCont fn
-  -- This may result in additional functions becoming contifiable,
-  -- so we collect the uses of the old ret cont first.
-
-  --liftIO $ putDocLn $ text "   substutituing " <> pretty cont <> text " for old ret " <> pretty oldret
-  collectRedexesUsingFnRetCont oldret   wr fd
-  substVarForVar'' cont oldret
-
-  -- Replacing the Call with a Cont will kill the old cont occurrences.
-  mapM_ (\occ -> do
-    mb_tm <- readOrdRef (freeLink occ)
-    case mb_tm of
-      Nothing -> do
-        liftIO $ putDocLn $ red (text "WARNING: not contifying call to " <> pretty (boundVar bv) <> text " due to missing occ term")
-        return ()
-
-      Just tm@(MKCall uplink ty v vs _cont) -> do
-        linkResult <- getActiveLinkFor tm
-        case linkResult of
-          ActiveSubterm target -> do
-            let newterm = MKCont uplink ty v vs -- TODO: kosher to reuse uplink?
-            replaceTermWith bindingWorklistRef target tm newterm
-            writeOrdRef (freeLink occ) (Just newterm)
-          TermIsDead -> do
-            liftIO $ putStrLn $ "WARNING: term is dead..."
-            return ()) occs
-
-doContifyWith_part2 cont bvs fns bindingWorklistRef relocDomMarkers mredex fnrest replaceActiveSubtermWith = do
-  rdm <- liftIO $ readIORef relocDomMarkers
-  let ids = map (tidIdent.boundVar) bvs
-  (target, targetrest) <-
-      case Map.lookup ids rdm of
-        Nothing -> do
-          -- We have    fun f = F in fR
-          -- and want to end up with
-          --           cont f = F in fR
-          -- Replace the function with a continuation; be sure to
-          -- replace the fn's global ident with a local version!
-          return (mredex, fnrest)
-
-        Just targetandrest -> do
-          -- We have fun f = F in fR   and somewhere else,   domreloc f in dR
-          -- and want to end up with
-          --                      fR                         cont f = F in dR
-          --
-          -- Remove the contified function explicitly
-          replaceActiveSubtermWith fnrest
-          return targetandrest
-
-  linkResult <- getActiveLinkFor target
-  case linkResult of
-    ActiveSubterm link -> do
-      contfns <- mapM (\(fn, bv) -> mkKnown' bv $ fn { mkfnCont = Nothing }) (zip fns bvs)
-      let letcont = MKLetCont (parentLinkT target) contfns targetrest
-      replaceTermWith bindingWorklistRef link target letcont
-    TermIsDead -> return ()
-
--- A function is eligible for arity raising if every usage is a call
--- (no higher-order usages) and every call passes a known tuple.
---
-considerFunctionForArityRaising expBindsMapRef bindingWorklistRef fn fnlink callee = do
-  expBindsMap <- liftIO $ readIORef expBindsMapRef -- for looking up tuple params
-  calleeb <- freeBinder callee
-  occs <- collectOccurrences calleeb
-  directs <- mapM (isDirectCallWithKnownTupleArg expBindsMap calleeb) occs
-  if allSameNonZeroLength directs
-    then do -- Replace each call site to pass the tuple parameters instead of the tuple.
-            mapM_ (\ (DC_WithTuple calltm tupleparts) -> do
-              case calltm of
-                MKCall uplink ty v _tup sr -> do
-                  linkResult <- getActiveLinkFor calltm
-                  case linkResult of
-                    ActiveSubterm target -> do
-                      tupleparts' <- mapM (\fv -> freeBinder fv >>= mkFreeOccForBinder) tupleparts
-                      let newterm = MKCall uplink ty v tupleparts' sr -- TODO: kosher to reuse uplink?
-                      replaceTermWith bindingWorklistRef target calltm newterm
-                    _ -> do
-                      return (error "skipping call because we didn't find an active subterm (!?)")
-                _ -> error $ "line 1782 invariant violated") directs
-
-            let createFnArg (n, bv) = do
-                  genBinderAndOcc ("_" ++ show n ++ ".tuparity") (tidType (boundVar bv))
-
-
-            let (DC_WithTuple _ tupleparts) = head directs
-            tupleparts_bvs <- mapM freeBinder tupleparts
-            newArgsAndOccs <- evalStateT (mapM createFnArg (zip [0..] tupleparts_bvs)) Map.empty
-            let (newArgs, newOccs) = unzip newArgsAndOccs
-
-            -- The body of the arity-raised function should construct a tuple
-            -- out of the new function parameters, for the existing body to use.
-            let tupbnd = head (mkfnVars fn)
-            let tuptyp = tidType (boundVar tupbnd)
-            tnu <- newOrdRef Nothing
-            nu  <- newOrdRef Nothing
-            let tupexp = MKTuple tnu tuptyp newOccs (MissingSourceRange "tup")
-            tuplink <- newOrdRef (Just tupexp)
-            let letval = MKLetVal nu (mkKnownE tupbnd tuplink) (mkfnBody fn)
-            vallink <- newOrdRef (Just letval)
-            backpatchE letval [tuplink]
-            _ <- backpatchT letval [mkfnBody fn]
-            let newbody = vallink
-
-            let fnvar = boundVar (mkfnVar fn)
-            let newfntype = case tidType fnvar of
-                              FnType [_] rng cc pf ->
-                                FnType (map (tidType.boundVar) tupleparts_bvs) rng cc pf
-                              _ -> error "expected arity-raised function to have single-arg function type?!?"
-
-            -- We can't (yet) change the local identifier the function is bound to, so if we generated a new
-            -- identifier here, we'd encounter errors later in codegen due to the mismatch. Instead, we carefully
-            -- reuse the existing identifier (ew).
-            newcbvar <- evalStateT (mkBinder (TypedId newfntype (tidIdent (boundVar calleeb)))) Map.empty
-
-            -- Different issue here: the backend special-cases function entry basic blocks based on name.
-            newfnid  <- ccRefreshLocal (tidIdent $ boundVar (mkfnVar fn))
-            newfnvar <- evalStateT (mkBinder (TypedId newfntype newfnid)) Map.empty
-
-            -- Make sure we examine the tuple for subsequent optimizations (e.g. case-of-tuple elimination).
-            liftIO $ modIORef' bindingWorklistRef (\w -> worklistAdd w tupbnd)
-            liftIO $ modIORef' expBindsMapRef (\m -> Map.insert tupbnd tuplink m)
-
-            -- Apply variable substitutions to ensure the new types take effect.
-            substVarForVar''  newfnvar (           mkfnVar  fn)
-            substVarForVar''  newcbvar calleeb
-
-            -- Replace the original function with an arity-raised version.
-            let newfn =
-                  MKFn { mkfnVar   = newfnvar
-                       , mkfnVars  = newArgs
-                       , mkfnCont  = mkfnCont fn
-                       , mkfnBody  = newbody
-                       , mkfnIsRec = (mkfnIsRec fn)
-                       ,_mkfnAnnot = (_mkfnAnnot fn)
-                       }
-            writeOrdRef fnlink (Just newfn)
-    else do dbgDoc $ text "not all calls with known tuples"
-            return ()
-
-data DirectCallWithTupleArg = DC_WithTuple (MKTerm MonoType) [FreeVar MonoType]
-                            | DC_Other
-
-allSameNonZeroLength [] = False
-allSameNonZeroLength (d:rest)= go rest (tupLen d)
-  where go [] len = len > 0
-        go (d:rest) len = if tupLen d == len
-                            then go rest len
-                            else False
-        tupLen (DC_WithTuple _ vs) = length vs
-        tupLen _ = 0
-
-isDirectCallWithKnownTupleArg expBindsMap fnvar occ = do
-  mb_tm <- readOrdRef (freeLink occ)
-  case mb_tm of
-    Just tm@(MKCall _ _ v [arg] _) -> do
-      vb <- freeBinder v
-      va <- freeBinder arg
-      if vb /= fnvar then return DC_Other else
-          case Map.lookup va expBindsMap of
-            Nothing -> return $ DC_Other
-            Just link -> do
-              e <- readLink "isDirectCall.Exp" link
-              case e of
-                 MKTuple  _u _ vs _sr | not (null vs) -- No point in arity-raising unit values.
-                   -> return $ DC_WithTuple tm vs
-                 _ -> return DC_Other
-    _ -> return DC_Other
-
-
 isRecursiveButNotTailRecursive fn = do
   occs <- collectOccurrences (mkfnVar fn)
   isRecAndNotTailRec <- mapM (\occ -> do
@@ -1982,46 +1733,18 @@
   occs <- collectOccurrences oldret
   mb_callees <- mapM calleeOfCont occs
 
-  fndefs <- liftIO $ readIORef fd
+  fndefs <- liftIO $ readIORef fd  
   mapM_ (\calleeBV -> do
       case Map.lookup calleeBV fndefs of
         Nothing -> return ()
         Just tm -> liftIO $ modIORef' wr (\w -> worklistAdd w tm)
     ) [c | Just c <- mb_callees]
 
-findParentFn tm = do
-  parent <- readLink "findParentFn" (parentLinkT tm)
-  case parent of
-    ParentTerm t -> findParentFn t
-    ParentFn f -> return f
-
--- We need to make sure we ignore trivial rebindings, which might
--- otherwise prevent us from recognizing contifiable functions.
-peekTrivialCont knownFns bv = do
-    mb_fn <- lookupBinding' bv knownFns
-    case mb_fn of
-      Nothing -> return bv -- No known continuation; nothing else to do.
-      Just cf -> do
-        bodytm <- readLink "analyzeContifiability" (mkfnBody cf)
-        case bodytm of
-          MKCont _u _ty dv fvs -> do
-            binders <- mapM freeBinder fvs
-            if binders == mkfnVars cf
-              then do -- If we a have a call (foo ...) returning to cont C,
-                      -- and C x = D x, then replace the call's cont with D.
-                      db <- freeBinder dv
-                      substVarForBound (dv, bv)
-                      return db
-              else return bv -- Reordered parameters, uh oh!
-          _ -> return bv -- The cont body is non-trivial.
-
-
-
 data Contifiability =
     GlobalsArentContifiable
   | NoNeedToContifySingleton
   | HadUnknownContinuations
-  | HadMultipleContinuations ( [MKBoundVar MonoType] , [MKBoundVar  MonoType] )
+  | HadMultipleContinuations
   | CantContifyNestedTailCalls
   | CantContifyWithNoFn
   | NoSupportForMultiBindingsYet
@@ -2029,15 +1752,9 @@
                 (MKBound MonoType)
                 (MKFn (Subterm MonoType) MonoType)
                 [FreeOcc MonoType]
-  | ContifyWithMulti (MKBound MonoType)
-              [((MKBound MonoType)
-              , [FreeOcc MonoType]
-              , (MKFn (Subterm MonoType) MonoType))]
 
-analyzeContifiability :: [Known MonoType (Link (MKFn (Subterm MonoType) MonoType))]
-            -> (Map (MKBoundVar MonoType) (Link (MKFn (Subterm MonoType) MonoType)))
-            -> Compiled Contifiability
-analyzeContifiability knowns knownFns = do
+--analyzeContifiability :: ... -> Compiled Contifiability
+analyzeContifiability knowns = do
   let isTopLevel (GlobalSymbol _ _) = True
       isTopLevel _ = False
   if all isTopLevel $ map (tidIdent.boundVar.fst) knowns
@@ -2055,28 +1772,25 @@
               mbs_conts <- mapM (contOfCall bv) occs
               case allFoundConts mbs_conts of
                 Nothing -> return HadUnknownContinuations
-                Just rawConts -> do
-                  conts <- mapM (peekTrivialCont knownFns) rawConts
+                Just conts -> do
                   let (tailconts, nontailconts) = partitionEithers $
                         [if Just bv == mkfnCont fn
                           then Left bv else Right bv
-                        | bv <- removeDuplicates conts]
-                  dbgIf dbgCont $ yellow (text "       had just these conts: ")
-                                        <$> text "               all conts: " <> pretty (map (tidIdent.boundVar) conts)
+                        | bv <- Set.toList $ Set.fromList conts]
+                  dbgDoc $ yellow (text "       had just these conts: ")
                                         <$> text "              tail calls: " <> pretty (map (tidIdent.boundVar) tailconts)
                                         <$> text "          non-tail calls: " <> pretty (map (tidIdent.boundVar) nontailconts)
                   case (tailconts, nontailconts) of
-                    ((_:_:_), _) -> return $ HadMultipleContinuations (tailconts, nontailconts) -- Multiple tail calls: no good!
+                    ((_:_:_), _) -> return HadMultipleContinuations -- Multiple tail calls: no good!
                     (_ ,  [cont]) -> do -- Happy case: zero or one tail call, one outer continuation.
                          return (ContifyWith cont bv fn occs)
-                    _ -> return $ HadMultipleContinuations (tailconts, nontailconts) -- Multiple outer continuations: no good!
+                    _ -> return HadMultipleContinuations -- Multiple outer continuations: no good!
 
         _ -> do
           let bvs     = map fst knowns
               fnlinks = map snd knowns
           mb_fns <- mapM readOrdRef fnlinks
           occss <- mapM collectOccurrences bvs
-          let combinedOccs = concat occss
 
           mbs_retconts <- mapM (\mb_fn -> do
               case mb_fn of
@@ -2085,37 +1799,18 @@
           let retconts = [c | Just c <- mbs_retconts]
 
           liftIO $ putDocLn $ text "recursive nest: {{{"
-
-          mbs_parentFns <- mapM (\occ -> do
+          mapM_ (\(occs, bv, mb_fn) -> do
+            case occs of [_] -> liftIO $ putDocLn $ text "   (is  singleton)"
+                         _   -> liftIO $ putDocLn $ text "   (not singleton)"
+            liftIO $ putDocLn $ text "   occ:"
+            mapM_ (\occ -> do
               mb_tm <- readOrdRef (freeLink occ)
               case mb_tm of
-                Nothing -> do return $ Nothing
-                Just tm -> do findParentFn tm >>= return . Just
-              ) combinedOccs
-          let allSame = case map (tidIdent.boundVar.mkfnVar) [pf | Just pf <- mbs_parentFns] of
-                          []  -> True
-                          [_] -> True
-                          (id:rest) -> List.all (== id) rest
-          liftIO $ putDocLn $ text "        all same parent? " <> pretty allSame
-
-          -- Describe/debug print situation.
-          mapM_ (\ (occs, bv, mb_fn) -> do
-            case occs of [_] -> liftIO $ putDocLn $ text "   (is  singleton)"
-                         _   -> liftIO $ putDocLn $ text "   (not singleton)"
-
-            --liftIO $ putDocLn $ text "   occ:"
-            --mapM_ (\occ -> do
-            --  mb_tm <- readOrdRef (freeLink occ)
-            --  case mb_tm of
-            --    Nothing -> do
-            --      liftIO $ putDocLn $ text "      no term"
-            --    Just tm -> do
-            --      --do kn <- knOfMK NoCont tm
-            --      --   liftIO $ putDocLn $ text "      " <> pretty kn
-            --      --parentFn <- findParentFn tm
-            --      --liftIO $ putDocLn $ text "      parent: " <> pretty (boundVar $ mkfnVar parentFn)
-            --  ) occs
-
+                Nothing -> do
+                  liftIO $ putDocLn $ text "      no term"
+                Just tm -> do
+                  do kn <- knOfMK NoCont tm
+                     liftIO $ putDocLn $ text "      " <> pretty kn) occs
 
 
             case mb_fn of
@@ -2140,45 +1835,9 @@
             ([_], _) -> do return NoNeedToContifySingleton -- Singleton call; no need to contify since we'll just inline it...
             (_, Just fn) -> do
              -}
-          splitContss <- mapM (\ (occs, bv, mb_fn) -> do
-            case mb_fn of
-              Nothing -> do return ([], [])
-              Just _fn -> do
-                aconts <- mapM (contOfCall bv) occs
-                case allFoundConts aconts of
-                  Nothing -> return ([], [])
-                  Just conts -> do
-                             let (tailconts, nontailconts) = partitionEithers $
-                                                    [if bv `elem` retconts
-                                                      then Left bv else Right bv
-                                                    | bv <- Set.toList $ Set.fromList conts]
-                             return (tailconts, nontailconts)
-            ) (zip3 occss bvs mb_fns)
-
-          let (tailcontss, nontailcontss) = unzip splitContss
-          liftIO $ putDocLn $ text "  tails:    " <> pretty (removeDuplicates (concat tailcontss))
-          liftIO $ putDocLn $ text "  nontails: " <> pretty (removeDuplicates (concat nontailcontss))
 
           liftIO $ putDocLn $ text "}}}"
-
-          let fns = [f | Just f <- mb_fns]
-          case removeDuplicates (concat nontailcontss) of
-            nt@(_:_:_) -> do
-              liftIO $ putDocLn $ text "(recursive nest had too many non-tail continuations)"
-              return $ HadMultipleContinuations ( nt, removeDuplicates (concat tailcontss) )
-
-            [] -> do
-              liftIO $ putDocLn $ text "(recursive nest had zero non-tail continuations)"
-              return $ HadMultipleContinuations ( [], removeDuplicates (concat tailcontss) )
-
-            [cont] ->
-              if allSame
-                then do
-                  liftIO $ putDocLn $ text "(no support for relocating recursive nests yet)"
-                  return $ NoSupportForMultiBindingsYet
-                else do
-                  return (ContifyWithMulti cont (zip3 bvs occss fns))
-                  --return $ NoSupportForMultiBindingsYet
+          return $ NoSupportForMultiBindingsYet
 
           -- TODO for contifiable nests, determine whether the calls all come from
           -- within one of the functions in the nest; if so, the contified functions
@@ -2187,7 +1846,6 @@
 
 -- Collect the list of occurrences for the given binder,
 -- but "peek through" any bitcasts.
-collectOccurrences :: MKBoundVar t -> Compiled [FreeOcc t]
 collectOccurrences bv = do
   inits <- dlcToList bv
   initss <- mapM (\fv -> do
@@ -2285,29 +1943,30 @@
   -}
   False
 
-replaceWith :: Pretty ty => IORef (WorklistQ (MKBoundVar ty)) -> Subterm ty ->
-               Subterm ty -> Subterm ty -> Compiled ()
-replaceWith bindingWorklistRef activeLink poss_indir_target newsubterm = do
+replaceWith :: Pretty ty => Subterm ty -> Subterm ty -> Compiled ()
+replaceWith poss_indir_target newsubterm = do
   oldterm <- readLink "replaceWith" poss_indir_target
   newterm <- readLink "replaceWith" newsubterm
-  replaceTermWith bindingWorklistRef activeLink oldterm newterm
+  replaceTermWith oldterm newterm
   writeOrdRef poss_indir_target     (Just newterm)
+  --newlink2 <- getActiveLinkFor newterm
+  --return ()
 
-replaceTermWith :: Pretty ty => IORef (WorklistQ (MKBoundVar ty)) -> Subterm ty ->
-                   MKTerm ty -> MKTerm ty -> Compiled ()
-replaceTermWith bindingWorklistRef activeLink oldterm newterm = do
-  writeOrdRef activeLink           (Just newterm)
+replaceTermWith :: Pretty ty => MKTerm ty -> MKTerm ty -> Compiled ()
+replaceTermWith oldterm newterm = do
+  target <- getActiveLinkFor oldterm
+
+  writeOrdRef target            (Just newterm)
 
   let oldoccs = directFreeVarsT oldterm
   let newoccs = directFreeVarsT newterm
-  mapM_ (killOccurrence bindingWorklistRef) (oldoccs `butnot` newoccs)
+  mapM_ killOccurrence (oldoccs `butnot` newoccs)
   mapM_ (\fv -> setFreeLink fv newterm) newoccs
   readOrdRef (parentLinkT oldterm) >>= writeOrdRef (parentLinkT newterm)
 
-killOccurrence :: Pretty ty => IORef (WorklistQ (MKBoundVar ty)) ->
-                  FreeVar ty -> Compiled ()
-killOccurrence bindingWorklistRef fo = do
-    b@(MKBound _v r) <- freeBinder fo -- r :: OrdRef (Maybe (FreeOcc tyx))
+killOccurrence :: Pretty ty => FreeVar ty -> Compiled ()
+killOccurrence fo = do
+    MKBound _v r <- freeBinder fo -- r :: OrdRef (Maybe (FreeOcc tyx))
 
     isSingleton <- freeOccIsSingleton fo
     if isSingleton
@@ -2315,14 +1974,13 @@
        ccWhen ccVerbose $ do
          dbgDoc $ red (text "killing singleton binding ") <> prettyId _v
        writeOrdRef r Nothing
-       liftIO $ modIORef' bindingWorklistRef (\w -> worklistAdd w b)
      else do
        n <- dlcNext fo
        p <- dlcPrev fo
        writeOrdRef (dlcNextRef p) n
        writeOrdRef (dlcPrevRef n) p
 
-    -- Make sure binder doesn't point to ``fo``.
+    -- Make sure binder doesn't point to fo
     mb_fo' <- readOrdRef r
     case mb_fo' of
       Just fo' | dlcIsSameNode fo fo' -> do
@@ -2355,10 +2013,8 @@
       Nothing   -> return Nothing
       Just link -> readOrdRef link
 
-zipSameLength = zip
-
 betaReduceOnlyCall fn args kv    wr fd = do
-    mapM_ substVarForBound (zip args (mkfnVars fn))
+    mapM_ substVarForBound (zipSameLength args (mkfnVars fn))
     kvb1 <- freeBinder kv
 
     case mkfnCont fn of
@@ -2370,7 +2026,9 @@
         substVarForBound (kv, oldret)
 
     kvb2 <- freeBinder kv
-    dbgDoc $ text $ "      betaReduceOnlyCall on " ++ show (pretty (mkfnVar fn))
+    do argBinders <- mapM freeBinder args
+       dbgDoc $ text "      betaReduceOnlyCall on " <> pretty (mkfnVar fn)
+                              <+> text " with args " <+> pretty argBinders
     if kvb1 /= kvb2
       then do
         dbgDoc $ text "       kv before: " <> pretty kvb1
@@ -2417,15 +2075,13 @@
               dbgDoc $ text "pccOfTopTerm saw nulled-out function link " <> pretty x
             return ()
           Just fn -> do
-          {--
-            do
-              knfn <- lift $ knOfMKFn NoCont fn
-              dbgIf dbgFinal $ indent 10 (pretty x)
-              dbgIf dbgFinal $ indent 20 (pretty knfn)
-              dbgIf dbgFinal $ text "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~"
-             --}
-
-            dbgDoc $ text "cffnOfMKFn from link # " <> pretty (ordRefUniq link)
+            knfn <- lift $ knOfMKFn NoCont fn
+            {--
+            dbgDoc $ indent 10 (pretty x)
+            dbgDoc $ indent 20 (pretty knfn)
+            dbgDoc $ text "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~"
+            --}
+            
             cffn <- lift $ cffnOfMKFn uref fn
             !(fns, topbinds) <- get
             put (cffn : fns, topbinds)
@@ -2441,38 +2097,13 @@
           MKLetRec      {} -> do error $ "MKLetRec in pccTopTerm"
           MKLetFuns     _u   knowns  k  -> do mapM_ grabFn knowns ; go k
           MKCall        {}              -> return ()
-          MKLetCont _ [(kb,c)] subterm2 -> do
-            isDead  <- lift $ binderIsDead kb
-            if isDead then go subterm2
-              else error $ "MKLetCont in pccTopTerm, known: " ++ show (map (tidIdent.boundVar.fst) [(kb,c)])
-
-          MKLetCont _ knowns subterm2 -> do
-                                 --subtm <- lift $ readLink "pccOfTopTerm(subterm2)" subterm2
-                                 --kn <- lift $ knOfMK NoCont subtm
-                                 error $ "MKLetCont in pccTopTerm, knowns: " ++ show (map (tidIdent.boundVar.fst) knowns)
+          MKLetCont     {} -> do error $ "MKLetCont in pccTopTerm"
           MKCont        {} -> do error $ "MKCont in pccTopTerm"
           MKRelocDoms   {} -> do error $ "MKRelocDoms in pccTopTerm"
 
       handleTopLevelBinding id expr k = do
         case expr of
-          MKLiteral _ ty lit -> do !(fns, topbinds) <- get
-                                   put (fns, TopBindLiteral id ty lit : topbinds)
-                                   go k
-
-          MKTuple     _ ty fvs _sr -> do
-                                   !(fns, topbinds) <- get
-                                   bvs <- lift $ mapM freeBinder fvs
-                                   let ids = map (tidIdent . boundVar) bvs
-                                   put (fns, TopBindTuple id ty ids : topbinds)
-                                   go k
-
-          MKAppCtor  _ ty (cid, crep) fvs -> do
-                                   !(fns, topbinds) <- get
-                                   bvs <- lift $ mapM freeBinder fvs
-                                   let ids = map (tidIdent . boundVar) bvs
-                                   put (fns, TopBindAppCtor id ty (cid, crep) ids : topbinds)
-                                   go k
-
+          MKLiteral    {} -> go k
           MKAllocArray {} -> go k
 
           MKArrayLit _ ty _fv litsOrVars -> do
@@ -2505,15 +2136,9 @@
                           return $ boundVar bound
 
 cffnOfMKCont :: MKBoundVar MonoType -> MKFn (Subterm MonoType) MonoType -> BlockAccum ()
-cffnOfMKCont cv (MKFn _v vs _ subterm _isrec _annot) = do
+cffnOfMKCont cv (MKFn _ vs _ subterm _isrec _annot) = do
   headerBlockId <- blockIdOf cv
   let head = ILabel (headerBlockId, map boundVar vs)
-  dbgDoc $ text "cffnOfMKCont head = " <> pretty head
-  dbgDoc $ text "cffnOfMKCont cv == " <> pretty (boundVar cv)
-  dbgDoc $ text "                ~~ " <> pretty (tidType (boundVar cv))
-  dbgDoc $ text "cffnOfMKCont  v == " <> pretty (boundVar _v)
-  dbgDoc $ text "                ~~ " <> pretty (tidType (boundVar _v))
-  dbgDoc $ text "cffnOfMKCont vs = " <> align (vsep (map (pretty.boundVar) vs))
 
   -- Walk the term;
   --    accumulate a block body of [Insn O O]
@@ -2529,6 +2154,7 @@
           MKLetVal      _u (bv, subexpr) k -> do
               letable <- lift $ letableOfSubexpr subexpr
               isDead  <- lift $ binderIsDead bv
+              
               if isDead && isPure letable
                 then go k head insns
                 else go k head (ILetVal (tidIdent $ boundVar bv) letable : insns)
@@ -2539,10 +2165,9 @@
                                                   mb_mkfn <- readOrdRef link
                                                   case mb_mkfn of
                                                     Nothing -> do
-                                                      --dbgDoc2 $ text $ "cffnOfMKCont removed dead fn: " ++ show (tidIdent $ boundVar bv)
+                                                      dbgDoc $ text $ "cffnOfMKCont removed dead fn: " ++ show (tidIdent $ boundVar bv)
                                                       return []
                                                     Just mkfn -> do
-                                                      dbgDoc $ text "read fn from link # " <> pretty (ordRefUniq link)
                                                       cffn <- cffnOfMKFn uref mkfn
                                                       return [(tidIdent (boundVar bv), cffn)] ) knowns
                                               let (ids, fns) = unzip (concat idsfnss)
@@ -2627,8 +2252,6 @@
   --dbgDoc $ vcat (map pretty allblocks)
   --dbgDoc $ indent 20 (pretty graph)
 
-  dbgDoc $ text "converted function, type is " <> pretty (tidType (boundVar v))
-
   return $ Fn { fnVar = boundVar v,
                 fnVars = map boundVar vs,
                 fnBody = BasicBlockGraph (entryLab blocks) rk graph,
@@ -2677,6 +2300,12 @@
     _ -> error $ "non-Letable thing seen by letableOfSubexpr..."
 
 
+dbgDoc :: MonadIO m => Doc -> m ()
+dbgDoc d =
+  if False
+    then liftIO $ putDocLn d
+    else return ()
+
 --  * A very important pattern to inline is     iter x { E2 },
 --    which ends up looking like    f = { E2 }; iter x f;
 --    with f not referenced elsewhere.  Even if E2 is big enough
@@ -2688,6 +2317,68 @@
 --    a use-once function. Such "budget donation" can reduce the variability
 --    of inlining's run-time benefits due to inlining thresholds.
 
+{- {{{ Online constant folding
+{-
+    KNCase        ty v patbinds -> do
+        ...
+        -- If something is known about v's value,
+        -- select or discard the appropriate branches.
+        -- TODO when are default branches inserted?
+        mb_const <- extractConstExpr env v
+        case mb_const of
+          IsConstant v' c -> do
+                   mr <- matchConstExpr v' c patbinds
+                   case {-trace ("match result for \n\t" ++ show c ++ " is\n\t" ++ show mr)-} mr of
+                      Right e -> knInline' e env
+                      Left patbinds0 -> do v' <- q v
+                                           !patbinds' <- mapM inlineArm patbinds0
+                                           residualize $ KNCase ty v' patbinds'
+          _ -> do v' <- q v
+                  !patbinds' <- mapM inlineArm patbinds
+                  residualize $ KNCase ty v' patbinds'
+-}
+
+data ConstExpr = Lit            MonoType Literal
+               | LitTuple       MonoType [ConstStatus] SourceRange
+               | KnownCtor      MonoType (CtorId, CtorRepr) [ConstStatus]
+               deriving Show
+
+data ConstStatus = IsConstant (TypedId MonoType) ConstExpr
+                 | IsVariable (TypedId MonoType)
+                 deriving Show
+
+extractConstExpr :: SrcEnv -> TypedId MonoType -> In ConstStatus
+extractConstExpr env var = extractConstExprWith env var lookupVarOp
+
+extractConstExpr' :: SrcEnv -> TypedId MonoType -> In ConstStatus
+extractConstExpr' env var = extractConstExprWith env var (\e v -> Just $ lookupVarOp' e v)
+
+extractConstExprWith env var lookup = go var where
+  go v =
+     case lookup env v of
+       (Just (VO_E ope)) -> do
+         (e', _) <- visitE (tidIdent v, ope)
+         case e' of
+            KNLiteral ty lit      -> return $ IsConstant v $ Lit ty lit
+            KNTuple   ty vars rng -> do results <- mapM go vars
+                                        return $ IsConstant v $ LitTuple ty results rng
+            KNAppCtor ty cid vars -> do results <- mapM go vars
+                                        return $ IsConstant v $ KnownCtor ty cid results
+            -- TODO could recurse through binders
+            -- TODO could track const-ness of ctor args
+            _                     -> return $ IsVariable v
+       _ -> return $ IsVariable v
+addBindings [] e = e
+addBindings ((id, cs):rest) e = KNLetVal id (exprOfCS cs) (addBindings rest e)
+
+exprOfCS (IsVariable v)                         = KNVar v
+exprOfCS (IsConstant _ (Lit ty lit))            = KNLiteral ty lit
+exprOfCS (IsConstant _ (KnownCtor ty cid []))   = KNAppCtor ty cid []
+exprOfCS (IsConstant _ (KnownCtor ty cid args)) = KNAppCtor ty cid (map varOfCS args)
+exprOfCS (IsConstant _ (LitTuple ty args rng))  = KNTuple ty (map varOfCS args) rng
+
+varOfCS (IsVariable v  ) = v
+varOfCS (IsConstant v _) = v
 
 
 -- We'll iterate through the list of arms. Initially, our match status will be
@@ -2704,132 +2395,67 @@
 data MatchStatus = NoPossibleMatchYet | MatchPossible
                    deriving Show
 
-data PatternMatchStatus t = MatchDef [(Ident, FreeVar t)]
-                          | MatchSeq [(FreeVar t, PatternRepr t)]
-                          | MatchAmbig
-                          | MatchNeg
-
-matchSeq :: t -> SourceRange -> Subterm t -> (FreeVar t, PatternRepr t) -> Compiled (Subterm t)
-matchSeq ty range subterm (v, pat) = do
-  parentLink <- newOrdRef Nothing
-  let todoBindings = []
-  let rv = MKCase parentLink ty v [MKCaseArm pat subterm todoBindings range]
-  _ <- backpatchT rv [subterm]
-  newOrdRef (Just rv)
+data PatternMatchStatus = MatchDef [(Ident, ConstStatus)] | MatchAmbig | MatchNeg
+                          deriving Show
 
--- Compute the residual of matching the given term ``T`` against the given
--- pattern arms ``p1 -> e1, ... , pn -> en``.
--- If ``c`` definitely matches one of the patterns ``pj``,
--- replace the given case subterm by ``ej`` with appropriate substitutions.
--- 
-findMatchingArm :: Pretty ty =>
-                   (Subterm ty -> Compiled ()) -> ty
-                ->  FreeVar ty -> [MKCaseArm (Subterm ty) ty]
-                -> (FreeVar ty -> Compiled (Maybe (MKExpr ty)))
-                -> Compiled ()
-findMatchingArm replaceCaseWith ty v arms lookupVar = go arms NoPossibleMatchYet
-  where go [] _ = return ()
-        go ((MKCaseArm pat subterm _bindings range):rest) potentialMatch = do
-              -- Map from pattern variable ids to bound vars.
-              let boundsFor = Map.fromList [(tidIdent (boundVar b), b) | b <- _bindings]
+-- Given a constant expression c, match against  (p1 -> e1) , ... , (pn -> en).
+-- If c definitely matches some pattern pk, return ek.
+-- Otherwise, return the list of arms which might possibly match c.
+-- TODO handle partial matches:
+--        case (a,b) of (True, x) -> f(x)
+--      should become
+--        case (a,b) of (True, x) -> f(b)
+--      even thought it can't become simply ``f(b)`` because a might not be True.
+matchConstExpr :: TypedId MonoType -> ConstExpr
+               ->            [CaseArm PatternRepr (KNMono) MonoType]
+               -> In (Either [CaseArm PatternRepr (KNMono) MonoType]
+                             SrcExpr)
+matchConstExpr v c arms = go arms [] NoPossibleMatchYet
+  where go [] reverseArmsWhichMightMatch _ =
+                 -- No conclusive match found, but we can still
+                 -- match against only those arms that we didn't rule out.
+                return $ Left (reverse reverseArmsWhichMightMatch)
 
-              matchRes <- matchPatternAgainst pat v
-              case (matchRes, potentialMatch) of
-                -- A definite match only "counts" if there were no earlier possible matches.
-                (MatchDef matches, NoPossibleMatchYet) -> do
-                  -- Note: matches is a list of (id, v) where id is a pattern ident and v is a FreeVar.
-                  let boundAndVars = [(v, boundsFor Map.! id) | (id, v) <- matches]
-                  mapM_ substVarForBound boundAndVars
-                  replaceCaseWith subterm
+        go (arm@(CaseArm pat e guard _ _):rest) armsWhichMightMatch potentialMatch =
+          let rv = matchPatternWithConst pat (IsConstant v c) in
+          case (guard, rv, potentialMatch) of
+               (Nothing, MatchDef bindings, NoPossibleMatchYet)
+                                      -> return $ Right (addBindings bindings e)
+               -- We can (in theory) discard arms which definitely won't match,
+               -- but pattern match compilation would then think that the match
+               -- is incomplete and generate DT_Fail nodes unnecessarily.
+               (Nothing, MatchNeg, _) -> go rest (arm:armsWhichMightMatch) potentialMatch
+               _                      -> go rest (arm:armsWhichMightMatch) MatchPossible
 
-                -- A match sequence is similar to a definite match, except that it
-                -- replaces one pattern match with several simpler matches,
-                -- instead of replacing a pattern match with the associated arm.
-                (MatchSeq varsPats, NoPossibleMatchYet) -> do
-                  if null varsPats
-                    then return ()
-                    else do
-                      -- TODO maybe mark the generated cases as redexes?
-                      caseSeq <- foldlM (matchSeq ty range) subterm varsPats
-                      replaceCaseWith caseSeq
-
-                -- A match that definitely won't happen can be ignored.
-                (MatchNeg  , _) -> go rest potentialMatch
-
-                -- Otherwise, we have a possible match.
-                _               -> go rest MatchPossible
+        nullary True  = MatchDef []
+        nullary False = MatchNeg
 
         -- If the constant matches the pattern, return the list of bindings generated.
-        --matchPatternAgainst :: PatternRepr ty -> FreeVar ty -> Compiled [PatternMatchStatus ty]
-        matchPatternAgainst p v = do
-          case p of
-            PR_Atom (P_Wildcard _ _  ) -> return $ MatchDef []
-            PR_Atom (P_Variable _ tid) -> return $ MatchDef [(tidIdent tid, v)]
-            _ -> do
-              mb_e <- lookupVar v
-              case mb_e of
-                Nothing -> return $ MatchAmbig
-                Just e  ->
-                  case (e, p) of
-                    (MKLiteral _ _ (LitInt  i1), PR_Atom (P_Int  _ _ i2)) -> nullary $ litIntValue i1 == litIntValue i2
-                    (MKLiteral _ _ (LitBool b1), PR_Atom (P_Bool _ _ b2)) -> nullary $ b1 == b2
-                    (MKTuple _ _ args _        , PR_Tuple _ _ pats) -> do
-                        parts <- mapM (uncurry matchPatternAgainst) (zip pats args)
-                        let pms = concatMapStatuses parts
-                        if List.all guaranteedToMatch pats
-                          then case pms of
-                                 MatchDef defs -> return $ MatchDef defs
-                                 _ -> return $ MatchSeq (zip args pats)
-                          else return pms
-                        --trace ("matched tuple const against tuple pat " ++ show p ++ "\n, parts = " ++ show parts ++ " ;;; res = " ++ show res) res
+        matchPatternWithConst :: PatternRepr ty -> ConstStatus -> PatternMatchStatus
+        matchPatternWithConst p cs =
+          case (cs, p) of
+            (_, PR_Atom (P_Wildcard _ _  )) -> MatchDef []
+            (_, PR_Atom (P_Variable _ tid)) -> MatchDef [(tidIdent tid, cs)]
+            (IsVariable _  , _)     -> MatchAmbig
+            (IsConstant _ c, _)     -> matchConst c p
+              where matchConst c p =
+                      case (c, p) of
+                        (Lit _ (LitInt  i1), PR_Atom (P_Int  _ _ i2)) -> nullary $ litIntValue i1 == litIntValue i2
+                        (Lit _ (LitBool b1), PR_Atom (P_Bool _ _ b2)) -> nullary $ b1 == b2
+                        (LitTuple _ args _, PR_Tuple _ _ pats) ->
+                            let parts = map (uncurry matchPatternWithConst) (zip pats args) in
+                            let res = concatMapStatuses parts in
+                            res
+                            --trace ("matched tuple const against tuple pat " ++ show p ++ "\n, parts = " ++ show parts ++ " ;;; res = " ++ show res) res
+                        (KnownCtor _ (kid, _) args, PR_Ctor _ _ pats (LLCtorInfo cid _ _)) | kid == cid ->
+                            concatMapStatuses $ map (uncurry matchPatternWithConst) (zip pats args)
+                        (_ , _) -> nullary False
 
-                    (MKAppCtor _ _ (kid, _) args, PR_Ctor _ _ pats (LLCtorInfo cid _ _ _)) | kid == cid -> do
-                      parts <- mapM (uncurry matchPatternAgainst) (zip pats args)
-                      return $ concatMapStatuses parts
-
-                    (_ , _) -> return $ MatchAmbig
-        
-        -- guaranteedToMatch :: PatternRepr ty -> Bool
-        guaranteedToMatch p =
-          case p of
-            PR_Atom (P_Wildcard {}) -> True
-            PR_Atom (P_Variable {}) -> True
-            PR_Or    _ _ pats -> List.any guaranteedToMatch pats
-            PR_Tuple _ _ pats -> List.all guaranteedToMatch pats
-            PR_Ctor  _ _ pats ctorinfo -> ctorLLInfoLone ctorinfo && List.all guaranteedToMatch pats
-            _ -> False
-
-        nullary True  = return $ MatchDef []
-        nullary False = return $ MatchNeg
-                
-        concatMapStatuses :: [PatternMatchStatus ty] -> PatternMatchStatus ty
+        concatMapStatuses :: [PatternMatchStatus] -> PatternMatchStatus
         concatMapStatuses mbs = go mbs []
           where go []               acc = MatchDef (concat acc)
-                go [MatchSeq vp]    []  = MatchSeq vp
                 go (MatchNeg:_)    _acc = MatchNeg
                 go (MatchAmbig:_)  _acc = MatchAmbig
                 go ((MatchDef xs):rest) acc = go rest (xs : acc)
-                go ((MatchSeq _ ):_rst) _   = error $ "can't yet process MatchSeq embedded..."
-
-{-
--- TODO handle partial matches:
---        case (v1,v2) of (True, x) -> f(x)
---      can become
---        case (v1,v2) of (True, x) -> f(v2)
---      even thought it can't become simply ``f(v2)`` because v1 might not be True.
--}
 
-dbgDoc :: MonadIO m => Doc -> m ()
-dbgDoc d =
-  if False
-    then liftIO $ putDocLn d
-    else return ()
-
-dbgIf :: MonadIO m => Bool -> Doc -> m ()
-dbgIf cond d =
-  if cond
-    then liftIO $ putDocLn d
-    else return ()
-
-dbgCont = False
-dbgFinal = False
+-}-- }}} 
\ No newline at end of file
diff --git a/compiler/me/src/Foster/MainCtorHelpers.hs b/compiler/me/src/Foster/MainCtorHelpers.hs
--- a/compiler/me/src/Foster/MainCtorHelpers.hs
+++ b/compiler/me/src/Foster/MainCtorHelpers.hs
@@ -52,7 +52,7 @@
 ctorIdFor :: String -> DataCtor t -> (CtorName, CtorId)
 ctorIdFor tynm ctor = (dataCtorName ctor, ctorId tynm ctor)
 
-ctorId   tynm (DataCtor ctorName _tyformals types _repr _lone _range) =
+ctorId   tynm (DataCtor ctorName _tyformals types _repr _range) =
   CtorId tynm (T.unpack ctorName) (Prelude.length types)
 
 -----------------------------------------------------------------------
diff --git a/compiler/me/src/Foster/MonoType.hs b/compiler/me/src/Foster/MonoType.hs
--- a/compiler/me/src/Foster/MonoType.hs
+++ b/compiler/me/src/Foster/MonoType.hs
@@ -44,7 +44,7 @@
 instance IntSizedBits MonoType where
         intSizeBitsOf (PrimInt isb) = isb
         intSizeBitsOf (RefinedType v _ _) = intSizeBitsOf (tidType v)
-        intSizeBitsOf t = error $ "Unable to compute IntSizedBits for non-PrimInt type: " <> show (pretty t)
+        intSizeBitsOf _ = error $ "Unable to compute IntSizedBits for non-PrimInt type"
 
 extractFnType (FnType _ _ cc pf) = (cc, pf)
 extractFnType (PtrType (StructType [FnType _ _ cc FT_Proc, _])) = (cc, FT_Func)
diff --git a/compiler/me/src/Foster/Monomo.hs b/compiler/me/src/Foster/Monomo.hs
--- a/compiler/me/src/Foster/Monomo.hs
+++ b/compiler/me/src/Foster/Monomo.hs
@@ -410,9 +410,9 @@
    PR_Ctor     rng t pats ctor  -> liftM3 (PR_Ctor     rng) (monoType subst t) (mp pats)
                                                             (monoCtorInfo subst ctor)
 
-monoCtorInfo subst (LLCtorInfo cid repr tys isLoneCtor) = do
+monoCtorInfo subst (LLCtorInfo cid repr tys) = do
           tys' <- mapM (monoType subst) tys
-          return $ (LLCtorInfo cid repr tys' isLoneCtor)
+          return $ (LLCtorInfo cid repr tys')
 
 monomorphizedEffectDeclsFrom :: [EffectDecl TypeIL] -> [(String, [MonoType])] -> Mono [DataType MonoType]
 monomorphizedEffectDeclsFrom eds specs = do
@@ -427,9 +427,9 @@
          subst = extendSubst emptyMonoSubst formals args
 
          monomorphizedEffectCtor :: MonoSubst -> EffectCtor TypeIL -> Mono (DataCtor MonoType)
-         monomorphizedEffectCtor subst (EffectCtor (DataCtor name _tyformals types repr lone range) _outty) = do
+         monomorphizedEffectCtor subst (EffectCtor (DataCtor name _tyformals types repr range) _outty) = do
            types' <- mapM (monoType subst) types
-           return $ DataCtor name [] types' repr lone range
+           return $ DataCtor name [] types' repr range
 
        dtSpecMap = mapAllFromList specs
 
@@ -456,9 +456,9 @@
 
          monomorphizedCtor :: MonoSubst -> DataCtor TypeIL -> Mono (DataCtor MonoType)
          monomorphizedCtor subst
-                   (DataCtor name _tyformals types repr lone range) = do
+                   (DataCtor name _tyformals types repr range) = do
            types' <- mapM (monoType subst) types
-           return $ DataCtor name [] types' repr lone range
+           return $ DataCtor name [] types' repr range
 
        dtSpecMap = mapAllFromList specs
 
diff --git a/compiler/me/src/Foster/PatternMatch.hs b/compiler/me/src/Foster/PatternMatch.hs
--- a/compiler/me/src/Foster/PatternMatch.hs
+++ b/compiler/me/src/Foster/PatternMatch.hs
@@ -5,8 +5,7 @@
 -- found in the LICENSE.txt fCFe or at http://eschew.org/txt/bsd.txt
 -----------------------------------------------------------------------------
 
-module Foster.PatternMatch(DecisionTree(..), DataTypeSigs, compilePatterns)
-where
+module Foster.PatternMatch where
 
 import Prelude hiding ((<$>))
 
@@ -54,11 +53,12 @@
 
 -}
 
+type ClauseCol t = [SPattern t]
 data ClauseMatrix a t = ClauseMatrix [ClauseRow a t]
 data ClauseRow a t  = ClauseRow { rowOrigPat  :: (SPattern t)
                                 , rowPatterns :: [SPattern t]
                                 , rowAction   :: a
-                                ,_rowSourceRange :: SourceRange }
+                                , rowSourceRange :: SourceRange }
 
 data SPattern t = SP_Wildcard
                 | SP_Variable (TypedId t)
@@ -100,13 +100,13 @@
     (PR_Tuple _ _ pats)     -> SP_Ctor (tupleCtor pats) (map compilePattern pats)
     (PR_Or    _ _ pats)     -> SP_Or                    (map compilePattern pats)
     where
-          ctorInfo tynm dcnm dctys repr isLoneCtor =
-             LLCtorInfo (CtorId tynm dcnm (Prelude.length $ dctys)) repr dctys isLoneCtor
+          ctorInfo tynm dcnm dctys repr =
+             LLCtorInfo (CtorId tynm dcnm (Prelude.length $ dctys)) repr dctys
 
-          boolCtor False = ctorInfo "Bool"  "False" []                (CR_Value 0)   False
-          boolCtor True  = ctorInfo "Bool"  "True"  []                (CR_Value 1)   False
-          tupleCtor pats = ctorInfo "()"    "()"    (map typeOf pats) (CR_Default 0) True
-          intCtor ty li  = ctorInfo ctnm ("<"++ctnm++">") []          (CR_Value tag) False
+          boolCtor False = ctorInfo "Bool"  "False" []                (CR_Value 0)
+          boolCtor True  = ctorInfo "Bool"  "True"  []                (CR_Value 1)
+          tupleCtor pats = ctorInfo "()"    "()"    (map typeOf pats) (CR_Default 0)
+          intCtor ty li  = ctorInfo ctnm ("<"++ctnm++">") []          (CR_Value tag)
                              where
                               isb  = intSizeBitsOf ty
                               bits = intSizeOf     isb
@@ -156,7 +156,7 @@
       let caselist = [ ((c, r),
                            cc (expand o1  llci ++ orest)
                               (specialize (c,r) cm)  (ranges cm) allSigs)
-                     | llci@(LLCtorInfo c r _ _) <- Set.toList headCtorInfos] in
+                     | llci@(LLCtorInfo c r _) <- Set.toList headCtorInfos] in
       -- The selected column contains some set of constructors C1 .. Ck.
       -- Pair each constructor with the clause matrix obtained from
       -- specializing the current match matrix w/r/t that constructor,
@@ -200,7 +200,7 @@
 extendOccurrence :: Occurrence t -> Int -> LLCtorInfo t -> Occurrence t
 extendOccurrence occ n cinfo = occ ++ [(n, cinfo)]
 
-ctorInfoArity (LLCtorInfo cid _ _ _) = ctorArity cid
+ctorInfoArity (LLCtorInfo cid _ _) = ctorArity cid
 
 instance Pretty (ClauseMatrix a t) where
   pretty (ClauseMatrix rows) =
@@ -234,7 +234,7 @@
  ClauseMatrix (concat [specializeRow row cinfo | row <- rows])
   where
 
-    compatWith (LLCtorInfo c1 r1 _ _) (c2, r2) = c1 == c2 && r1 == r2
+    compatWith (LLCtorInfo c1 r1 _) (c2, r2) = c1 == c2 && r1 == r2
 
     specializeRow (ClauseRow orig []       a rng) _cinfo = [ClauseRow orig [] a rng]
     specializeRow (ClauseRow orig (p:rest) a rng)  cinfo@(ctor, _) =
diff --git a/compiler/me/src/Foster/Primitives.hs b/compiler/me/src/Foster/Primitives.hs
--- a/compiler/me/src/Foster/Primitives.hs
+++ b/compiler/me/src/Foster/Primitives.hs
@@ -22,14 +22,12 @@
             [TyAppP (TyConP "Array") [typ "Int8"]
             , typ "Int32"]
             (Just (CR_Default 0))
-            False -- is lone?
             (MissingSourceRange "Text.TextFragment")
         ,DataCtor (T.pack "TextConcat"  ) tf
             [typ "Text"
             ,typ "Text"
             ,typ "Int32"]
             (Just (CR_Default 1))
-            False -- is lone?
             (MissingSourceRange "Text.TextConcat")]
         False (MissingSourceRange "Text")),
 
diff --git a/compiler/me/src/Foster/ProtobufFE.hs b/compiler/me/src/Foster/ProtobufFE.hs
--- a/compiler/me/src/Foster/ProtobufFE.hs
+++ b/compiler/me/src/Foster/ProtobufFE.hs
@@ -81,22 +81,18 @@
         (name, expr) -> ToplevelDefn (name, expr)
     TList [tok, _, cbr, TList [tyformal_nm, mu_tyformals, mu_data_ctors]]
                                                       | tok `tm` tok_DATATYPE ->
-       let tyf = map cb_parse_tyformal (unMu mu_tyformals)
-           ctors = unMu mu_data_ctors
-           lone = length ctors == 1 in
+       let tyf = (map cb_parse_tyformal  (unMu mu_tyformals)) in
        ToplevelData $ DataType (cb_parse_tyformal      tyformal_nm)
                                    tyf
-                                   (map (cb_parse_data_ctor tyf lone) ctors)
+                                   (map (cb_parse_data_ctor tyf) (unMu mu_data_ctors))
                                    False
                                    (cb_parse_range          cbr)
     TList [tok, _, cbr, TList [tyformal_nm, mu_tyformals, mu_effect_ctors]]
                                                       | tok `tm` tok_EFFECT ->
-       let tyf = (map cb_parse_tyformal  (unMu mu_tyformals))
-           ctors = unMu mu_effect_ctors
-           lone = length ctors == 1 in
+       let tyf = (map cb_parse_tyformal  (unMu mu_tyformals)) in
        ToplevelEffect $ EffectDecl (cb_parse_tyformal      tyformal_nm)
                                    tyf
-                                   (map (cb_parse_effect_ctor tyf lone) ctors)
+                                   (map (cb_parse_effect_ctor tyf) (unMu mu_effect_ctors))
                                    (cb_parse_range          cbr)
 
     TList [tok, _,_cbr, TList [TList (tag:_), x, t]] | tok `tm` tok_FOREIGN
@@ -115,20 +111,19 @@
     _ -> error $ "cb_parseToplevelItem failed: " ++ show cbor
 
   -- ^(OF dctor tatom*);
-  cb_parse_data_ctor tyf lone cbor = case cbor of
+  cb_parse_data_ctor tyf cbor = case cbor of
     TList [tok, _, cbr, TList (dctor : tatoms)] | tok `tm` tok_OF ->
-      Foster.Base.DataCtor (cb_parse_dctor dctor) tyf (map cb_parse_tatom tatoms)
-                           Nothing lone (cb_parse_range cbr)
+      Foster.Base.DataCtor (cb_parse_dctor dctor) tyf (map cb_parse_tatom tatoms) Nothing (cb_parse_range cbr)
     _ -> error $ "cb_parse_data_ctor failed: " ++ show cbor
 
   cb_parse_dctor cbor = cb_parse_ctor cbor
 
   -- ^(OF dctor ^(MU tatom*) ^(MU t?));
-  cb_parse_effect_ctor tyf lone cbor = case cbor of
+  cb_parse_effect_ctor tyf cbor = case cbor of
     TList [tok, _, cbr, TList [dctor, mu_tatoms, mu_mb_t]] | tok `tm` tok_OF ->
       Foster.Base.EffectCtor
         (Foster.Base.DataCtor (cb_parse_dctor dctor) tyf (map cb_parse_tatom (unMu mu_tatoms))
-                              Nothing lone (cb_parse_range cbr))
+                              Nothing (cb_parse_range cbr))
         (case unMu mu_mb_t of
           []  -> -- Default to unit type if no explicit annotation
                  -- has been provided.
diff --git a/compiler/me/src/Foster/TypeAST.hs b/compiler/me/src/Foster/TypeAST.hs
--- a/compiler/me/src/Foster/TypeAST.hs
+++ b/compiler/me/src/Foster/TypeAST.hs
@@ -165,7 +165,6 @@
 f32 = TyAppAST (TyConAST "Float32") []
 
 primTyVars tyvars = map (\v -> (v, KindAnySizeType)) tyvars
-boxedTyVars tyvars = map (\v -> (v, KindPointerSized)) tyvars
 
 -- These names correspond to (the C symbols of)
 -- functions implemented by the Foster runtime.
@@ -255,8 +254,7 @@
 
     ,(,) "foster_subheap_create"   $ mkProcType [] [fosSubheapType]
     ,(,) "foster_subheap_create_small" $ mkProcType [] [fosSubheapType]
-    ,(,) "foster_subheap_activate" $ mkProcType [fosSubheapType] [fosSubheapType]
-    ,(,) "foster_subheap_ignore"   $ mkProcType [fosSubheapType] []
+    ,(,) "foster_subheap_activate" $ mkProcType [fosSubheapType] []
     ,(,) "foster_subheap_collect"  $ mkProcType [fosSubheapType] []
     ]
 
@@ -321,27 +319,16 @@
   ,mkPrim "-"       $ mkProcType [ty, ty] [ty]
   ,mkPrim "*"       $ mkProcType [ty, ty] [ty]
   ,mkPrim "div"     $ mkProcType [ty, ty] [ty]
-  ,mkPrim "rem"     $ mkProcType [ty, ty] [ty]
   ,mkPrim "<"       $ mkProcType [ty, ty] [i1]
   ,mkPrim ">"       $ mkProcType [ty, ty] [i1]
   ,mkPrim "<="      $ mkProcType [ty, ty] [i1]
   ,mkPrim ">="      $ mkProcType [ty, ty] [i1]
   ,mkPrim "=="      $ mkProcType [ty, ty] [i1]
   ,mkPrim "!="      $ mkProcType [ty, ty] [i1]
-  ,mkPrim "abs"     $ mkProcType [ty]     [ty]
   ,mkPrim "sqrt"    $ mkProcType [ty]     [ty]
-  ,mkPrim "exp"     $ mkProcType [ty]     [ty]
-  ,mkPrim "exp2"    $ mkProcType [ty]     [ty]
-  ,mkPrim "log"     $ mkProcType [ty]     [ty]
-  ,mkPrim "log2"    $ mkProcType [ty]     [ty]
-  ,mkPrim "log10"   $ mkProcType [ty]     [ty]
   ,mkPrim "sin"     $ mkProcType [ty]     [ty]
   ,mkPrim "cos"     $ mkProcType [ty]     [ty]
   ,mkPrim "tan"     $ mkProcType [ty]     [ty]
-  ,mkPrim "ceil"    $ mkProcType [ty]     [ty]
-  ,mkPrim "floor"   $ mkProcType [ty]     [ty]
-  ,mkPrim "round"   $ mkProcType [ty]     [ty]
-  ,mkPrim "trunc"   $ mkProcType [ty]     [ty]
   ,mkPrim "powi"    $ mkProcType [ty, i32]    [ty]
   ,mkPrim "muladd"  $ mkProcType [ty, ty, ty] [ty]
   ]
@@ -424,10 +411,6 @@
             ForAllAST (primTyVars [a])
               (mkProcType [RefTypeAST (TyVarAST a), RefTypeAST (TyVarAST a)] [fosBoolType])
 
-eqBoxedType = let a = BoundTyVar "a" (MissingSourceRange "==Boxed") in
-            ForAllAST (boxedTyVars [a])
-              (mkProcType [(TyVarAST a), (TyVarAST a)] [fosBoolType])
-
 eqArrayType = let a = BoundTyVar "a" (MissingSourceRange "==Ref") in
             ForAllAST (primTyVars [a])
               (mkProcType [ArrayTypeAST (TyVarAST a), ArrayTypeAST (TyVarAST a)] [fosBoolType])
@@ -438,7 +421,6 @@
   [(,) "not"                  $ (,) (mkFnType [i1]  [i1]  ) $ PrimOp "bitnot" i1
   ,(,) "==Bool"               $ (,) (mkFnType [i1,i1][i1] ) $ PrimOp "==" i1
   ,(,) "==Ref"                $ (,) eqRefType               $ PrimOp "==" (RefTypeAST unitTypeAST)
-  ,(,) "==Boxed"              $ (,) eqBoxedType             $ PrimOp "==" (unitTypeAST)
   ,(,) "sameArrayStorage"     $ (,) eqArrayType             $ PrimOp "==" (ArrayTypeAST unitTypeAST)
   ,(,) "f64-to-s32-unsafe"    $ (,) (mkFnType [f64] [i32] ) $ PrimOp "fptosi_f64_i32" i32
   ,(,) "f64-to-u32-unsafe"    $ (,) (mkFnType [f64] [i32] ) $ PrimOp "fptoui_f64_i32" i32
@@ -450,14 +432,10 @@
   ,(,) "u32-to-f64"    $(,) (mkFnType [i32] [f64]     ) $ PrimOp "uitofp_f64" i32
   ,(,) "f64-as-i64"    $(,) (mkFnType [f64] [i64]     ) $ PrimOp "bitcast_i64" f64
   ,(,) "i64-as-f64"    $(,) (mkFnType [i64] [f64]     ) $ PrimOp "bitcast_f64" i64
-  ,(,) "s64-to-f32-unsafe"    $ (,) (mkFnType [i64] [f32] ) $ PrimOp "sitofp_f32" i64
-  ,(,) "u64-to-f32-unsafe"    $ (,) (mkFnType [i64] [f32] ) $ PrimOp "uitofp_f32" i64
   ,(,) "s32-to-f32-unsafe"    $ (,) (mkFnType [i32] [f32] ) $ PrimOp "sitofp_f32" i32
   ,(,) "u32-to-f32-unsafe"    $ (,) (mkFnType [i32] [f32] ) $ PrimOp "uitofp_f32" i32
   ,(,) "f32-to-s32-unsafe"    $ (,) (mkFnType [f32] [i32] ) $ PrimOp "fptosi_f32_i32" i32
   ,(,) "f32-to-u32-unsafe"    $ (,) (mkFnType [f32] [i32] ) $ PrimOp "fptoui_f32_i32" i32
-  ,(,) "f32-to-f64"           $ (,) (mkFnType [f32] [f64] ) $ PrimOp "fpext_f64" f32
-  ,(,) "f64-to-f32"           $ (,) (mkFnType [f64] [f32] ) $ PrimOp "fptrunc_f32" f64
   ] ++ concatMap fixnumPrimitives [I32, I64, I8, I16, IWd, IDw]
     ++ sizeConversions
     ++ flonumPrimitives "f64" f64
diff --git a/compiler/me/src/Foster/Typecheck.hs b/compiler/me/src/Foster/Typecheck.hs
--- a/compiler/me/src/Foster/Typecheck.hs
+++ b/compiler/me/src/Foster/Typecheck.hs
@@ -1903,7 +1903,7 @@
   --  * only check for effect ctors at toplevel; nested ctors are data
   --  * merge somehow the CtorInfo and (CtorId, EffectCtor) paths in getCtorInfoForCtor
   EP_Ctor     r eps s -> do
-    info@(CtorInfo cid (DataCtor _ tyformals types _repr _lone _crng)) <- getCtorInfoForCtor ctx r s
+    info@(CtorInfo cid (DataCtor _ tyformals types _repr _crng)) <- getCtorInfoForCtor ctx r s
     sanityCheck (ctorArity cid == List.length eps) $
           "Incorrect pattern arity: expected " ++
           (show $ ctorArity cid) ++ " pattern(s), but got "
@@ -2368,7 +2368,7 @@
   [pretty n <> text ")" <+> hang (length (show n) + 2) d | (d, n) <- zip docs [1 :: Int ..]]
 
 tcDataCtor :: String -> Context SigmaTC -> DataCtor TypeTC -> Tc ()
-tcDataCtor dtname ctx (DataCtor nm _tyfs tys _repr _lone _rng) = do
+tcDataCtor dtname ctx (DataCtor nm _tyfs tys _repr _rng) = do
   let msg = "in field of data constructor " ++ T.unpack nm ++ " of type " ++ dtname
   mapM_ (tcTypeWellFormed msg ctx) tys
 -- }}}
@@ -2493,7 +2493,7 @@
   pretty (CtorInfo cid dc) = parens (text "CtorInfo" <+> text (show cid) <+> pretty dc)
 
 instance Pretty ty => Pretty (DataCtor ty) where
-  pretty (DataCtor name _tyformals _ctortyargs _repr _lone _range) =
+  pretty (DataCtor name _tyformals _ctortyargs _repr _range) =
         parens (text "DataCtor" <+> text (T.unpack name))
 
 retypeTID :: (t1 -> Tc t2) -> TypedId t1 -> Tc (TypedId t2)
diff --git a/compiler/me/src/Foster/TypecheckInt.hs b/compiler/me/src/Foster/TypecheckInt.hs
--- a/compiler/me/src/Foster/TypecheckInt.hs
+++ b/compiler/me/src/Foster/TypecheckInt.hs
@@ -125,19 +125,7 @@
                                text "the type" <+> pretty t]
   case base of
     2  -> error $ "binary float literals not yet implemented"
-    16 ->
-      if exptTooLargeForType expt ty then
-        tcFails [text "Exponent too large", highlightFirstLineDoc (rangeOf annot)]
-      else do
-       case Atto.parseOnly (hexDoubleParser <* Atto.endOfInput) $ T.pack clean of
-         Left err -> tcFails [text "Failed to parse hex float literal " <+> parens (text clean)
-                             ,highlightFirstLineDoc (rangeOf annot)
-                             ,text "Error was:"
-                             ,indent 8 (text err) ]
-         Right (part1,part2,denom,part3) -> do
-           let val = (fromInteger part1 +
-                      (fromInteger part2 / denom)) * (encodeFloat 2 (part3 - 1))
-           return (AnnLiteral annot ty (LitFloat $ LiteralFloat val originalText))
+    16 -> error $ "hex floating point literals not yet implemented"
     10 ->
       if exptTooLargeForType expt ty then
          tcFails [text "Exponent too large", highlightFirstLineDoc (rangeOf annot)]
@@ -153,14 +141,6 @@
             return (AnnLiteral annot ty (LitFloat $ LiteralFloat val originalText))
     _ -> error $ "Unexpected rational literal base " ++ show base
 
-hexDoubleParser :: Atto.Parser (Integer, Integer, Double, Int)
-hexDoubleParser = do
-  part1 <- Atto.hexadecimal
-  (t, part2) <- Atto.option (T.pack "", 0)
-                  (Atto.char '.' *> Atto.match Atto.hexadecimal)
-  part3 <- (Atto.asciiCI (T.pack "p") *> Atto.signed Atto.decimal)
-  return (part1, part2, 16.0 ** (fromIntegral $ T.length t), part3)
-
 tcMaybeWarnMisleadingRat range cleanText val = do
   -- It's possible that the literal given is "misleading",
   -- in the sense that it is neither the shortest string to
diff --git a/compiler/me/src/Foster/Worklist.hs b/compiler/me/src/Foster/Worklist.hs
--- a/compiler/me/src/Foster/Worklist.hs
+++ b/compiler/me/src/Foster/Worklist.hs
@@ -7,7 +7,7 @@
 module Foster.Worklist where
 
 import Data.Sequence
-import Data.Sequence as Seq(empty, fromList, length)
+import Data.Sequence as Seq(empty)
 
 -- Add new items to the right, and take items from the left.
 data WorklistQ item = WorklistQ (Seq item)
@@ -21,16 +21,9 @@
 worklistAdd :: WorklistQ item -> item -> WorklistQ item
 worklistAdd (WorklistQ s) a = WorklistQ (s |> a)
 
-worklistAddList :: WorklistQ item -> [item] -> WorklistQ item
-worklistAddList (WorklistQ s) xs = WorklistQ (s >< Seq.fromList xs)
-
 worklistGet :: WorklistQ item -> Maybe (item, WorklistQ item)
 worklistGet (WorklistQ s) =
   case viewl s of
         EmptyL  -> Nothing
         a :< s' -> Just (a, WorklistQ s')
 
-worklistLength :: WorklistQ item -> Int
-worklistLength (WorklistQ s) = Seq.length s
-        
-        
\ No newline at end of file
diff --git a/compiler/me/src/Main.hs b/compiler/me/src/Main.hs
--- a/compiler/me/src/Main.hs
+++ b/compiler/me/src/Main.hs
@@ -370,7 +370,7 @@
 
    extractCtorTypes :: DataType TypeAST -> [(String, Either TypeAST TypeAST, CtorId)]
    extractCtorTypes dt = map nmCTy (dataTypeCtors dt)
-     where nmCTy dc@(DataCtor name tyformals types _repr _lone _range) =
+     where nmCTy dc@(DataCtor name tyformals types _repr _range) =
                  (T.unpack name, ctorTypeAST tyformals dtType types, cid)
                          where dtType = typeOfDataType dt
                                cid    = ctorId (typeFormalName $ dataTypeName dt) dc
diff --git a/compiler/parse/FosterIL.capnp b/compiler/parse/FosterIL.capnp
--- a/compiler/parse/FosterIL.capnp
+++ b/compiler/parse/FosterIL.capnp
@@ -112,7 +112,6 @@
     ilnamedtypedecl   @ 19;
     ilunboxedtuple    @ 20;
     ilkillprocess     @ 21;
-    ilglobalappctor   @ 22;
   }
   parts   @ 0 : List(TermVar); # repeated
   tag     @ 1 : Tag;
@@ -276,15 +275,9 @@
   isForeign @ 2 : Bool;
 }
 
-struct CtorApp {
-  info  @ 0 : PbCtorInfo;
-  vars  @ 1 : List(TermVar);
-}
-
 struct PbToplevelItem {
   name         @ 1 : Text;
-  arr          @ 0 : PbArrayLiteral $optional;
-  lit          @ 2 : Letable $optional;
+  arr          @ 0 : PbArrayLiteral;
 }
 
 struct Module {
diff --git a/compiler/parse/FosterTypeAST.cpp b/compiler/parse/FosterTypeAST.cpp
--- a/compiler/parse/FosterTypeAST.cpp
+++ b/compiler/parse/FosterTypeAST.cpp
@@ -283,9 +283,10 @@
 
 // Returns {i64, [t, n]}*
 llvm::Type* ArrayTypeAST::getSizedArrayTypeRef(llvm::Type* t, int64_t n) {
-  return llvm::StructType::get(t->getContext(),
+  return getHeapPtrTo(
+          llvm::StructType::get(t->getContext(),
                                 { llvm::IntegerType::get(t->getContext(), 64)
-                                , llvm::ArrayType::get(t, n) });
+                                , llvm::ArrayType::get(t, n) }));
 }
 
 
@@ -295,7 +296,7 @@
 
 llvm::Type* ArrayTypeAST::getLLVMType() const {
   if (!repr) {
-    repr = getHeapPtrTo(getZeroLengthTypeRef(this->cell));
+    repr = getZeroLengthTypeRef(this->cell);
   }
   return repr;
 }
diff --git a/compiler/passes/CapnpToLLExpr.cpp b/compiler/passes/CapnpToLLExpr.cpp
--- a/compiler/passes/CapnpToLLExpr.cpp
+++ b/compiler/passes/CapnpToLLExpr.cpp
@@ -159,14 +159,6 @@
   return new LLCallPrimOp(e.getPrimopname(), tag, args);
 }
 
-LLExpr* parseGlobalAppCtor(const pb::Letable::Reader& e) {
-  std::vector<LLVar*> args;
-  for (auto p : e.getParts()) {
-    args.push_back(parseTermVar(p));
-  }
-  return new LLGlobalAppCtor(parseCtorInfo(e.getCtorinfo()), args);
-}
-
 LLExpr* parsePbInt(const pb::PBInt::Reader& i) {
   return new LLInt(i.getClean(), i.getBits());
 }
@@ -209,7 +201,7 @@
   if (a.hasCtorrepr()) {
     ctorRepr = parseCtorRepr(a.getCtorrepr());
   } else {
-    int bogusCtorTag = 42;
+    int bogusCtorTag = -2;
     ctorRepr.smallId       = bogusCtorTag;
     ctorRepr.isTransparent = false;
   }
@@ -392,11 +384,7 @@
 }
 
 LLTopItem* parseToplevelItem(const pb::PbToplevelItem::Reader& e) {
-  if (e.hasArr()) {
-    return new LLTopItem(e.getName(), parseTopArrayLiteral(e.getArr()));
-  } else {
-    return new LLTopItem(e.getName(), LLExpr_from_pb(e.getLit()));
-  }
+  return new LLTopItem(e.getName(), parseTopArrayLiteral(e.getArr()));
 }
 
 LLExpr* parseByteArray(const pb::Letable::Reader& e) {
@@ -470,7 +458,7 @@
   for (auto p : e.getParts()) {
     args.push_back(parseTermVar(p));
   }
-  return new LLUnboxedTuple(args, e.hasType());
+  return new LLUnboxedTuple(args);
 }
 
 LLExpr* parseKillProcess(const pb::Letable::Reader& e) {
@@ -556,7 +544,6 @@
   case pb::Letable::Tag::ILOBJECTCOPY: rv = parseObjectCopy(e); break;
   case pb::Letable::Tag::ILUNBOXEDTUPLE:rv =parseUnboxedTuple(e); break;
   case pb::Letable::Tag::ILKILLPROCESS:rv = parseKillProcess(e); break;
-  case pb::Letable::Tag::ILGLOBALAPPCTOR:rv = parseGlobalAppCtor(e); break;
 
   default:
     EDiag() << "Unknown protobuf tag: " << int(e.getTag());
diff --git a/compiler/passes/Codegen/Codegen-coro.cpp b/compiler/passes/Codegen/Codegen-coro.cpp
--- a/compiler/passes/Codegen/Codegen-coro.cpp
+++ b/compiler/passes/Codegen/Codegen-coro.cpp
@@ -60,14 +60,11 @@
   }
 }
 
-// in LLCodegen.cpp
-Value* getElementFromComposite(CodegenPass* pass, Value* compositeValue, int indexValue, const std::string& msg);
-
 void addCoroArgs(std::vector<Value*>& fnArgs,
                  llvm::Value* argVals) {
   llvm::StructType* sty;
   if (isSingleElementStruct(argVals->getType(), sty)) {
-    fnArgs.push_back(getElementFromComposite(nullptr, argVals, 0, "coroarg"));
+    fnArgs.push_back(getElementFromComposite(argVals, 0, "coroarg"));
   } else {
     fnArgs.push_back(argVals);
   }
diff --git a/compiler/passes/Codegen/Codegen-typemaps.cpp b/compiler/passes/Codegen/Codegen-typemaps.cpp
--- a/compiler/passes/Codegen/Codegen-typemaps.cpp
+++ b/compiler/passes/Codegen/Codegen-typemaps.cpp
@@ -192,8 +192,6 @@
                                  ArrayOrNot         arrayStatus,
                                  int8_t             ctorId,
                                  llvm::Module*      mod) {
-  //llvm::outs() << "Constructing type map with name " << name << " and ctor id " << int(ctorId) << "\n";
-
   int numPointers = pointerOffsets.size();
   StructType* typeMapTy = getTypeMapType(numPointers, mod);
 
@@ -318,8 +316,6 @@
                         std::string desiredName,
                         CtorRepr       ctorRepr,
                         llvm::Module* mod) {
-  ASSERT(structty != nullptr) << "registerStructType() needs a non-null type to work with.";
-
   static std::map<TypeSig, bool> registeredTypes;
 
   llvm::Type* ty = structty->getLLVMType();
@@ -394,7 +390,7 @@
   return false;
 }
 
-llvm::GlobalVariable* CodegenPass::getTypeMapForType(TypeAST* typ,
+llvm::GlobalVariable* getTypeMapForType(TypeAST* typ,
                                         CtorRepr ctorRepr,
                                         llvm::Module* mod,
                                         ArrayOrNot arrayStatus) {
@@ -415,33 +411,10 @@
   }
 
   if (!gv) {
-    EDiag() << "No type map for type " << str(ty) << "\nMaybe you need to call registerStructType()?\n";
+    EDiag() << "No type map for type " << str(ty) << "\n";
     exit(1);
-  } else {
-    
   }
 
   return gv;
 }
 
-void CodegenPass::emitTypeMapListGlobal() {
-  auto ty = builder.getInt8PtrTy();
-  std::vector<Constant*> vals;
-  for (auto p : typeMapCache) {
-    vals.push_back(llvm::ConstantExpr::getBitCast(p.second, ty));
-  }
-  vals.push_back(llvm::ConstantPointerNull::getNullValue(ty));
-
-  llvm::ArrayType* arrTy = llvm::ArrayType::get(ty, vals.size());
-  Constant* carr = llvm::ConstantArray::get(arrTy, vals);
-
-  GlobalVariable* typeMapListGlobal = new GlobalVariable(
-      /*Module=*/      *mod,
-      /*Type=*/        carr->getType(),
-      /*isConstant=*/  true,
-      /*Linkage=*/     GlobalValue::ExternalLinkage,
-      /*Initializer=*/ carr,
-      /*Name=*/        "foster__typeMapList");
-  typeMapListGlobal->setAlignment(8);
-}
-
diff --git a/compiler/passes/Codegen/CodegenUtils.cpp b/compiler/passes/Codegen/CodegenUtils.cpp
--- a/compiler/passes/Codegen/CodegenUtils.cpp
+++ b/compiler/passes/Codegen/CodegenUtils.cpp
@@ -49,7 +49,7 @@
   ASSERT(fosterBoundsCheck != NULL);
 
   Value* msg_array = builder.CreateGlobalString(srclines);
-  Value* msg = emitBitcast(msg_array, builder.getInt8PtrTy(), "boundscheck.msg");
+  Value* msg = builder.CreateBitCast(msg_array, builder.getInt8PtrTy());
   Value* ext = signExtend(idx, len64->getType());
   llvm::CallInst* call = builder.CreateCall(fosterBoundsCheck, { ext, len64, msg });
   markAsNonAllocating(call);
@@ -100,6 +100,27 @@
   return builder.CreateGEP(ptrToCompositeValue, llvm::makeArrayRef(idx), name.c_str());
 }
 
+Value* getElementFromComposite(Value* compositeValue, int indexValue,
+                               const std::string& msg) {
+  ASSERT(indexValue >= 0);
+  Value* idxValue = builder.getInt32(indexValue);
+  Type* compositeType = compositeValue->getType();
+  // To get an element from an in-memory object, compute the address of
+  // the appropriate struct field and emit a load.
+  if (llvm::isa<llvm::PointerType>(compositeType)) {
+    Value* gep = getPointerToIndex(compositeValue, idxValue, (msg + ".subgep").c_str());
+    return builder.CreateLoad(gep, gep->getName() + "_ld");
+  } else if (llvm::isa<llvm::StructType>(compositeType)) {
+    return builder.CreateExtractValue(compositeValue, indexValue, (msg + "subexv").c_str());
+  } else if (llvm::isa<llvm::VectorType>(compositeType)) {
+    return builder.CreateExtractElement(compositeValue, idxValue, (msg + "simdexv").c_str());
+  } else {
+    EDiag() << "Cannot index into value type " << str(compositeType)
+            << " with non-constant index " << str(idxValue);
+  }
+  return NULL;
+}
+
 ////////////////////////////////////////////////////////////////////
 
 Constant* getConstantArrayOfString(llvm::StringRef s, bool addNull) {
@@ -168,14 +189,13 @@
 }
 
 void markGCRoot(llvm::AllocaInst* stackslot, CodegenPass* pass) {
-  if (true || !pass->config.useGC) { return; }
   markGCRootWithMetadata(stackslot, pass, getSlotName(stackslot, pass));
 }
 
 extern char kFosterMain[];
 void CodegenPass::markFosterFunction(Function* f) {
   f->setAttributes(this->fosterFunctionAttributes);
-  if (this->config.useGC) { f->setGC("statepoint-example"); }
+  if (this->config.useGC) { f->setGC("fostergc"); }
 
   // We must not inline foster__main, which is marked with our gc,
   // into its caller, which is a gc-less function!
@@ -271,14 +291,7 @@
     emitRecordMallocCallsite(mod, typemap, linesgv);
   }
 
-  std::vector<llvm::Type*> args; args.push_back(typemap_type);
-  llvm::Value* mc_casted = builder.CreateBitCast(memalloc_cell,
-    rawPtrTo(llvm::FunctionType::get(
-      getUnitType()->getLLVMType(), args, false // isVarArg
-    )));
-
-  llvm::CallInst* mem = builder.CreateCall(mc_casted, typemap, "mem");
-  //llvm::CallInst* mem = builder.CreateCall(memalloc_cell, typemap, "mem");
+  llvm::CallInst* mem = builder.CreateCall(memalloc_cell, typemap, "mem");
 
   llvm::Type* ty = typ->getLLVMType();
   if (init) {
@@ -286,7 +299,7 @@
                                                   slotSizeOf(ty), /*align*/ 4));
   }
 
-  return emitBitcast(mem, ptrTo(ty), "ptr");
+  return builder.CreateBitCast(mem, ptrTo(ty), "ptr");
 }
 
 llvm::Value*
@@ -304,22 +317,10 @@
   llvm::Type* typemap_type = getFunctionTypeArgType(memalloc->getType(), 1);
   llvm::Value* typemap = builder.CreateBitCast(ti, typemap_type);
   llvm::Value* num_elts = signExtend(n, builder.getInt64Ty());
-
-  std::vector<llvm::Type*> args;
-  args.push_back(typemap_type);
-  args.push_back(builder.getInt64Ty());
-  args.push_back(builder.getInt8Ty());
-  llvm::Value* mc_casted = builder.CreateBitCast(memalloc,
-    rawPtrTo(llvm::FunctionType::get(
-      getUnitType()->getLLVMType(), args, false // isVarArg
-    )));
-
-  llvm::CallInst* mem = builder.CreateCall(mc_casted, { typemap, num_elts,
+  llvm::CallInst* mem = builder.CreateCall(memalloc, { typemap, num_elts,
                                                        builder.getInt8(init) }, "arrmem");
-  //llvm::CallInst* mem = builder.CreateCall(memalloc, { typemap, num_elts,
-  //                                                     builder.getInt8(init) }, "arrmem");
-  return emitBitcast(mem,
-           getHeapPtrTo(ArrayTypeAST::getZeroLengthTypeRef(elt_type)), "arr_ptr");
+  return builder.CreateBitCast(mem,
+                  ArrayTypeAST::getZeroLengthTypeRef(elt_type), "arr_ptr");
 }
 
 // If _template has type i32, returns (v & 31) unless v is a constant < 32, in
@@ -364,7 +365,7 @@
 llvm::Value*
 createCheckedOp(CodegenPass* pass,
                 IRBuilder<>& b, llvm::Value* v1, llvm::Value* v2,
-                    const char* valname, const std::string& op, llvm::Intrinsic::ID id) {
+                    const char* valname, llvm::Intrinsic::ID id) {
   unsigned  arr[] = { 1 };
   llvm::Value* agg = createIntrinsicCall2(b, v1, v2, valname, id);
   llvm::Value* overflow_bit = b.CreateExtractValue(agg, arr);
@@ -379,7 +380,7 @@
   b.SetInsertPoint(bb_err);
   std::string s;
   llvm::raw_string_ostream ss(s);
-  ss << "invariant violated for LLVM intrinisic corresponding to " << op
+  ss << "invariant violated for " << llvm::Intrinsic::getName(id)
      << "(" << str(v1->getType()) << ")"
      << "; try running under gdb and break on `foster__assert_failed`";
   llvm::Value* msg = builder.CreateBitCast(pass->getGlobalString(ss.str()),
@@ -458,23 +459,11 @@
   else if (op == "trunc_i64"){ return b.CreateTrunc(VL, b.getInt64Ty(), "trunci64tmp"); }
   else if (op == "trunc_w0") { return b.CreateTrunc(VL, getWordTy(b),   "truncw0tmp"); }
   else if (op == "trunc_w1") { return b.CreateTrunc(VL, getWordX2Ty(b), "truncw1tmp"); }
-  else if (op == "fpext_f64")   { return b.CreateFPExt(VL, b.getDoubleTy(), "fptexttmp"); }
-  else if (op == "fptrunc_f32") { return b.CreateFPTrunc(VL, b.getFloatTy(), "fptrunctmp"); }
   else if (op == "ctlz")     { return createCtlz(b, VL, "ctlztmp"); }
   else if (op == "ctpop")    { return createIntrinsicCall(b, VL, "ctpoptmp", llvm::Intrinsic::ctpop); }
   else if (op == "fsqrt")    { return createIntrinsicCall(b, VL, "fsqrttmp", llvm::Intrinsic::sqrt); }
-  else if (op == "fabs")     { return createIntrinsicCall(b, VL, "fabstmp",  llvm::Intrinsic::fabs); }
   else if (op == "fsin")     { return createIntrinsicCall(b, VL, "fsintmp",  llvm::Intrinsic::sin); }
   else if (op == "fcos")     { return createIntrinsicCall(b, VL, "fcostmp",  llvm::Intrinsic::cos); }
-  else if (op == "fexp")     { return createIntrinsicCall(b, VL, "fexptmp",  llvm::Intrinsic::exp); }
-  else if (op == "fexp2")    { return createIntrinsicCall(b, VL, "fexp2tmp", llvm::Intrinsic::exp2); }
-  else if (op == "flog"    ) { return createIntrinsicCall(b, VL, "flogtmp",  llvm::Intrinsic::log); }
-  else if (op == "flog2"   ) { return createIntrinsicCall(b, VL, "flogtmp",  llvm::Intrinsic::log2); }
-  else if (op == "flog10"  ) { return createIntrinsicCall(b, VL, "flogtmp",  llvm::Intrinsic::log10); }
-  else if (op == "fceil"   ) { return createIntrinsicCall(b, VL, "fceiltmp", llvm::Intrinsic::ceil); }
-  else if (op == "ffloor"  ) { return createIntrinsicCall(b, VL, "floortmp", llvm::Intrinsic::floor); }
-  else if (op == "fround"  ) { return createIntrinsicCall(b, VL, "frondtmp", llvm::Intrinsic::round); }
-  else if (op == "ftrunc"  ) { return createIntrinsicCall(b, VL, "ftrnctmp", llvm::Intrinsic::trunc); }
   else if (op == "fptosi_f64_i32") { return b.CreateFPToSI(VL, b.getInt32Ty(), "fptosi_f64_i32tmp"); }
   else if (op == "fptoui_f64_i32") { return b.CreateFPToUI(VL, b.getInt32Ty(), "fptoui_f64_i32tmp"); }
   else if (op == "fptosi_f64_i64") { return b.CreateFPToSI(VL, b.getInt64Ty(), "fptosi_f64_i64tmp"); }
@@ -507,12 +496,12 @@
        if (op == "+") { return b.CreateAdd(VL, VR, "addtmp", this->config.useNUW, this->config.useNSW); }
   else if (op == "-") { return b.CreateSub(VL, VR, "subtmp", this->config.useNUW, this->config.useNSW); }
   else if (op == "*") { return b.CreateMul(VL, VR, "multmp", this->config.useNUW, this->config.useNSW); }
-  else if (op == "+uc") { return createCheckedOp(this, b, VL, VR, "addtmp", op, llvm::Intrinsic::uadd_with_overflow); }
-  else if (op == "*uc") { return createCheckedOp(this, b, VL, VR, "multmp", op, llvm::Intrinsic::umul_with_overflow); }
-  else if (op == "-uc") { return createCheckedOp(this, b, VL, VR, "subtmp", op, llvm::Intrinsic::usub_with_overflow); }
-  else if (op == "+sc") { return createCheckedOp(this, b, VL, VR, "addtmp", op, llvm::Intrinsic::sadd_with_overflow); }
-  else if (op == "*sc") { return createCheckedOp(this, b, VL, VR, "multmp", op, llvm::Intrinsic::smul_with_overflow); }
-  else if (op == "-sc") { return createCheckedOp(this, b, VL, VR, "subtmp", op, llvm::Intrinsic::ssub_with_overflow); }
+  else if (op == "+uc") { return createCheckedOp(this, b, VL, VR, "addtmp", llvm::Intrinsic::uadd_with_overflow); }
+  else if (op == "*uc") { return createCheckedOp(this, b, VL, VR, "multmp", llvm::Intrinsic::umul_with_overflow); }
+  else if (op == "-uc") { return createCheckedOp(this, b, VL, VR, "subtmp", llvm::Intrinsic::usub_with_overflow); }
+  else if (op == "+sc") { return createCheckedOp(this, b, VL, VR, "addtmp", llvm::Intrinsic::sadd_with_overflow); }
+  else if (op == "*sc") { return createCheckedOp(this, b, VL, VR, "multmp", llvm::Intrinsic::smul_with_overflow); }
+  else if (op == "-sc") { return createCheckedOp(this, b, VL, VR, "subtmp", llvm::Intrinsic::ssub_with_overflow); }
   else if (op == "sdiv-unsafe") { return b.CreateSDiv(VL, VR, "sdivtmp"); }
   else if (op == "udiv-unsafe") { return b.CreateUDiv(VL, VR, "udivtmp"); }
   else if (op == "srem-unsafe") { return b.CreateSRem(VL, VR, "sremtmp"); }
@@ -521,7 +510,6 @@
   else if (op == "f+"         ) { return b.CreateFAdd(VL, VR, "faddtmp"); }
   else if (op == "f-"         ) { return b.CreateFSub(VL, VR, "fsubtmp"); }
   else if (op == "fdiv"       ) { return b.CreateFDiv(VL, VR, "fdivtmp"); }
-  else if (op == "frem"       ) { return b.CreateFRem(VL, VR, "fremtmp"); }
   else if (op == "f*"         ) { return b.CreateFMul(VL, VR, "fmultmp"); }
 
   else if (op ==  "<s"        ) { return b.CreateICmpSLT(VL, VR, "slttmp"); }
@@ -566,8 +554,6 @@
 // which it's awkward to arrange for the LLVM representation of the Foster
 // arg/return type to agree with Clang's representation. For these functions,
 // we want to generate a wrapper which bitcasts away the type mismatch.
-//
-// We'll create a function with the given symbolName which calls function F.
 void codegenAutoWrapper(llvm::Function* F,
                         llvm::FunctionType* wrappedTy,
                         //llvm::CallingConv::ID cc,
@@ -582,8 +568,8 @@
     std::vector<llvm::Value*> args;
     auto arg = Ffunc->arg_begin();
     for (int n = 0; arg != Ffunc->arg_end(); ++n) {
-      args.push_back(emitBitcast(&*arg,
-                        F->getFunctionType()->getParamType(n), "casted.arg"));
+      args.push_back(builder.CreateBitCast(&*arg,
+                        F->getFunctionType()->getParamType(n)));
       ++arg;
     }
 
@@ -596,7 +582,7 @@
         builder.CreateRet(getUnitValue());
       } else builder.CreateRetVoid();
     } else {
-      builder.CreateRet(emitBitcast(callInst, wrappedTy->getReturnType()));
+      builder.CreateRet(builder.CreateBitCast(callInst, wrappedTy->getReturnType()));
     }
     //pass->markFosterFunction(Ffunc);
 }
@@ -643,7 +629,7 @@
     llvm::CallInst* call = builder.CreateCall(f);
     call->setCallingConv(llvm::CallingConv::C);
     // Implicitly: called function may GC...
-    builder.CreateRet(emitBitcast(call, F->getReturnType()));
+    builder.CreateRet(builder.CreateBitCast(call, F->getReturnType()));
 }
 
 void codegenCall1ToFunctionWithArg(llvm::Function* F, llvm::Value* f, llvm::Value* n) {
@@ -653,7 +639,7 @@
     if (F->getReturnType()->isVoidTy()) {
       builder.CreateRetVoid();
     } else {
-      builder.CreateRet(emitBitcast(call, F->getReturnType()));
+      builder.CreateRet(builder.CreateBitCast(call, F->getReturnType()));
     }
 
 }
@@ -677,7 +663,7 @@
   virtual void codegenToFunction(CodegenPass* pass, llvm::Function* F) {
     pass->markFosterFunction(F);
     return codegenCall1ToFunction(F,
-            emitBitcast(pass->lookupFunctionOrDie("foster_get_cmdline_arg_n_raw"), this->type->getLLVMType()));
+             pass->lookupFunctionOrDie("foster_get_cmdline_arg_n_raw"));
   }
 };
 
@@ -694,7 +680,7 @@
   virtual void codegenToFunction(CodegenPass* pass, llvm::Function* F) {
     pass->markFosterFunction(F);
     return codegenCall1ToFunction(F,
-            emitBitcast(pass->lookupFunctionOrDie("foster_fmttime_secs_raw"), this->type->getLLVMType()));
+             pass->lookupFunctionOrDie("foster_fmttime_secs_raw"));
   }
 };
 
@@ -711,8 +697,8 @@
     // called by the runtime, which will do the appropriate intialization.
     CtorRepr bogusCtor; bogusCtor.smallId = -1;
     Value* dcoro = pass->emitMalloc(getSplitCoroTyp(getUnitType()), bogusCtor, "dcoro", /*init*/ true);
-    builder.CreateRet(emitBitcast(dcoro,
-                  getHeapPtrTo(foster_generic_coro_ast->getLLVMType())));
+    builder.CreateRet(builder.CreateBitCast(dcoro,
+                  ptrTo(foster_generic_coro_ast->getLLVMType())));
   }
 
   // Returns { { ... generic coro ... }, argTypes }
@@ -763,16 +749,15 @@
       std::vector<TypeAST*> argTypes;
       argTypes.push_back(foster::ParsingContext::lookupType("Subheap"));
       std::map<std::string, std::string> annots; annots["callconv"] = "ccc";
-      this->type = new FnTypeAST(foster::ParsingContext::lookupType("Subheap"),
+      this->type = new FnTypeAST(VoidTypeAST::get(),
                                  argTypes, annots);
   }
   virtual void codegenToFunction(CodegenPass* pass, llvm::Function* F) {
     pass->markFosterFunction(F);
     Function::arg_iterator AI = F->arg_begin();
     Value* n = &*(AI++);
-    codegenCall1ToFunctionWithArg(F,
-      emitBitcast(pass->lookupFunctionOrDie("foster_subheap_activate_raw"), this->type->getLLVMType()),
-      n);
+    codegenCall1ToFunctionWithArg(F, pass->lookupFunctionOrDie("foster_subheap_activate_raw"),
+      builder.CreateBitCast(n, builder.getInt8PtrTy()));
   }
 };
 
@@ -792,49 +777,11 @@
 
     Function::arg_iterator AI = F->arg_begin();
     Value* n = &*(AI++);
-    codegenCall1ToFunctionWithArg(F,
-      emitBitcast(pass->lookupFunctionOrDie("foster_subheap_collect_raw"), this->type->getLLVMType()),
-      n);
+    codegenCall1ToFunctionWithArg(F, pass->lookupFunctionOrDie("foster_subheap_collect_raw"),
+      builder.CreateBitCast(n, builder.getInt8PtrTy()));
   }
 };
 
-struct LLProcGCSafepointPoll : public LLProcPrimBase {
-  explicit LLProcGCSafepointPoll() {
-      this->name = "gc.safepoint_poll";
-      std::vector<TypeAST*> argTypes;
-      std::map<std::string, std::string> annots; annots["callconv"] = "ccc";
-      this->type = new FnTypeAST(VoidTypeAST::get(),
-                                 argTypes, annots);
-  }
-  virtual void codegenToFunction(CodegenPass* pass, llvm::Function* F) {
-    //pass->markFosterFunction(F);
-    llvm::CallInst* call = builder.CreateCall(pass->lookupFunctionOrDie("foster_gc_safepoint_poll"));
-    call->setCallingConv(llvm::CallingConv::C);
-    builder.CreateRetVoid();
-   }
- };
-
-
-struct LLProcSubheapIgnorePrim : public LLProcPrimBase {
-  explicit LLProcSubheapIgnorePrim() {
-      this->name = "foster_subheap_ignore";
-      this->argnames.push_back("handle");
-      std::vector<TypeAST*> argTypes;
-      argTypes.push_back(foster::ParsingContext::lookupType("Subheap"));
-      std::map<std::string, std::string> annots; annots["callconv"] = "ccc";
-      this->type = new FnTypeAST(VoidTypeAST::get(),
-                                 argTypes, annots);
-  }
-  virtual void codegenToFunction(CodegenPass* pass, llvm::Function* F) {
-    pass->markFosterFunction(F);
-
-    Function::arg_iterator AI = F->arg_begin();
-    Value* n = &*(AI++);
-    codegenCall1ToFunctionWithArg(F,
-      emitBitcast(pass->lookupFunctionOrDie("foster_subheap_ignore_raw"), this->type->getLLVMType()),
-      n);
-  }
-};
 
 void extendWithImplementationSpecificProcs(CodegenPass* _pass,
                                            std::vector<LLProc*>& procs) {
@@ -847,13 +794,10 @@
 
   procs.push_back(new LLProcAllocDefaultCoro());
 
-  procs.push_back(new LLProcGCSafepointPoll());
-
   procs.push_back(new LLProcSubheapCreatePrim());
   procs.push_back(new LLProcSubheapCreateSmallPrim());
   procs.push_back(new LLProcSubheapActivatePrim());
   procs.push_back(new LLProcSubheapCollectPrim());
-  procs.push_back(new LLProcSubheapIgnorePrim());
 }
 
 
diff --git a/compiler/passes/LLCodegen.cpp b/compiler/passes/LLCodegen.cpp
--- a/compiler/passes/LLCodegen.cpp
+++ b/compiler/passes/LLCodegen.cpp
@@ -48,7 +48,6 @@
 void codegenLL(LLModule* prog, llvm::Module* mod, CodegenPassConfig config) {
   CodegenPass cp(mod, config);
   prog->codegenModule(&cp);
-  cp.emitTypeMapListGlobal();
 }
 
 void deleteCodegenPass(CodegenPass* cp) { delete cp; }
@@ -60,7 +59,7 @@
 int  kUnknownBitsize = 999; // keep in sync with IntSizeBits in Base.hs
 
 // {{{ Internal helper functions
-bool tryBindArray(CodegenPass* pass, Value* base, Value*& arr, Value*& len);
+bool tryBindArray(Value* base, Value*& arr, Value*& len);
 
 namespace {
 
@@ -72,6 +71,15 @@
 llvm::Type* slotType(llvm::Type* t) { return t->getContainedType(0); }
 llvm::Type* slotType(llvm::Value* v) { return slotType(v->getType()); }
 
+bool isLargishStructPointerTy(llvm::Type* ty) {
+  if (llvm::PointerType* pt = llvm::dyn_cast<llvm::PointerType>(ty)) {
+    if (llvm::StructType* st = llvm::dyn_cast<llvm::StructType>(pt->getElementType())) {
+      return st->getNumElements() >= 2;
+    }
+  }
+  return false;
+}
+
 bool isPointerToUnknown(Type* ty) {
   return ty->isPointerTy() &&
          slotType(ty)->isIntegerTy(kUnknownBitsize);
@@ -101,7 +109,6 @@
   return true;
 }
 
-/*
 llvm::Value* emitBitcast(llvm::Value* v, llvm::Type* dstTy, llvm::StringRef msg = "") {
   llvm::Type* srcTy = v->getType();
   if (srcTy->isVoidTy()) {
@@ -117,7 +124,6 @@
 
   return builder.CreateBitCast(v, dstTy, msg);
 }
-*/
 
 llvm::Value* emitGCWrite(CodegenPass* pass, Value* val, Value* base, Value* slot) {
   if (!base) base = getNullOrZero(builder.getInt8PtrTy());
@@ -136,9 +142,9 @@
   llvm::outs() << "  slot :: " << str(slot->getType()) << "\n";
 */
 
-  Value* base_generic = emitBitcast(base, builder.getInt8PtrTy());
-  Value* slot_generic = emitBitcast(slot, builder.getInt8PtrTy()->getPointerTo(0));
-  Value*  val_generic = emitBitcast(val,  builder.getInt8PtrTy());
+  Value* base_generic = builder.CreateBitCast(base, builder.getInt8PtrTy());
+  Value* slot_generic = builder.CreateBitCast(slot, builder.getInt8PtrTy()->getPointerTo(0));
+  Value*  val_generic = builder.CreateBitCast(val,  builder.getInt8PtrTy());
   return builder.CreateCall(llvm_gcwrite, { val_generic, base_generic, slot_generic });
 }
 
@@ -159,14 +165,13 @@
                        llvm::Value* base,
                        llvm::Value* ptr,
                        WriteSelector w = WriteUnspecified) {
-  bool useBarrier = val->getType()->isPointerTy()
+
+
+  if (isPointerToType(ptr->getType(), val->getType())) {
+    if (val->getType()->isPointerTy()
         && !llvm::isa<llvm::AllocaInst>(ptr)
         && w != WriteKnownNonGC
-        && pass->config.useGC;
-  //maybeEmitCallToLogPtrWrite(pass, ptr, val, useBarrier);
-
-  if (isPointerToType(ptr->getType(), val->getType())) {
-    if (useBarrier) {
+        && pass->config.useGC) {
       return emitGCWrite(pass, val, base, ptr);
     } else {
       return builder.CreateStore(val, ptr, /*isVolatile=*/ false);
@@ -174,37 +179,25 @@
   }
 
   builder.GetInsertBlock()->getParent()->dump();
-  ASSERT(false) << "in basic block " << builder.GetInsertBlock()->getName() << ":\n"
-          << "ELIDING STORE DUE TO MISMATCHED TYPES:\n"
-          << "    ptr type: " << str(ptr->getType()) << "\n"
-          << "    val type: " << str(val->getType()) << "\n"
-          << "    val is  : " << str(val) << "\n"
-          << "    ptr is  : " << str(ptr);
+  ASSERT(false) << "ELIDING STORE DUE TO MISMATCHED TYPES:\n"
+          << "ptr type: " << str(ptr->getType()) << "\n"
+          << "val type: " << str(val->getType()) << "\n"
+          << "val is  : " << str(val) << "\n"
+          << "ptr is  : " << str(ptr);
   return NULL;
 }
 
-llvm::Type* // nullable
-needsBitcastToMediateUnknownPointerMismatch(llvm::Value* val, llvm::Value* ptr) {
-  if (ptr->getType()->isPointerTy()
-      && !isPointerToType(ptr->getType(), val->getType())) {
-    auto eltTy = llvm::dyn_cast<llvm::PointerType>(ptr->getType())->getElementType();
-    if (matchesExceptForUnknownPointers(val->getType(), eltTy)) {
-      return eltTy;
-    }
-  }
-  return nullptr;
-}
-
 llvm::Value* emitStore(CodegenPass* pass,
                        llvm::Value* val,
                        llvm::Value* ptr,
                        WriteSelector w = WriteUnspecified) {
-  if (val->getType()->isVoidTy()) {
-    val = getUnitValue();
-  }
-
-  if (auto eltTy = needsBitcastToMediateUnknownPointerMismatch(val, ptr)) {
-    val = emitBitcast(val, eltTy, "specSgen");
+  ASSERT(!val->getType()->isVoidTy());
+  if (ptr->getType()->isPointerTy()
+    && !isPointerToType(ptr->getType(), val->getType())) {
+    auto eltTy = llvm::dyn_cast<llvm::PointerType>(ptr->getType())->getElementType();
+    if (matchesExceptForUnknownPointers(val->getType(), eltTy)) {
+      val = emitBitcast(val, eltTy, "specSgen");
+    }
   }
 
   return emitGCWriteOrStore(pass, val, nullptr, ptr, w);
@@ -213,7 +206,7 @@
 Value* emitCallToInspectPtr(CodegenPass* pass, Value* ptr) {
    llvm::Value* rmc = pass->mod->getFunction("inspect_ptr_for_debugging_purposes");
    ASSERT(rmc) << "couldnt find inspect_ptr_for_debugging_purposes";
-   return markAsNonAllocating(builder.CreateCall(rmc, emitBitcast(ptr, builder.getInt8PtrTy())));
+   return markAsNonAllocating(builder.CreateCall(rmc, builder.CreateBitCast(ptr, builder.getInt8PtrTy())));
 }
 
 std::vector<llvm::Value*>
@@ -317,29 +310,6 @@
 
 // Implementation of CodegenPass helpers {{{
 
-Value* getElementFromComposite(CodegenPass* pass, Value* compositeValue,
-                               int indexValue, const std::string& msg) {
-  ASSERT(indexValue >= 0);
-  Value* idxValue = builder.getInt32(indexValue);
-  Type* compositeType = compositeValue->getType();
-  // To get an element from an in-memory object, compute the address of
-  // the appropriate struct field and emit a load.
-  if (llvm::isa<llvm::PointerType>(compositeType)) {
-    Value* gep = getPointerToIndex(compositeValue, idxValue, (msg + ".subgep").c_str());
-    //maybeEmitCallToLogPtrRead(pass, gep);
-    return emitNonVolatileLoad(gep, gep->getName() + "_ld");
-  } else if (llvm::isa<llvm::StructType>(compositeType)) {
-    return builder.CreateExtractValue(compositeValue, indexValue, (msg + "subexv").c_str());
-  } else if (llvm::isa<llvm::VectorType>(compositeType)) {
-    return builder.CreateExtractElement(compositeValue, idxValue, (msg + "simdexv").c_str());
-  } else {
-    EDiag() << "Cannot index into value type " << str(compositeType)
-            << " with non-constant index " << str(idxValue);
-  }
-  return NULL;
-}
-
-
 CodegenPass::CodegenPass(llvm::Module* m, CodegenPassConfig config)
     : config(config), mod(m), currentProcName("<no proc yet>") {
   //dib = new DIBuilder(*mod);
@@ -372,7 +342,7 @@
   llvm::Function* f = mod->getFunction(fullyQualifiedSymbol);
   assertHaveFunctionNamed(f, fullyQualifiedSymbol, this);
   if (llvm::Type* expTy = gDeclaredSymbolTypes[fullyQualifiedSymbol]) {
-    return emitBitcast(f, expTy);
+    return builder.CreateBitCast(f, expTy);
   } else {
     return f;
   }
@@ -391,7 +361,7 @@
   // so it does not need a GC root.
 
   Value* hstr_bytes; Value* len;
-  if (tryBindArray(this, hstr, /*out*/ hstr_bytes, /*out*/ len)) {
+  if (tryBindArray(hstr, /*out*/ hstr_bytes, /*out*/ len)) {
     markAsNonAllocating(builder.CreateMemCpy(hstr_bytes,
                               cstr, sz, /*alignment*/ 4));
   } else { ASSERT(false); }
@@ -431,7 +401,7 @@
 
 void createGCMapsSymbolIfNeeded(CodegenPass* pass) {
   if (!pass->config.useGC && !pass->config.standalone) {
-    // The runtime needs a "__LLVM_StackMaps" symbol for linking to succeed.
+    // The runtime needs a "foster__gcmaps" symbol for linking to succeed.
     // If we're not letting the GC plugin run, we'll need to emit it ourselves.
     new llvm::GlobalVariable(
     /*Module=*/      *(pass->mod),
@@ -439,7 +409,7 @@
     /*isConstant=*/  true,
     /*Linkage=*/     llvm::GlobalValue::ExternalLinkage,
     /*Initializer=*/ llvm::ConstantInt::get(builder.getInt32Ty(), 0),
-    /*Name=*/        "__LLVM_StackMaps",
+    /*Name=*/        "foster__gcmaps",
     /*InsertBefore=*/NULL,
     /*ThreadLocal=*/ llvm::GlobalVariable::NotThreadLocal);
   }
@@ -455,40 +425,14 @@
       //llvm::outs() << "addExternDecls() saw " << declName << " :: " << str(fosterType) << "\n";
       if (const FnTypeAST* fnty = fosterType->castFnTypeAST()) {
 
-        std::string autowrappedName = declName + std::string("__autowrap");
-        if (auto existing = pass->mod->getFunction(autowrappedName)) {
-          // If we import ``foo`` and we have a symbol ``foo__autowrap``, use it.
-          codegenAutoWrapper(existing, fnty->getLLVMFnType(), declName, pass);
-        } else if (llvm::Function* target = pass->mod->getFunction(declName)) {
-          // We have ``foo`` but no ``foo__autowrap``.
-
-          if ((!target->isDeclaration()) &&
-                str(target->getType()->getContainedType(0))
-                        != str(fnty->getLLVMFnType())) {
-            // If the function we import has a different type than we expected,
-            // automatically generate a wrapper to resolve the differences in types.
-            // The original function gets renamed; the new function wraps the original
-            // to provide the expected types to Foster code.
-            // We only generate a wrapper when we can rename the definition;
-            // renaming a declaration only causes problems when we link against the
-            // real definition.
-            target->setName(declName + std::string("__autowrap"));
-            codegenAutoWrapper(target, fnty->getLLVMFnType(), declName, pass);
-          } else {
-            // Nothing to do; either the imported symbol already has the right type,
-            // or it's a declaration instead a definition, so we can't rename it.
-          }
+        if (llvm::Function* wrapped = pass->mod->getFunction(d->getName() + "__autowrap")) {
+          codegenAutoWrapper(wrapped, fnty->getLLVMFnType(), declName, pass);
         } else {
-          // Unable to find either ``foo`` or ``foo__autowrap``; insert a declaration.
           pass->mod->getOrInsertFunction(declName, fnty->getLLVMFnType());
         }
 
-      } else { // Not a function type, must be a regular global.
-
-        auto g = pass->mod->getOrInsertGlobal(declName, fosterType->getLLVMType());
-        if (d->autoDeref) {
-          pass->autoDerefs[declName] = g;
-        }
+      } else {
+        pass->mod->getOrInsertGlobal(declName, fosterType->getLLVMType());
       }
 
     }
@@ -514,14 +458,7 @@
   }
 
   for (auto& item : items) {
-    //llvm::errs() << "Adding " << item->name << " to pass->globalValues\n";
-
-    if (item->arrlit) {
-      pass->globalValues[item->name] = item->arrlit->codegen(pass);
-    } else {
-      pass->globalValues[item->name] = item->lit->codegen(pass);
-      pass->insertScopedValue(item->name, pass->globalValues[item->name]);
-    }
+    pass->globalValues[item->name] = item->arrlit->codegen(pass);
   }
 
   // Ensure that the llvm::Function*s are created for all the function
@@ -551,7 +488,7 @@
 
     CtorRepr ctorRepr; ctorRepr.smallId = -1;
     auto globalCell = emitGlobalNonArrayCell(pass,
-                          pass->getTypeMapForType(TypeAST::i(64), ctorRepr, pass->mod, NotArray),
+                          getTypeMapForType(TypeAST::i(64), ctorRepr, pass->mod, NotArray),
                           const_cell,
                           cloname + ".closure.cell");
 
@@ -732,7 +669,7 @@
     if (const StructTypeAST* sty = rootvar->type->castStructTypeAST()) {
       registerStructType(sty, "unboxed_tuple", ctorRepr, pass->mod);
     }
-    llvm::GlobalVariable* typemap = pass->getTypeMapForType(rootvar->type, ctorRepr, pass->mod, NotArray);
+    llvm::GlobalVariable* typemap = getTypeMapForType(rootvar->type, ctorRepr, pass->mod, NotArray);
     auto padded_ty = llvm::StructType::get(foster::fosterLLVMContext,
                                             { builder.getInt64Ty(), builder.getInt64Ty(), ty });
     llvm::AllocaInst* slot = CreateEntryAlloca(padded_ty, rootvar->getName());
@@ -977,7 +914,7 @@
   if (v->getType()->isVoidTy() && tgt == getUnitType()->getLLVMType()) {
     // Can't cast a void value to a unit value,
     // but we can manufacture a unit ptr...
-    return getNullOrZero(tgt);
+    return llvm::ConstantPointerNull::getNullValue(tgt);
   } else return (v->getType() == tgt) ? v : emitBitcast(v, tgt);
 }
 
@@ -1016,15 +953,15 @@
   llvm::outs() << "  base is " << *base << "\n";
   llvm::outs() << "  slot is " << *slot << "\n";
 
-  Value* base_generic = emitBitcast(base, builder.getInt8PtrTy());
-  Value* slot_generic = emitBitcast(slot, builder.getInt8PtrTy()->getPointerTo(0));
+  Value* base_generic = builder.CreateBitCast(base, builder.getInt8PtrTy());
+  Value* slot_generic = builder.CreateBitCast(slot, builder.getInt8PtrTy()->getPointerTo(0));
   Value* val_generic = builder.CreateCall(llvm_gcread, { base_generic, slot_generic });
-  return emitBitcast(val_generic, slot->getType()->getPointerElementType());
+  return builder.CreateBitCast(val_generic, slot->getType()->getPointerElementType());
 }
 
 llvm::Value* LLDeref::codegen(CodegenPass* pass) {
   llvm::Value* ptr = base->codegen(pass);
-  if (false && isTraced && !llvm::isa<llvm::AllocaInst>(ptr)) {
+  if (isTraced && !llvm::isa<llvm::AllocaInst>(ptr)) {
     return emitGCRead(pass, nullptr, ptr);
   } else {
     return emitNonVolatileLoad(ptr, "deref");
@@ -1084,7 +1021,7 @@
     auto newty = rawPtrTo(llvm::FunctionType::get(builder.getInt1Ty(), 
                                   { getHeapPtrTo(foster_generic_split_coro_ty) }, false));
     llvm::outs() << "trying to cast " << str(orig->getType()) << "\nto\n" << str(newty) << "\n";
-    return emitBitcast(orig, newty);
+    return builder.CreateBitCast(orig, newty);
   }
   if (this->primName == "coro_parent") {
     auto fn = Function::Create(
@@ -1102,7 +1039,7 @@
     BasicBlock* prevBB = builder.GetInsertBlock();
     pass->addEntryBB(fn);
     builder.CreateRet(
-      emitBitcast(pass->getCurrentCoroParent(),
+      builder.CreateBitCast(pass->getCurrentCoroParent(),
         getHeapPtrTo(foster_generic_coro_t)));
     if (prevBB) {
       builder.SetInsertPoint(prevBB);
@@ -1183,26 +1120,23 @@
   return emitPrivateGlobal(pass, const_cell, name);
 }
 
-// Returns a tidy pointer.
 llvm::Value* emitByteArray(CodegenPass* pass, llvm::StringRef bytes, llvm::StringRef cellname) {
   auto const_arr_tidy = emitConstantArrayTidy(bytes.size(), getConstantArrayOfString(bytes));
 
   CtorRepr ctorRepr; ctorRepr.smallId = -1;
   auto arrayGlobal = emitGlobalArrayCell(pass,
-                        pass->getTypeMapForType(TypeAST::i(8), ctorRepr, pass->mod, YesArray),
+                        getTypeMapForType(TypeAST::i(8), ctorRepr, pass->mod, YesArray),
                         const_arr_tidy,
                         cellname);
 
-  return emitBitcast(getPointerToIndex(arrayGlobal, builder.getInt32(1), "cellptr"),
-            ptrTo(ArrayTypeAST::getZeroLengthTypeRef(TypeAST::i(8))), "arr_ptr");
+  return builder.CreateBitCast(getPointerToIndex(arrayGlobal, builder.getInt32(1), "cellptr"),
+                                ArrayTypeAST::getZeroLengthTypeRef(TypeAST::i(8)), "arr_ptr");
 }
 
 llvm::Value* LLText::codegen(CodegenPass* pass) {
   Value* hstr = emitByteArray(pass, this->stringValue, ".txt_cell");
   Value* textfragment = pass->lookupFunctionOrDie("TextFragment");
-  auto call = builder.CreateCall(textfragment, {
-        emitBitcast(hstr, getHeapPtrTo(ArrayTypeAST::getZeroLengthTypeRef(TypeAST::i(8)))),
-        builder.getInt32(this->stringValue.size()) });
+  auto call = builder.CreateCall(textfragment, { hstr, builder.getInt32(this->stringValue.size()) });
   call->setCallingConv(parseCallingConv("fastcc"));
   return call;
 }
@@ -1216,10 +1150,6 @@
   // remiss to use the nullary global closure wrapper instead of the proc!
   bool isProc = NULL != this->type->castFnTypeAST();
 
-  if (auto v = pass->autoDerefs[this->name]) {
-    return builder.CreateLoad(v, this->name);
-  }
-
   auto v = pass->globalValues[this->name];
   if (!v || isProc) {
     if (!v) llvm::errs() << "falling back to global proc instead of closure for " << this->name << "\n";
@@ -1290,7 +1220,7 @@
     // We enforce the invariant that the GC will scan but not attempt to copy
     // stack-allocated cells to the heap, by marking stack memory regions
     // as "stable" in foster_gc.cpp.
-    llvm::GlobalVariable* ti = pass->getTypeMapForType(type, ctorRepr, pass->mod, NotArray);
+    llvm::GlobalVariable* ti = getTypeMapForType(type, ctorRepr, pass->mod, NotArray);
     llvm::Type* typemap_type = ti->getType();
     // We include padding in order for the padding plus the typemap pointer to
     // be 16 bytes wide. This, in turn, ensures that we will align the payload
@@ -1316,11 +1246,6 @@
   }
 }
 
-llvm::Value* emitNullaryCtor(CtorRepr ctorRepr, llvm::Type* ptrty) {
-  llvm::Value* val = builder.getInt8(ctorRepr.smallId);
-  return builder.CreateIntToPtr(val, ptrty);
-}
-
 // If we represent a constant array as a globally-allocated/static value,
 // we simply won't call this function.
 llvm::Value* LLAllocate::codegen(CodegenPass* pass) {
@@ -1334,18 +1259,9 @@
     }
     if (this->ctorRepr.isNullary) {
       emitFakeComment("nullary ctor!");
-
       llvm::Value* val = builder.getInt8(this->ctorRepr.smallId);
       llvm::Type* ptrty = getHeapPtrTo(this->type->getLLVMType());
-//    return builder.CreateIntToPtr(val, ptrty);
-      /*
-      return emitNullaryCtor(this->ctorRepr, getHeapPtrTo(this->type->getLLVMType()));
-      */
-      return
-        emitBitcast(
-          builder.CreateCall(pass->mod->getFunction("foster__get_ctor_as_ptr"), { val }),
-          ptrty);
-
+      return builder.CreateIntToPtr(val, ptrty);
       // return null pointer, or'ed with ctor smallId, bitcast to appropriate result.
     } else {
       return allocateCell(pass, this->type, this->region, this->ctorRepr,
@@ -1358,7 +1274,7 @@
 //////////////// Arrays ////////////////////////////////////////////
 /////////////////////////////////////////////////////////////////{{{
 
-bool tryBindArray(CodegenPass* pass, llvm::Value* base, Value*& arr, Value*& len) {
+bool tryBindArray(llvm::Value* base, Value*& arr, Value*& len) {
   // {i64, [0 x T]}*
   if (isPointerToStruct(base->getType())) {
     llvm::Type* sty = slotType(base);
@@ -1368,7 +1284,7 @@
         llvm::dyn_cast<llvm::ArrayType>(sty->getContainedType(1))) {
         if (aty->getNumElements() == 0) {
           arr = getPointerToIndex(base, builder.getInt32(1), "arr");
-          len = getElementFromComposite(pass, base, 0, "len");
+          len = getElementFromComposite(base, 0, "len");
           return true;
         }
       }
@@ -1382,11 +1298,11 @@
   Value* arr = NULL; Value* len;
 
   if (isPointerToUnknown(base->getType())) {
-    auto arrayType = getHeapPtrTo(ArrayTypeAST::getSizedArrayTypeRef(ty, 0));
+    auto arrayType = ArrayTypeAST::getSizedArrayTypeRef(ty, 0);
     base = emitBitcast(base, arrayType, "genAspec");
   }
 
-  if (tryBindArray(pass, base, arr, len)) {
+  if (tryBindArray(base, arr, len)) {
     if (dynCheck && !pass->config.disableAllArrayBoundsChecks) {
       emitFosterArrayBoundsCheck(pass->mod, idx, len, srclines);
     }
@@ -1428,11 +1344,6 @@
   Value* val  = this->value->codegen(pass);
   Value* base = NULL;
   Value* slot = ari->codegenARI(pass, &base, val->getType());
-
-  if (auto eltTy = needsBitcastToMediateUnknownPointerMismatch(val, slot)) {
-    val = emitBitcast(val, eltTy, "specSgen");
-  }
-
   //builder.CreateStore(val, slot, /*isVolatile=*/ false);
   emitGCWriteOrStore(pass, val, base, slot);
   return getNullOrZero(getUnitType()->getLLVMType());
@@ -1442,7 +1353,7 @@
 llvm::Value* LLArrayLength::codegen(CodegenPass* pass) {
   Value* val  = this->value->codegen(pass);
   Value* _bytes; Value* len;
-  if (tryBindArray(pass, val, /*out*/ _bytes, /*out*/ len)) {
+  if (tryBindArray(val, /*out*/ _bytes, /*out*/ len)) {
     // len already assigned.
   } else { ASSERT(false); }
   return len;
@@ -1480,12 +1391,12 @@
 
     CtorRepr ctorRepr; ctorRepr.smallId = -1;
     auto arrayGlobal = emitGlobalArrayCell(pass,
-                          pass->getTypeMapForType(this->elem_type, ctorRepr, pass->mod, YesArray),
+                          getTypeMapForType(this->elem_type, ctorRepr, pass->mod, YesArray),
                           const_arr_tidy,
                           ".arr_cell");
 
-    return emitBitcast(getPointerToIndex(arrayGlobal, builder.getInt32(1), "cellptr"),
-                         getHeapPtrTo(ArrayTypeAST::getZeroLengthTypeRef(this->elem_type)), "arr_ptr");
+    return builder.CreateBitCast(getPointerToIndex(arrayGlobal, builder.getInt32(1), "cellptr"),
+                                 ArrayTypeAST::getZeroLengthTypeRef(this->elem_type), "arr_ptr");
   } else {
     llvm::GlobalVariable* arrayGlobal = emitPrivateGlobal(pass, const_arr, ".arr");
 
@@ -1493,7 +1404,7 @@
     llvm::Value* heap_arr = this->arr->codegen(pass);
 
     Value* heapmem; Value* _len;
-    if (tryBindArray(pass, heap_arr, /*out*/ heapmem, /*out*/ _len)) {
+    if (tryBindArray(heap_arr, /*out*/ heapmem, /*out*/ _len)) {
       MEMCPY_FROM_GLOBAL_TO_HEAP++;
       // Memcpy from global to heap.
 
@@ -1510,9 +1421,7 @@
         unsigned k  = ncvals[i].second;
         Value* val  = ncvals[i].first;
         Value* slot = getPointerToIndex(heapmem, llvm::ConstantInt::get(i32, k), "arr_slot");
-        bool useBarrier = val->getType()->isPointerTy() && pass->config.useGC;
-        //maybeEmitCallToLogPtrWrite(pass, slot, val, useBarrier);
-        if (useBarrier) {
+        if (val->getType()->isPointerTy() && pass->config.useGC) {
           emitGCWrite(pass, val, heapmem, slot);
         } else {
           builder.CreateStore(val, slot, /*isVolatile=*/ false);
@@ -1565,68 +1474,7 @@
 }
 
 Value* LLUnboxedTuple::codegen(CodegenPass* pass) {
-  if (this->isStatic) {
-    if (this->vars.empty()) {
-        return getUnitValue();
-    } else {
-        std::vector<llvm::Constant*> consts;
-        for (auto v : this->vars) {
-          auto gv = pass->globalValues[v->getName()];
-          if (auto cgv = dyn_cast<llvm::Constant>(gv)) {
-            consts.push_back(cgv);
-          } else {
-            llvm::errs() << "var  " << v->getName() << " was not constant! ... " << *gv << "\n";
-            exit(2);
-          }
-        }
-
-        auto ct = llvm::ConstantStruct::getAnon(consts);
-        CtorRepr ctorRepr; ctorRepr.smallId = 126;
-        // TODO merge type maps for similar types?
-        
-        std::vector<int> noSkippedIndices;
-        registerStructType(this->type->castStructTypeAST(), "cstupty", ctorRepr, pass->mod);
-        auto typemap = pass->getTypeMapForType(this->type, ctorRepr, pass->mod, NotArray);
-        auto globalCell = emitGlobalNonArrayCell(pass, typemap, ct, "cstup");
-        llvm::Type* ty = ptrTo(getLLVMType(this->type)); // heap-formatted but not on-heap.
-        llvm::outs() << "************ " << "type map for unboxed (?) type " << str(this->type) << "\n";
-        return emitBitcast(builder.CreateConstGEP2_32(NULL, globalCell, 0, 2), ty);
-        //return emitPrivateGlobal(pass, ct, "cstup");
-        /*
-    llvm::GlobalVariable* emitGlobalNonArrayCell(CodegenPass* pass,
-                                        llvm::GlobalVariable* typemap,
-                                        llvm::Constant* body,
-                                        const std::string& name) {
-                                        */
-
-    }
-  } else {
-    return createUnboxedTuple(codegenAll(pass, this->vars));
-  }
-}
-
-
-Value* LLGlobalAppCtor::codegen(CodegenPass* pass) {
-  llvm::PointerType* ty = asNonHeap(getLLVMType(this->type));
-  if (this->args.empty()) {
-    return emitNullaryCtor(this->ctor.ctorId.ctorRepr, ty);
-  }
-
-  std::vector<llvm::Constant*> consts;
-  for (auto v : this->args) {
-    auto gv = pass->globalValues[v->getName()];
-    if (auto cgv = dyn_cast<llvm::Constant>(gv)) {
-      consts.push_back(cgv);
-    } else {
-      llvm::errs() << "var  " << v->getName() << " was not constant! ... " << *gv << "\n";
-      exit(2);
-    }
-  }
-
-  llvm::GlobalVariable* ti = pass->getTypeMapForType(type, this->ctor.ctorId.ctorRepr, pass->mod, NotArray);
-  auto ct = llvm::ConstantStruct::getAnon(consts);
-  auto globalCell = emitGlobalNonArrayCell(pass, ti, ct, "csctor");
-  return emitBitcast(builder.CreateConstGEP2_32(NULL, globalCell, 0, 2), ty);
+  return createUnboxedTuple(codegenAll(pass, this->vars));
 }
 
 ///}}}//////////////////////////////////////////////////////////////
@@ -1671,7 +1519,7 @@
       continue;
     }
 
-    v = getElementFromComposite(pass, v, offsets[i], "switch_insp");
+    v = getElementFromComposite(v, offsets[i], "switch_insp");
   }
 
   // Consider code like         case v of Some x -> ... x ...
@@ -1784,8 +1632,8 @@
     ASSERT(isPointerToStruct(FV->getType()));
     // Load code and env pointers from closure...
     llvm::Value* envPtr =
-        getElementFromComposite(pass, FV, 1, "getCloEnv");
-    FV = getElementFromComposite(pass, FV, 0, "getCloCode");
+        getElementFromComposite(FV, 1, "getCloEnv");
+    FV = getElementFromComposite(FV, 0, "getCloCode");
     FT = dyn_cast<llvm::FunctionType>(slotType(FV));
     // Pass env pointer as first parameter to function.
     valArgs.push_back(envPtr);
diff --git a/grammar/foster.g b/grammar/foster.g
--- a/grammar/foster.g
+++ b/grammar/foster.g
@@ -52,9 +52,9 @@
 imports :       ('snafuinclude' id s=DQUO_STR ';')     -> ^(SNAFUINCLUDE id $s);
 
 decl_or_defn :
-        'REC'? x ( '::' t ';'             -> ^(DECL x t)
-                 | EQ phrase ';'          -> ^(DEFN x phrase) // We should allow suffixes, but only of type application.
-                 )
+        x ( '::' t ';'                    -> ^(DECL x t)
+          | EQ phrase ';'                 -> ^(DEFN x phrase) // We should allow suffixes, but only of type application.
+          )
         | data_defn ';'                   -> data_defn
         | effect_defn ';'                 -> effect_defn
         | FOREIGN IMPORT x (AS id)? '::' t ';'
@@ -73,7 +73,7 @@
                          effect_ctor*             -> ^(EFFECT $nm ^(MU $args*) ^(MU effect_ctor*));
 effect_ctor : OF dctor tatom* ('=>' t)?           -> ^(OF dctor ^(MU tatom*) ^(MU t?));
 
-opr     :       SYMBOL | MINUS;
+opr     :       SYMBOL;
 id      :       SMALL_IDENT | UPPER_IDENT | UNDER_IDENT;
 
 name    :     id ('.' name -> ^(QNAME id name)
@@ -107,12 +107,14 @@
 stmts   :  stmt_ stmt_cont* ';'* -> ^(STMTS stmt_ stmt_cont*);
 stmt_   : abinding | e ;
 stmt_cont : semi=';'+ stmt_ -> ^(MU $semi stmt_);
-abinding : 'REC' pbinding -> ^(ABINDING 'REC' pbinding)
-         |       pbinding -> ^(ABINDING       pbinding);
+abinding : 'REC' idbinding -> ^(ABINDING 'REC' idbinding)
+         |        pbinding -> ^(ABINDING        pbinding);
+
+idbinding : xid '=' e    -> ^(BINDING xid e);
 pbinding  : patbind '=' e    -> ^(BINDING patbind e);
 
 patbind :
-  xid                                      // variables
+  pid                                      // variables
   | '_'                  -> ^(WILDCARD)    // wildcards
   | 'let' '(' p (',' p)* ')'   -> ^(TUPLE p+)    // tuples (products)
   ;
@@ -129,7 +131,7 @@
         ;
 
 nopr    : name | opr ;
-phrase  :       lvalue+                         -> ^(PHRASE lvalue+)
+phrase  :       '-'?   lvalue+                  -> ^(PHRASE '-'?  lvalue+)
         |       'prim' nopr tyapp? lvalue*      -> ^(PRIMAPP nopr ^(MU tyapp?) lvalue*);
 lvalue  :              atom suffix*             -> ^(LVALUE atom suffix*);
 
@@ -263,7 +265,6 @@
 //    12.34`56
 //    12.34e+01
 //    12.34e-10
-//    0x1.2p3
 //    0xe
 //    1e4
 
diff --git a/notes/misc.rst b/notes/misc.rst
--- a/notes/misc.rst
+++ b/notes/misc.rst
@@ -137,68 +137,16 @@
 .. note:
         See also https://downloads.haskell.org/~ghc/latest/docs/html/users_guide/hp2ps.html
 
-
-Native Code Interop Example: SDL2
----------------------------------
-
-A command line to build a C++ program against SDL2 might look something like this::
-
-    clang++ simplegl.cpp -O2 -lm -lSDL2 -lGL -lGLEW -std=c++11 -o simplegl.exe
-
-Foster provides (some) support for linking aginst such libraries as well.
-Foster's foreign language support is oriented around functions and primitive types.
-Unlike the equivalent C++ program, Foster cannot make direct use of the preprocessor,
-nor can Foster access constants or perform direct struct member lookups.
-To bridge the gap, you must wrap such functionality in a small auxilliary C library.
-For the "hello world" equivalent in SDL, we only need two such helper functions::
-
-    #include <SDL2/SDL.h>
-
-    SDL_PixelFormat* SDL_GetSurfaceFormat(SDL_Surface* s) { return s->format; }
-    SDL_Rect* SDL_NullRect() { return NULL; }
-
-These symbols can be imported and used on the Foster side like so::
-
-    foreign type SDLPixelFormat;
-    foreign type SDLSurface;
-    foreign import SDL_GetSurfaceFormat as sdlGetSurfaceFormat :: { SDLSurface => SDLPixelFormat };
-
-    main = {
-       ...
-       surface = ...;
-       pixfmt = sdlGetSurfaceFormat surface;
-       ...
-    };
-
-We begin by compiling the above library (in ``sdlWrap.c``) to LLVM bitcode::
-
-    clang sdlWrap.c -emit-llvm -c -o sdlWrap.bc
-
-Putting potential hot-loop operations, such as struct accesses, behind a function call
-boundary might seem doomed to be slow. But fear not!
-LLVM's powerful optimizer will boil away the wrapper functions when we compile
-our program with ``--backend-optimize``.
-
-We can then compile and run our program, linking the SDL library and our wrapper::
-
-    runfoster simplegl.foster --nativelib SDL --bitcode sdlWrap.bc --backend-optimize
-
-We can also compile to a native executable::
-
-    fosterc   simplegl.foster --nativelib SDL --bitcode sdlWrap.bc --backend-optimize -o fostergl.exe
-
-
-
 Performance-related notes
 -------------------------
 
 * The middle-end compiler takes 2m2s to build with -O2, and roughly 48s to build without optimization.
   The middle-end then runs about 30% faster, but serialization time is not affected at all.
 
-* In a hello-world comparison, the foster binary is ~53KB bigger than the C binary.
+* foster-generated binaries require C++ shared libraries (chromium_base, etc)
+  due to the runtime. In a hello-world comparison, the foster binary is ~50KB bigger
+  than the C binary, and dynamic linking etc takes about 2ms.
   Use of ``strings`` suggests strings account for 14KB of the size increase.
-  Foster-generated binaries dynamically link against a minimial selection of "standard" libraries:
-  libc, libm, librt, pthreads, libgcc_s, and libstdc++
 
 * fannkuchredux(-nogc)-unchecked
     runs 100% slower than the reference C program.
diff --git a/runtime/foster_globals.cpp b/runtime/foster_globals.cpp
--- a/runtime/foster_globals.cpp
+++ b/runtime/foster_globals.cpp
@@ -40,11 +40,11 @@
       } else {
         if (i == argc - 1) continue; // no more to look at!
         if (streq("--foster-heap-KB", arg)) {
-          __foster_globals.semispace_size = ssize_t(parse_double(argv[i + 1], 1024.0) * 1024.0);
+          __foster_globals.semispace_size = int(parse_double(argv[i + 1], 1024.0) * 1024.0);
         } else if (streq("--foster-heap-MB", arg)) {
-          __foster_globals.semispace_size = ssize_t(parse_double(argv[i + 1], 1.0) * 1024.0 * 1024.0);
+          __foster_globals.semispace_size = int(parse_double(argv[i + 1], 1.0) * 1024.0 * 1024.0);
         } else if (streq("--foster-heap-GB", arg)) {
-          __foster_globals.semispace_size = ssize_t(parse_double(argv[i + 1], 0.001) * 1024.0 * 1024.0 * 1024.0);
+          __foster_globals.semispace_size = int(parse_double(argv[i + 1], 0.001) * 1024.0 * 1024.0 * 1024.0);
         } else if (streq("--foster-json-stats", arg)) {
           __foster_globals.dump_json_stats_path = argv[i + 1];
         }
diff --git a/runtime/gc/foster_gc.cpp b/runtime/gc/foster_gc.cpp
--- a/runtime/gc/foster_gc.cpp
+++ b/runtime/gc/foster_gc.cpp
@@ -17,8 +17,6 @@
 #include <functional>
 #include <stddef.h> // offsetof
 
-#include <sparsehash/dense_hash_set>
-
 // jemalloc_pages
 bool  pages_boot(void);
 void* pages_map(void* addr, size_t size, size_t alignment, bool* commit);
@@ -35,46 +33,42 @@
 // These are defined as compile-time constants so that the compiler
 // can do effective dead-code elimination. If we were JIT compiling
 // the GC we could (re-)specialize these config vars at runtime...
-#define GCLOG_DETAIL 0
+#define ENABLE_GCLOG 0
 #define ENABLE_LINE_GCLOG 0
 #define ENABLE_GCLOG_PREP 0
-#define ENABLE_GCLOG_ENDGC 1
-#define PRINT_STDOUT_ON_GC 0
+#define ENABLE_GCLOG_ENDGC 0
 #define FOSTER_GC_TRACK_BITMAPS       0
 #define FOSTER_GC_ALLOC_HISTOGRAMS    0
-#define FOSTER_GC_TIME_HISTOGRAMS     1 // Adds ~300 cycles per collection
+#define FOSTER_GC_TIME_HISTOGRAMS     0 // Adds ~300 cycles per collection
 #define FOSTER_GC_EFFIC_HISTOGRAMS    0
-#define ENABLE_GC_TIMING              1
-#define ENABLE_GC_TIMING_TICKS        1 // Adds ~430 cycles per collection
+#define ENABLE_GC_TIMING              0
+#define ENABLE_GC_TIMING_TICKS        0 // Adds ~430 cycles per collection
 #define GC_ASSERTIONS 0
 #define MARK_FRAME21S                 0
 #define MARK_FRAME21S_OOL             0
-#define COALESCE_FRAME15S             0
+#define COALESCE_FRAME15S             1
 #define MARK_OBJECT_WITH_BITS         0
 #define UNSAFE_ASSUME_F21_UNMARKED    false
-#define TRACK_NUM_ALLOCATIONS         1
-#define TRACK_NUM_ALLOC_BYTES         1
-#define TRACK_NUM_REMSET_ROOTS        1
-#define TRACK_NUM_OBJECTS_MARKED      1
-#define TRACK_WRITE_BARRIER_COUNTS    1
+#define TRACK_NUM_ALLOCATIONS         0
+#define TRACK_NUM_ALLOC_BYTES         0
+#define TRACK_NUM_REMSET_ROOTS        0
+#define TRACK_NUM_OBJECTS_MARKED      0
+#define TRACK_WRITE_BARRIER_COUNTS    0
 #define TRACK_BYTES_KEPT_ENTRIES      0
 #define TRACK_BYTES_ALLOCATED_ENTRIES 0
 #define TRACK_BYTES_ALLOCATED_PINHOOK 0
 #define GC_BEFORE_EVERY_MEMALLOC_CELL 0
-#define DEBUG_INITIALIZE_ALLOCATIONS  0 // Initialize even when the middle end doesn't think it's necessary
+#define DEBUG_INITIALIZE_ALLOCATIONS  0
+#define FORCE_INITIALIZE_ALLOCATIONS  0 // Initialize even when the middle end doesn't think it's necessary
 #define ELIDE_INITIALIZE_ALLOCATIONS  0 // Unsafe: ignore requests to initialize allocated memory.
 #define MEMSET_FREED_MEMORY           0
 // This included file may un/re-define these parameters, providing
 // a way of easily overriding-without-overwriting the defaults.
 #include "gc/foster_gc_reconfig-inl.h"
 
-const int kFosterGCMaxDepth = 250;
+const int kFosterGCMaxDepth = 1024;
 const ssize_t inline gSEMISPACE_SIZE() { return __foster_globals.semispace_size; }
 
-extern void* foster__typeMapList[];
-
-int64_t gNumRootsScanned = 0;
-
 /////////////////////////////////////////////////////////////////
 
 #include "foster_gc_utils.h"
@@ -88,36 +82,16 @@
 
 #define IMMIX_LINE_SIZE     256
 #define IMMIX_LINE_SIZE_LOG 8
-#define IMMIX_LINES_PER_FRAME15_LOG 7 /*15 - 8*/
-#define IMMIX_LINES_PER_FRAME15   128
-
-#define IMMIX_LINE_FRAME15_START_LINE 5
-#define IMMIX_LINES_PER_LINE_FRAME15 (IMMIX_LINES_PER_FRAME15 - IMMIX_LINE_FRAME15_START_LINE)
+#define IMMIX_CARDS_PER_FRAME15_LOG 7 /*15 - 8*/
+#define IMMIX_CARDS_PER_FRAME15   128
 
 #define COARSE_MARK_LOG 21
 
-#define IMMIX_F15_PER_F21 64
-#define IMMIX_BYTES_PER_LINE 256
-#define IMMIX_LINES_PER_BLOCK 128
-#define IMMIX_GRANULES_PER_LINE (IMMIX_BYTES_PER_LINE / 16)
-#define IMMIX_GRANULES_PER_BLOCK (128 * IMMIX_GRANULES_PER_LINE)
-#define IMMIX_ACTIVE_F15_PER_F21 (IMMIX_F15_PER_F21 - 1)
-
-static_assert(IMMIX_GRANULES_PER_LINE == 16,    "documenting expected values");
-static_assert(IMMIX_GRANULES_PER_BLOCK == 2048, "documenting expected values");
-
-
-int num_free_lines = 0; // TODO move to gcglobals
-
 extern "C" {
   void foster_pin_hook_memalloc_cell(uint64_t nbytes);
   void foster_pin_hook_memalloc_array(uint64_t nbytes);
 }
 
-//#define remset_t std::unordered_set<tori**>
-//#define remset_t std::set<tori**>
-#define remset_t google::dense_hash_set<tori**>
-
 namespace foster {
 namespace runtime {
 namespace gc {
@@ -134,33 +108,19 @@
     if (addr >= bound) return "after";
     return "within";
   }
-  ssize_t size() const { return distance(base, bound); }
-
-  void wipe_memory(uint8_t byte) { memset(base, byte, size()); }
+  size_t size() const { return distance(base, bound); }
 };
 
-// To be suitably aligned for allocation, a pointer should be positioned
-// such that the body -- after the header -- will have the proper default alignment.
-// Let P be a pointer, A be the alignment in bytes, and H the header size in bytes.
-// If P is aligned at default alignment, then P = kA, and P + (A - H) is aligned for allocation,
-// because (P + (A - H) + H) = P + A = kA + A = (k + 1) A.
-void* offset_for_allocation(void* bump) {
-  return offset(bump, FOSTER_GC_DEFAULT_ALIGNMENT - HEAP_CELL_HEADER_SIZE);
+void* realigned_to_line(void* bump) {
+ return offset(roundUpToNearestMultipleWeak(bump, IMMIX_LINE_SIZE)
+              ,HEAP_CELL_HEADER_SIZE);
 }
 
 void* realigned_for_allocation(void* bump) {
- return offset_for_allocation(roundUpToNearestMultipleWeak(bump, FOSTER_GC_DEFAULT_ALIGNMENT));
+ return offset(roundUpToNearestMultipleWeak(bump, FOSTER_GC_DEFAULT_ALIGNMENT)
+              ,HEAP_CELL_HEADER_SIZE);
 }
 
-void* realigned_for_heap_handle(void* bump) {
-  return roundUpToNearestMultipleWeak(bump, FOSTER_GC_DEFAULT_ALIGNMENT);
-}
-
-void* realigned_to_line_flat(void* bump) {
- return roundUpToNearestMultipleWeak(bump, IMMIX_LINE_SIZE);
-}
-
-
 class bump_allocator : public memory_range {
 public:
   bump_allocator() {
@@ -181,65 +141,12 @@
   }
 };
 
-
-// On a 64-bit machine, physical address space will only be 48 bits usually.
-// If we use 47 of those bits, we can drop the low-order 15 bits and be left
-// with 32 bits!
-typedef uint32_t frame15_id;
-typedef uint32_t frame21_id;
-
-frame15_id frame15_id_of(void* p) { return frame15_id(uintptr_t(p) >> 15); }
-frame21_id frame21_id_of(void* p) { return frame21_id(uintptr_t(p) >> 21); }
-
-uintptr_t low_n_bits(uintptr_t val, uintptr_t n) { return val & ((1 << n) - 1); }
-
-uintptr_t line_offset_within_f21(void* slot) {
-  return low_n_bits(uintptr_t(slot) >> 8, 21 - 8);
-}
-
-int line_offset_within_f15(void* slot) {
-  return int(low_n_bits(uintptr_t(slot) >> 8, 15 - 8));
-}
-
-struct immix_line_frame15;
-class frame15;
-class frame21;
-
-frame15* frame15_for_frame15_id(frame15_id f15) {
-  return (frame15*)(uintptr_t(f15) << 15);
-}
-
 // The pointer itself is the base pointer of the equivalent memory_range.
 struct free_linegroup {
   void*           bound;
   free_linegroup* next;
 };
 
-struct used_linegroup {
-  void*           base;
-  int             count;
-
-  int startline() const { return line_offset_within_f15(base); }
-  int endline()   const { return line_offset_within_f15(base) + count; }
-
-  size_t size_in_bytes() const { return count * IMMIX_BYTES_PER_LINE; }
-  size_t size_in_lines() const { return count; }
-
-  frame15_id associated_frame15_id() const { return frame15_id_of(base); }
-  immix_line_frame15* associated_lineframe() const {
-    return (immix_line_frame15*) frame15_for_frame15_id(associated_frame15_id());
-  }
-
-  used_linegroup singleton(int i) const {
-    return used_linegroup { .base  = offset(base,  i      * IMMIX_BYTES_PER_LINE),
-                            .count = 1 };
-  }
-
-  bool contains(void* slot) const { return slot > base && distance(base, slot) < size_in_bytes(); }
-
-  void clear_line_and_object_mark_bits();
-};
-
 struct byte_limit {
   ssize_t frame15s_left;
 
@@ -261,7 +168,6 @@
 typedef std::map<frameptr, const stackmap::PointCluster*> ClusterMap;
 // }}}
 
-
 class heap {
 public:
   virtual ~heap() {}
@@ -269,21 +175,14 @@
   virtual tidy* tidy_for(tori* t) = 0;
 
   virtual void dump_stats(FILE* json) = 0;
+  virtual byte_limit* get_byte_limit() = 0;
 
   virtual void force_gc_for_debugging_purposes() = 0;
 
   virtual void condemn() = 0;
-  virtual void uncondemn() = 0;
 
   virtual void visit_root(unchecked_ptr* root, const char* slotname) = 0;
 
-  virtual void immix_sweep(clocktimer<false>& phase,
-                           clocktimer<false>& gcstart) = 0;
-
-  virtual void trim_remset() = 0;
-
-  virtual bool is_empty() = 0;
-  virtual uint64_t approx_size_in_bytes() = 0;
 
   virtual void remember_outof(void** slot, void* val) = 0;
   virtual void remember_into(void** slot) = 0;
@@ -298,11 +197,10 @@
 
 #define immix_heap heap
 
-struct immix_common;
 struct immix_space;
-struct immix_worklist_t {
+struct immix_worklist {
     void       initialize()      { ptrs.clear(); idx = 0; }
-    void       process(immix_heap* target, immix_common& common);
+    void       process(immix_heap* target);
     bool       empty()           { return idx >= ptrs.size(); }
     void       advance()         { ++idx; }
     heap_cell* peek_front()      { return ptrs[idx]; }
@@ -316,6 +214,9 @@
 
 // {{{ Global data used by the GC
 
+class frame15;
+class frame21;
+
 enum class frame15kind : uint8_t {
   unknown = 0,
   immix_smallmedium, // associated is immix_space*
@@ -336,18 +237,6 @@
   mixed_condemned
 };
 
-/* We can collect the heap at three granularities:
- *   1) Collect the whole heap, ignoring subheap boundaries.
- *      This is used to find space triggered by heap exhaustion.
- *   2) Collect a single subheap.
- *   3) Collect whatever frames have been condemned.
- */
-enum class condemned_set_status : uint8_t {
-  whole_heap_condemned = 0,
-  single_subheap_condemned,
-  per_frame_condemned
-};
-
 struct frame15info {
   void*            associated;
   frame15kind      frame_classification;
@@ -355,14 +244,6 @@
   condemned_status frame_status;
 };
 
-// We track "available" rather than "marked" lines because it's more natural
-// to track incrementally for line frames, where different spaces own different groups,
-// and not all lines were necessarily part of the last condemned set. If a given line
-// wasn't condemned, it's arguable whether its state is "unmarked" in the same sense
-// that a condemned line might be, but it's straightforward to state that it's not available.
-
-void gc_assert(bool cond, const char* msg);
-
 // {{{
 #define arraysize(x) (sizeof(x)/sizeof((x)[0]))
 #define MAX_ARR_OBJ_PER_FRAME15 4
@@ -416,66 +297,17 @@
 }
 // }}}
 
-template <typename Allocator>
-struct condemned_set {
-  condemned_set_status status;
-
-  std::set<Allocator*> spaces;
-
-  // Some objects (in particular, subheap handles) are not allocated on regular frames
-  // and thus would otherwise not get their granule mark bits reset at the end of each collection.
-  // We track, above, the set of all created subheaps (in order to identify unmarked subheaps),
-  // but to avoid O(full-heap) work on a subheap collection,
-  // we only want to reset the marks we established during each collection.
-  std::set<heap_cell*> unframed_and_marked;
-
-  void uncondemn_all() {
-    // If we had a fine-grained condemned set, reset it.
-    if (status == condemned_set_status::per_frame_condemned) {
-      status = condemned_set_status::single_subheap_condemned;
-
-      for (auto space : spaces) {
-        space->uncondemn();
-      }
-    }
-
-    // Whole-heap collections ignore the condemned set,
-    // and single-subheap collections by definition have an otherwise-empty
-    // condemned set.
-  }
-
-  // Use line marks to reclaim space, then reset linemaps and object marks.
-  void sweep_condemned(Allocator* active_space,
-                       clocktimer<false>& phase, clocktimer<false>& gcstart,
-                       double deltaRecursiveMarking_us);
-};
-
 template<typename Allocator>
 struct GCGlobals {
   Allocator* allocator = NULL;
   Allocator* default_allocator = NULL;
 
-  // Invariant: null pointer when allocator == default_allocator,
-  // otherwise a heap_handle to the current allocator.
-  heap_handle<immix_heap>* allocator_handle;
-
-  condemned_set<Allocator> condemned_set;
-
-  byte_limit* space_limit;
-
-  std::map<void*, const StkMapRecord*> stackMapRecords;
-  memory_range typemap_memory;
+  ClusterMap clusterForAddress;
 
   bool had_problems;
-  bool logall;
 
   std::map<std::pair<const char*, typemap*>, int64_t> alloc_site_counters;
 
-  std::set<frame21*> all_frame21s;
-  std::vector<heap_handle<Allocator>*> all_subheap_handles_except_default_allocator;
-
-  std::set<heap_cell*> marked_in_current_gc;
-
   double gc_time_us;
 
   clocktimer<true> init_start;
@@ -487,15 +319,11 @@
 
   uint64_t num_allocations;
   uint64_t num_alloc_bytes;
-  uint64_t num_subheaps_created;
-  uint64_t num_subheap_activations;
 
   uint64_t write_barrier_phase0_hits;
   uint64_t write_barrier_phase1_hits;
-  int64_t  write_barrier_slow_path_ticks;
 
   uint64_t num_objects_marked_total;
-  uint64_t alloc_bytes_marked_total;
 
   frame15info*      lazy_mapped_frame15info;
   uint8_t*          lazy_mapped_coarse_marks;
@@ -504,7 +332,6 @@
   uint8_t*          lazy_mapped_granule_marks;
 
   struct hdr_histogram* hist_gc_stackscan_frames;
-  struct hdr_histogram* hist_gc_stackscan_roots;
   struct hdr_histogram* hist_gc_postgc_ticks;
   struct hdr_histogram* hist_gc_pause_micros;
   struct hdr_histogram* hist_gc_pause_ticks;
@@ -517,40 +344,46 @@
 GCGlobals<immix_heap> gcglobals;
 
 // The worklist would be per-GC-thread in a multithreaded implementation.
-immix_worklist_t immix_worklist;
-
-class condemned_set_status_manager {
-  condemned_set_status prev;
-
-public:
-  condemned_set_status_manager(condemned_set_status new_status) {
-    prev = gcglobals.condemned_set.status;
-    gcglobals.condemned_set.status = new_status;
-  }
-
-  ~condemned_set_status_manager() {
-    gcglobals.condemned_set.status = prev;
-  }
-};
-
+immix_worklist immix_worklist;
+
+#define IMMIX_F15_PER_F21 64
+#define IMMIX_BYTES_PER_LINE 256
+#define IMMIX_LINES_PER_BLOCK 128
+#define IMMIX_GRANULES_PER_LINE (IMMIX_BYTES_PER_LINE / 16)
+#define IMMIX_GRANULES_PER_BLOCK (128 * IMMIX_GRANULES_PER_LINE)
+#define IMMIX_ACTIVE_F15_PER_F21 (IMMIX_F15_PER_F21 - 1)
+
+static_assert(IMMIX_GRANULES_PER_LINE == 16,    "documenting expected values");
+static_assert(IMMIX_GRANULES_PER_BLOCK == 2048, "documenting expected values");
+
+// On a 64-bit machine, physical address space will only be 48 bits usually.
+// If we use 47 of those bits, we can drop the low-order 15 bits and be left
+// with 32 bits!
+typedef uint32_t frame15_id;
+typedef uint32_t frame21_id;
 
 template <typename T>
 inline T num_granules(T size_in_bytes) { return size_in_bytes / T(16); }
 
 uintptr_t global_granule_for(void* p) { return num_granules(uintptr_t(p)); }
 
+frame15_id frame15_id_of(void* p) { return frame15_id(uintptr_t(p) >> 15); }
+frame21_id frame21_id_of(void* p) { return frame21_id(uintptr_t(p) >> 21); }
+
 // Precondition: x >= 15
 uint32_t frameX_id_of(void* p, uintptr_t x) { return uint32_t(uintptr_t(p) >> x); }
 
 frame21* frame21_of_id(frame21_id x) { return (frame21*) (uintptr_t(x) << 21); }
 
-void clear_linemap(uint8_t* linemap) {
-  memset(linemap, 0, IMMIX_LINES_PER_BLOCK);
+uintptr_t low_n_bits(uintptr_t val, uintptr_t n) { return val & ((1 << n) - 1); }
+
+uintptr_t line_offset_within_f21(void* slot) {
+  return low_n_bits(uintptr_t(slot) >> 8, 21 - 8);
 }
 
-void clear_frame15(frame15* f15) { memset(f15, 0xDD, 1 << 15); }
-void clear_frame21(frame21* f21) { memset(f21, 0xDD, 1 << 21); }
-void do_clear_line_frame15(immix_line_frame15* f);
+uintptr_t line_offset_within_f15(void* slot) {
+  return low_n_bits(uintptr_t(slot) >> 8, 15 - 8);
+}
 
 inline
 frame15info* frame15_info_for_frame15_id(frame15_id fid) {
@@ -617,21 +450,17 @@
   gcglobals.lazy_mapped_frame15info[fid].associated = v;
 }
 
+
 inline bool obj_is_marked(heap_cell* obj) {
   if (MARK_OBJECT_WITH_BITS) {
     return bitmap::get_bit(global_granule_for(obj), gcglobals.lazy_mapped_granule_marks) == 1;
   } else {
-    uint8_t* markbyte = &gcglobals.lazy_mapped_granule_marks[global_granule_for(obj)];
-    return *markbyte == 1;
+    return gcglobals.lazy_mapped_granule_marks[global_granule_for(obj)] == 1;
   }
 }
 inline bool arr_is_marked(heap_array* obj) { return obj_is_marked((heap_cell*)obj); }
 
-frame15kind frame15_classification(void* addr);
-immix_heap* heap_for(void* val);
-
 inline void do_mark_obj(heap_cell* obj) {
-  if (GCLOG_DETAIL > 3) { fprintf(gclog, "setting granule bit  for object %p in frame %u\n", obj, frame15_id_of(obj)); }
   if (MARK_OBJECT_WITH_BITS) {
     bitmap::set_bit_to(global_granule_for(obj), 1, gcglobals.lazy_mapped_granule_marks);
   } else {
@@ -639,7 +468,7 @@
   }
 }
 
-inline void do_unmark_granule(void* obj) {
+inline void do_unmark_arr(heap_array* obj) {
   if (MARK_OBJECT_WITH_BITS) {
     bitmap::set_bit_to(global_granule_for(obj), 0, gcglobals.lazy_mapped_granule_marks);
   } else {
@@ -647,30 +476,10 @@
   }
 }
 
-void clear_object_mark_bits_for_frame15(void* f15) {
-  if (GCLOG_DETAIL > 2) { fprintf(gclog, "clearing granule bits for frame %p (%zu)\n", f15, frame15_id_of(f15)); }
-  if (MARK_OBJECT_WITH_BITS) {
-    memset(&gcglobals.lazy_mapped_granule_marks[global_granule_for(f15) / 8], 0, IMMIX_GRANULES_PER_BLOCK / 8);
-  } else {
-    memset(&gcglobals.lazy_mapped_granule_marks[global_granule_for(f15)], 0, IMMIX_GRANULES_PER_BLOCK);
-  }
-}
-
-void clear_object_mark_bits_for_frame15(void* f15, int startline, int numlines) {
-  uintptr_t granule = global_granule_for(offset(f15, startline * IMMIX_BYTES_PER_LINE));
-  if (GCLOG_DETAIL > 2) { fprintf(gclog, "clearing granule bits for %d lines starting at %d in frame %p (%zu), granule %zu\n", numlines, startline, f15, frame15_id_of(f15), granule); }
-  if (MARK_OBJECT_WITH_BITS) {
-    memset(&gcglobals.lazy_mapped_granule_marks[granule / 8], 0, (numlines * IMMIX_GRANULES_PER_LINE) / 8);
-  } else {
-    memset(&gcglobals.lazy_mapped_granule_marks[granule],     0, (numlines * IMMIX_GRANULES_PER_LINE));
-  }
-}
-
 
 bool line_is_marked(  int line, uint8_t* linemap) { return linemap[line] == 1; }
 bool line_is_unmarked(int line, uint8_t* linemap) { return linemap[line] == 0; }
 void do_mark_line(  int line, uint8_t* linemap) { linemap[line] = 1; }
-void do_unmark_line(int line, uint8_t* linemap) { linemap[line] = 0; }
 
 //static_assert(sizeof(frame15info) == 16, "expect frame15info to be two words");
 
@@ -700,26 +509,17 @@
     void* base = malloc(total_bytes + 8);
     heap_array* allot = align_as_array(base);
 
-    if (GC_ASSERTIONS) { gc_assert(frame15_id_of(allot) == frame15_id_of(allot->body_addr()), "large array: metadata and body address on different frames!"); }
-    if (DEBUG_INITIALIZE_ALLOCATIONS ||
+    if (FORCE_INITIALIZE_ALLOCATIONS ||
       (init && !ELIDE_INITIALIZE_ALLOCATIONS)) { memset((void*) base, 0x00, total_bytes + 8); }
     allot->set_header(arr_elt_map, mark_bits_current_value);
     allot->set_num_elts(num_elts);
     if (TRACK_BYTES_ALLOCATED_PINHOOK) { foster_pin_hook_memalloc_array(total_bytes); }
-    if (TRACK_NUM_ALLOCATIONS) { ++gcglobals.num_allocations; }
-    if (TRACK_NUM_ALLOC_BYTES) { gcglobals.num_alloc_bytes += total_bytes; }
 
     // TODO modify associated frame15infos, lazily allocate card bytes.
     toggle_framekinds_for(allot, offset(base, total_bytes + 7), parent);
     // TODO review when/where line mark bit setting happens,
     //      ensure it doesn't happen for pointers to arrays.
     allocated.push_front(base);
-
-    if (GCLOG_DETAIL > 0) {
-      fprintf(gclog, "mallocating large array (%p, body %p) in with mark bits %p, total bytes %zd, alloc #%zd\n",
-          allot, allot->body_addr(), (void*) mark_bits_current_value, total_bytes, gcglobals.num_allocations);
-    }
-
     return allot->body_addr();
   }
 
@@ -780,40 +580,37 @@
       void* base = *it;
       heap_array* arr = align_as_array(base);
       if (arr_is_marked(arr)) {
-        do_unmark_granule(arr);
+        do_unmark_arr(arr);
         ++it;
       } else { // unmarked, can free associated array.
-        if (GCLOG_DETAIL > 1) { fprintf(gclog, "freeing unmarked array %p\n", arr); }
+        if (ENABLE_GCLOG) { fprintf(gclog, "freeing unmarked array %p\n", arr); }
         it = allocated.erase(it); // erase() returns incremented iterator.
         framekind_malloc_cleanup(arr);
         free(base);
       }
     }
   }
-
-  bool empty() { return allocated.empty(); }
 };
 // }}}
 
 // {{{ Internal utility functions
 extern "C" foster_bare_coro** __foster_get_current_coro_slot();
 
+void gc_assert(bool cond, const char* msg);
+
 intr* from_tidy(tidy* t) { return (intr*) t; }
 
-void designate_as_lineframe(immix_line_frame15* f) {
-  auto finfo = frame15_info_for_frame15_id(frame15_id_of(f));
-  // For line frames, "available lines" means lines from this frame
-  // being stored in the global line allocator's avail_lines bucket.
-  finfo->num_available_lines_at_last_collection = 0;
-  finfo->associated = f;
-  finfo->frame_classification = frame15kind::immix_linebased;
+struct immix_line_frame15;
+void mark_lineframe(immix_line_frame15* f) {
+  auto fid = frame15_id_of(f);
+  gcglobals.lazy_mapped_frame15info[fid].associated = f;
+  gcglobals.lazy_mapped_frame15info[fid].frame_classification = frame15kind::immix_linebased;
 }
 
 void set_parent_for_frame(immix_space* p, frame15* f) {
-  auto finfo = frame15_info_for_frame15_id(frame15_id_of(f));
-  finfo->num_available_lines_at_last_collection = 0;
-  finfo->associated = p;
-  finfo->frame_classification = frame15kind::immix_smallmedium;
+  auto fid = frame15_id_of(f);
+  gcglobals.lazy_mapped_frame15info[fid].associated = p;
+  gcglobals.lazy_mapped_frame15info[fid].frame_classification = frame15kind::immix_smallmedium;
 }
 
 frame15kind frame15_classification(void* addr) {
@@ -843,8 +640,6 @@
   return heap_for((void*) body) == space;
 }
 
-tidy* assume_tori_is_tidy(tori* p) { return (tidy*) p; }
-
 condemned_status condemned_status_for(void* addr, frame15info* finfo);
 /*
 bool is_condemned_(void* slot, frame15info* finfo) {
@@ -852,13 +647,9 @@
 }
 */
 
-// ``finfo`` is only needed when the condemned portion is
-//   condemned_set_status::per_frame_condemned.
-template<condemned_set_status condemned_portion>
+template<bool use_space>
 bool is_condemned(void* slot, immix_heap* space, frame15info* finfo) {
-  if (condemned_portion == condemned_set_status::whole_heap_condemned) {
-    return true;
-  } else if (condemned_portion == condemned_set_status::single_subheap_condemned) {
+  if (use_space) {
     return owned_by((tori*)slot, space);
   } else {
     return condemned_status_for(slot, finfo) == condemned_status::yes_condemned;
@@ -868,14 +659,6 @@
 // }}}
 
 // {{{
-// {{{ Function prototype decls
-bool line_for_slot_is_marked(void* slot);
-void inspect_typemap(const typemap* ti);
-void visitGCRoots(void* start_frame, immix_heap* visitor);
-void coro_visitGCRoots(foster_bare_coro* coro, immix_heap* visitor);
-const typemap* tryGetTypemap(heap_cell* cell);
-// }}}
-
 
 namespace helpers {
 
@@ -898,9 +681,7 @@
                                   uintptr_t  mark_value,
                                   bool     init) {
     heap_array* allot = static_cast<heap_array*>(bumper->prechecked_alloc_noinit(total_bytes));
-
-    if (GC_ASSERTIONS) { gc_assert(frame15_id_of(allot) == frame15_id_of(allot->body_addr()), "pre array: metadata and body address on different frames!"); }
-    if (DEBUG_INITIALIZE_ALLOCATIONS ||
+    if (FORCE_INITIALIZE_ALLOCATIONS ||
       (init && !ELIDE_INITIALIZE_ALLOCATIONS)) { memset((void*) allot, 0x00, total_bytes); }
     //fprintf(gclog, "alloc'a %d, bump = %p, low bits: %x\n", int(total_bytes), bump, intptr_t(bump) & 0xF);
     allot->set_header(arr_elt_map, mark_value);
@@ -915,18 +696,6 @@
       //obj_start.set_bit(granule);
       //obj_limit.set_bit(granule + num_granules(total_bytes));
     }
-
-
-    if (GC_ASSERTIONS && line_for_slot_is_marked(allot)) {
-      fprintf(gclog, "INVARIANT VIOLATED: allocating array on a pre-marked line?!?\n");
-      exit(4);
-    }
-
-    if (GCLOG_DETAIL > 3) {
-      fprintf(gclog, "allocating array (%p, body %p) in line %d of frame %u, total bytes %zd, alloc #%zd\n",
-          allot, allot->body_addr(), line_offset_within_f15(allot), frame15_id_of(allot), total_bytes, gcglobals.num_allocations);
-    }
-
     return allot->body_addr();
   }
 
@@ -943,8 +712,6 @@
                                  int64_t  cell_size,
                                  uintptr_t  mark_value) {
     heap_cell* allot = static_cast<heap_cell*>(bumper->prechecked_alloc(cell_size));
-
-    if (GC_ASSERTIONS) { gc_assert(frame15_id_of(allot) == frame15_id_of(allot->body_addr()), "cell prechecked: metadata and body address on different frames!"); }
     //if (TRACK_BYTES_ALLOCATED_ENTRIES) { parent->record_bytes_allocated(map->cell_size); }
     if (TRACK_BYTES_ALLOCATED_PINHOOK) { foster_pin_hook_memalloc_cell(cell_size); }
     if (TRACK_NUM_ALLOCATIONS) { ++gcglobals.num_allocations; }
@@ -956,63 +723,9 @@
       //size_t granule = granule_for(tori_of_tidy(allot->body_addr()));
       //obj_start.set_bit(granule);
     }
-
-    if (GC_ASSERTIONS && line_for_slot_is_marked(allot)) {
-      fprintf(gclog, "INVARIANT VIOLATED: allocating cell (%p) on a pre-marked line?!?\n", allot);
-      exit(4);
-    }
-    if (GC_ASSERTIONS && obj_is_marked(allot)) {
-      fprintf(gclog, "INVARIANT VIOLATED: allocating cell (%p)      pre-marked     ?!?\n", allot);
-      exit(4);
-    }
-
     return allot->body_addr();
   }
 
-  bool remset_entry_is_externally_stale(tori** slot) {
-    return !line_for_slot_is_marked(slot);
-  }
-
-  bool remset_entry_is_internally_stale(tori** slot) {
-    tori* ptr = *slot;
-    if (non_kosher_addr(ptr)) { return true; }
-    heap_cell* cell = heap_cell::for_tidy((tidy*) ptr);
-    /*
-    fprintf(gclog, "considering remset      ptr %p: body line marked %d, cell line marked %d, cell granule marked %d\n",
-      ptr, line_for_slot_is_marked(ptr ),
-           line_for_slot_is_marked(cell), obj_is_marked(cell)); fflush(gclog);
-           */
-    if (!obj_is_marked(cell)) { return true; }
-    // TODO check to make sure that the space ownership hasn't changed?
-    return false;
-  }
-
-  // Precondition: line marks have been established and not yet cleared.
-  bool remset_entry_is_externally_or_internally_stale(tori** slot) {
-    if (remset_entry_is_externally_stale(slot)) { return true; }
-    return remset_entry_is_internally_stale(slot);
-  }
-
-  void do_trim_remset(remset_t& incoming_ptr_addrs, immix_heap* space) {
-    std::vector<tori**> slots(incoming_ptr_addrs.begin(), incoming_ptr_addrs.end());
-    incoming_ptr_addrs.clear();
-
-    //fprintf(gclog, "gc %d: pre-trim remset contains %zu slots in space %p\n", 
-    //  gcglobals.num_gcs_triggered, slots.size(), space);
-    for (tori** slot : slots) {
-      if (remset_entry_is_externally_or_internally_stale(slot)) {
-        // do nothing
-        /*
-        fprintf(gclog, "gc %d: dropping stale remset entry holding %p in space %p for slot %p\n",
-            gcglobals.num_gcs_triggered, *slot, space, slot);
-            */
-      } else {
-        incoming_ptr_addrs.insert(slot);
-      }
-    }
-    //fprintf(gclog, "gc %d: post-trim remset contains %d slots\n", gcglobals.num_gcs_triggered, incoming_ptr_addrs.size());
-  }
-
 } // namespace helpers
 // }}}
 
@@ -1020,6 +733,13 @@
 
 ////////////////////////////////////////////////////////////////////
 
+// {{{ Function prototype decls
+void inspect_typemap(const typemap* ti);
+void visitGCRoots(void* start_frame, immix_heap* visitor);
+void coro_visitGCRoots(foster_bare_coro* coro, immix_heap* visitor);
+const typemap* tryGetTypemap(heap_cell* cell);
+// }}}
+
 // TODO use stat_tracker again?
 
 frame21* allocate_frame21() {
@@ -1027,12 +747,10 @@
   frame21* rv = (frame21*) pages_map(nullptr, 1 << 21, 1 << 21, &commit);
   if (ENABLE_GCLOG_PREP) { fprintf(gclog, "allocate_frame21() returning %p\n", rv); }
   gc_assert(commit && rv != NULL, "unable to allocate a 2MB chunk from the OS");
-  gcglobals.all_frame21s.insert(rv);
   return rv;
 }
 
 void deallocate_frame21(frame21* f) {
-  gcglobals.all_frame21s.erase(f);
   pages_unmap(f, 1 << 21);
 }
 
@@ -1062,13 +780,11 @@
   }
 
   void give_frame15(frame15* f) {
-    if (MEMSET_FREED_MEMORY) { clear_frame15(f); }
+    if (ENABLE_GCLOG_PREP) { fprintf(gclog, "give_frame15(%p)\n", f); }
     spare_frame15s.push_back(f);
   }
 
   void give_frame21(frame21* f) {
-    if (MEMSET_FREED_MEMORY) { clear_frame21(f); }
-    fprintf(gclog, "marking frame21 %p, from space %p, as spare\n", f, heap_for(f));
     spare_frame21s.push_back(f);
   }
 
@@ -1109,22 +825,7 @@
     return curr_frame15;
   }
 
-  // Invariant: f must be completely clean.
-  void give_line_frame15(immix_line_frame15* f) {
-    if (MEMSET_FREED_MEMORY) {
-      fprintf(gclog, "clearing line frame15 %p (%u)\n", f, frame15_id_of(f));
-      do_clear_line_frame15(f); }
-
-    if (false && GC_ASSERTIONS) {
-      std::set<immix_line_frame15*> spares(spare_line_frame15s.begin(), spare_line_frame15s.end());
-      if (spares.count(f) > 0) {
-        fprintf(gclog, "GC INVARIANT VIOLATED: spare line frame15s contains %p already (%d duplicates)\n",
-          f, spare_line_frame15s.size() - spares.size());
-      }
-    }
-
-    spare_line_frame15s.push_back(f);
-  }
+  void give_line_frame15(immix_line_frame15* f) { spare_line_frame15s.push_back(f); }
 
   immix_line_frame15* get_line_frame15() {
     if (!spare_line_frame15s.empty()) {
@@ -1215,6 +916,14 @@
 frame15_allocator global_frame15_allocator;
 
 
+// Since these pointers are guaranteed to fit within a single line,
+// we can embed the information in the start of each free span.
+class free_line_span {
+  void* bump;
+  void* limit;
+  free_line_span* next;
+};
+
 // 64 * 32 KB = 2 MB  ~~~ 2^6 * 2^15 = 2^21
 struct frame21_15_metadata_block {
   union {
@@ -1244,13 +953,8 @@
   return uint8_t(low_n_bits(global_fid, 21 - 15));
 }
 
-bool is_in_metadata_frame(void* obj) {
-  bool would_be_metadata_if_smallmedium = frame15_id_within_f21(frame15_id_of(obj)) == 0;
-  auto frameclass = frame15_classification(obj);
-  if (frameclass == frame15kind::immix_smallmedium
-   || frameclass == frame15kind::immix_linebased
-   || frameclass == frame15kind::unknown) { return would_be_metadata_if_smallmedium; }
-  return false;
+frame15* frame15_for_frame15_id(frame15_id f15) {
+  return (frame15*)(uintptr_t(f15) << 15);
 }
 
 frame21_15_metadata_block* metadata_block_for_slot(void* slot) {
@@ -1266,11 +970,24 @@
 }
 
 
+uint8_t* cards_for_frame15_id(frame15_id fid) {
+  auto mdb = metadata_block_for_frame15_id(fid);
+  return &mdb->cardmap[frame15_id_within_f21(fid)][0];
+}
+
 uint8_t* linemap_for_frame15_id(frame15_id fid) {
   auto mdb = metadata_block_for_frame15_id(fid);
   return &mdb->linemap[frame15_id_within_f21(fid)][0];
 }
 
+void clear_linemap(uint8_t* linemap) {
+  memset(linemap, 0, IMMIX_LINES_PER_BLOCK);
+}
+
+void clear_frame15(frame15* f15) {
+  memset(f15, 0xDD, 1 << 15);
+}
+
 
 uint8_t* get_frame_map(frame21_15_metadata_block* mdb) {
   return &mdb->linemap[0][0];
@@ -1329,31 +1046,6 @@
   } else return !UNSAFE_ASSUME_F21_UNMARKED;
 }
 
-void used_linegroup::clear_line_and_object_mark_bits() {
-  uint8_t* linemap = linemap_for_frame15_id(associated_frame15_id());
-  auto lineframe = associated_lineframe();
-
-  gc_assert(startline() != endline(), "used linegroup had same start and end line...?");
-
-  //fprintf(gclog, "used_linegroup:: clear_line_and_object_mark_bits %p (%u), linemap: %p, lineframe: %p, startline %d, endline %d, lineframe is meta? %d, f15-off-in-f21: %d\n",
-  //    lineframe, associated_frame15_id(),
-  //    linemap, lineframe, startline(), endline(),
-  //    is_in_metadata_frame(lineframe),
-  //    frame15_id_within_f21(associated_frame15_id())
-  //    );
-  //fflush(gclog);
-
-  // Note: must clear only our bits, since those of other groups may not yet have been inspected.
-  for (int i = startline(); i < endline(); ++i) {
-    //fprintf(gclog, "clearing linemap entry %d for (%u), linemap addr: %p\n", i, associated_frame15_id(), &linemap[i]);  fflush(gclog);
-    do_unmark_line(i, linemap);
-  }
-  gc_assert(startline() >= 0, "invalid startline when clearing bits");
-  gc_assert(endline() <= IMMIX_LINES_PER_BLOCK, "invalid endline when clearing bits");
-  gc_assert(startline() < endline(), "invalid: startline after endline when clearing bits");
-  clear_object_mark_bits_for_frame15(lineframe, startline(), (endline() - startline()) + 1);
-}
-
 // {{{ metadata helpers
 
 static inline int64_t array_size_for(int64_t n, int64_t slot_size) {
@@ -1366,17 +1058,15 @@
                               const typemap* & map,
                               int64_t        & cell_size) {
   cell_size = cell->cell_size();
-  if (GCLOG_DETAIL > 3 || cell_size <= 0) { fprintf(gclog, "obj %p in frame (%u) has size %zd (0x%zx)\n", cell,
-    frame15_id_of(cell), cell_size, cell_size); fflush(gclog); }
   gc_assert(cell_size > 0, "cannot copy cell lacking metadata or length");
 
   if ((map = tryGetTypemap(cell))) {
-    if (GCLOG_DETAIL > 4) { inspect_typemap(map); }
+    if (ENABLE_GCLOG) { inspect_typemap(map); }
     if (map->isArray) {
       arr = heap_array::from_heap_cell(cell);
     }
   }
-  
+
   // {{{
   if (!map) {
     // already have cell size
@@ -1384,7 +1074,7 @@
     cell_size = map->cell_size; // probably an actual pointer
   } else {
     cell_size = array_size_for(arr->num_elts(), map->cell_size);
-    if (GCLOG_DETAIL > 1) {
+    if (ENABLE_GCLOG) {
       fprintf(gclog, "Collecting array of total size %" PRId64
                     " (rounded up from %" PRId64 " + %" PRId64 " = %" PRId64
                     "), cell size %" PRId64 ", len %" PRId64 "...\n",
@@ -1409,7 +1099,6 @@
   do_mark_line(line_offset_within_f21(slot), linemap);
 }
 
-// Precondition: slot is located in a markable frame.
 bool line_for_slot_is_marked(void* slot) {
   auto mdb = metadata_block_for_frame15_id(frame15_id_of(slot));
   uint8_t* linemap = &mdb->linemap[0][0];
@@ -1430,8 +1119,6 @@
   if (MARK_FRAME21S) { mark_frame21_for_slot(slot); }
   if (MARK_FRAME21S_OOL) { mark_frame21_ool_for_slot(slot); }
 
-  if (GCLOG_DETAIL > 3) { fprintf(gclog, "marking lines %d - %d for slot %p of size %zd\n", firstoff, lastoff, slot, cell_size); }
-
   linemap[firstoff] = 1;
   // Exact marking for small objects
   linemap[lastoff] = 1;
@@ -1447,8 +1134,6 @@
   }
 }
 
-// This struct contains per-frame state and code shared between
-// regular and line-based immix frames.
 struct immix_common {
 
   uintptr_t prevent_constprop;
@@ -1459,17 +1144,17 @@
   // register pressure, resulting in a net extra instruction in the critical path of allocation.
   uintptr_t prevent_const_prop() { return prevent_constprop; }
 
-  template <condemned_set_status condemned_portion>
+  template <bool use_space>
   void scan_with_map_and_arr(immix_heap* space,
                              heap_cell* cell, const typemap& map,
                              heap_array* arr, int depth) {
     //fprintf(gclog, "copying %lld cell %p, map np %d, name %s\n", cell_size, cell, map.numEntries, map.name); fflush(gclog);
     if (!arr) {
-      scan_with_map<condemned_portion>(space, from_tidy(cell->body_addr()), map, depth);
+      scan_with_map<use_space>(space, from_tidy(cell->body_addr()), map, depth);
     } else if (map.numOffsets > 0) { // Skip this loop for int arrays and such.
       int64_t numcells = arr->num_elts();
       for (int64_t cellnum = 0; cellnum < numcells; ++cellnum) {
-        scan_with_map<condemned_portion>(space, arr->elt_body(cellnum, map.cell_size), map, depth);
+        scan_with_map<use_space>(space, arr->elt_body(cellnum, map.cell_size), map, depth);
       }
     }
 
@@ -1479,43 +1164,19 @@
     }
   }
 
-  template <condemned_set_status condemned_portion>
+  template <bool use_space>
   void scan_with_map(immix_heap* space, intr* body, const typemap& map, int depth) {
     for (int i = 0; i < map.numOffsets; ++i) {
-      void** unchecked = (void**) offset(body, map.offsets[i]);
-      if (GCLOG_DETAIL > 4) {
-        fprintf(gclog, "scan_with_map scanning pointer %p from slot %p (field %d of %d in at offset %d in object %p)\n",
-            *unchecked, unchecked, i, map.numOffsets, map.offsets[i], body);
-      }
-      immix_trace<condemned_portion>(space, (unchecked_ptr*) unchecked,
-                                     depth);
+      immix_trace<use_space>(space, (unchecked_ptr*) offset(body, map.offsets[i]),
+                           depth);
     }
   }
 
-  bool is_immix_markable_frame(void* p) {
-    auto k = classification_for_frame15_id(frame15_id_of(p));
-    return (k == frame15kind::immix_smallmedium || k == frame15kind::immix_linebased);
-  }
-
-  // Precondition: cell is part of the condemned set.
-  template <condemned_set_status condemned_portion>
-  void scan_cell(immix_heap* space, heap_cell* cell, int depth_remaining) {
-    if (GCLOG_DETAIL > 3) {
-      fprintf(gclog, "scanning cell %p for space %p with remaining depth %d\n", cell, space, depth_remaining);
-      fflush(gclog); }
+  template <bool use_space>
+  void scan_cell(immix_heap* space, heap_cell* cell, int depth) {
     if (obj_is_marked(cell)) {
-      if (GC_ASSERTIONS) {
-        if (gcglobals.marked_in_current_gc.count(cell) == 0) {
-          fprintf(gclog, "GC INVARIANT VIOLATED: cell %p, of frame %u, appears marked before actually being marked!\n", cell, frame15_id_of(cell));
-          fprintf(gclog, "     default allocator is %p\n", gcglobals.default_allocator);
-          fflush(gclog);
-          inspect_typemap(cell->get_meta());
-          abort();
-        }
-      }
-      if (GCLOG_DETAIL > 3) { fprintf(gclog, "cell %p was already marked\n", cell); }
-
-      if (GC_ASSERTIONS && is_immix_markable_frame(cell) && !line_for_slot_is_marked(cell)) {
+      //fprintf(gclog, "cell %p was already marked\n", cell);
+      if (GC_ASSERTIONS && !line_for_slot_is_marked(cell)) {
         fprintf(gclog, "GC INVARIANT VIOLATED: cell %p marked but corresponding line not marked!\n", cell);
         fflush(gclog);
         abort();
@@ -1523,40 +1184,33 @@
       return;
     }
 
-    if (depth_remaining == 0) {
+    if (depth == 0) {
       immix_worklist.add(cell);
       return;
     }
 
-    auto frameclass = frame15_classification(cell);
-    if (GCLOG_DETAIL > 3) { fprintf(gclog, "frame classification for obj %p in frame %u is %d\n", cell, frame15_id_of(cell), int(frameclass)); }
-
     heap_array* arr = NULL;
     const typemap* map = NULL;
     int64_t cell_size;
     get_cell_metadata(cell, arr, map, cell_size);
 
-    if (GC_ASSERTIONS) { gcglobals.marked_in_current_gc.insert(cell); }
     do_mark_obj(cell);
     if (TRACK_NUM_OBJECTS_MARKED) { gcglobals.num_objects_marked_total++; }
-    if (TRACK_NUM_OBJECTS_MARKED) { gcglobals.alloc_bytes_marked_total += cell_size; }
+
+    auto frameclass = frame15_classification(cell);
 
     if (frameclass == frame15kind::immix_smallmedium
      || frameclass == frame15kind::immix_linebased) {
-        void* slot = (void*) cell;
+      void* slot = (void*) cell;
         mark_lines_for_slots(slot, cell_size);
-    } else if (frameclass == frame15kind::unknown || frameclass == frame15kind::staticdata) {
-      gcglobals.condemned_set.unframed_and_marked.insert(cell);
     }
 
     // Without metadata for the cell, there's not much we can do...
-    if (map && gcglobals.typemap_memory.contains((void*) map)) {
-      scan_with_map_and_arr<condemned_portion>(space, cell, *map, arr, depth_remaining - 1);
-    }
+    if (map) scan_with_map_and_arr<use_space>(space, cell, *map, arr, depth - 1);
   }
 
   // Jones/Hosking/Moss refer to this function as "process(fld)".
-  template <condemned_set_status condemned_portion>
+  template <bool use_space>
   void immix_trace(immix_heap* space, unchecked_ptr* root, int depth) {
     //       |------------|       obj: |------------|
     // root: |    body    |---\        |  size/meta |
@@ -1565,7 +1219,7 @@
     //                        |       ...          ...
     //                        \-*root->|            |
     //                                 |------------|
-    //tidy* tidyn;
+    tidy* tidyn;
     tori* body = untag(*root);
     if (!body) return;
 
@@ -1584,24 +1238,22 @@
     if (classification_for_frame15_id(f15id) == frame15kind::staticdata) {
       // Do nothing: no need to mark, since static data never points to
       // dynamically allocated data.
-      if (GCLOG_DETAIL > 3) { fprintf(gclog, "ignoring static data cell %p\n", body); }
       return;
     }
 
-    if (!(is_condemned<condemned_portion>(body, space, f15info))) {
+    // TODO is_full_heap || ....
+    //if (!is_condemned_(body, f15info)) {
+    //if (!(use_space ? is_condemned(body, space, f15info) : is_condemned_(body, f15info))) {
+    if (!(is_condemned<use_space>(body, space, f15info))) {
       // When collecting a subset of the heap, we only look at condemned objects,
       // and ignore objects stored in non-condemned regions (regardless of
       // whether they are part of this particular subheap or not).
       // The remembered set is guaranteed to contain all incoming pointers
       // from non-condemned regions.
-      if (GCLOG_DETAIL > 3) { fprintf(gclog, "ignoring non-condemned cell %p\n", body); }
       return;
     }
 
-    if (GCLOG_DETAIL > 4) {
-      fprintf(gclog, "immix_trace: space %p saw pointer %p, untagged from %p; f15id %u, condemned portion: %d\n",
-        space, body, root->val, f15id, condemned_portion);
-    }
+    //fprintf(gclog, "space %p saw pointer %p to owner space %p\n", this, body, owner);
 
     // TODO drop the assumption that body is a tidy pointer.
     heap_cell* obj = heap_cell::for_tidy(reinterpret_cast<tidy*>(body));
@@ -1613,94 +1265,74 @@
       //gc_assert(NULL != untag(*root), "copying gc should not null out slots");
       //gc_assert(body != untag(*root), "copying gc should return new pointers");
     } else {
-      scan_cell<condemned_portion>(space, obj, depth);
+      scan_cell<use_space>(space, obj, depth);
     }
   }
 
   void visit_root(immix_heap* space, unchecked_ptr* root, const char* slotname) {
-    switch (gcglobals.condemned_set.status) {
-    case                            condemned_set_status::single_subheap_condemned:
-      return visit_root_specialized<condemned_set_status::single_subheap_condemned>(space, root, slotname);
-    case                            condemned_set_status::per_frame_condemned:
-      return visit_root_specialized<condemned_set_status::per_frame_condemned>(space, root, slotname);
-    case                            condemned_set_status::whole_heap_condemned:
-      return visit_root_specialized<condemned_set_status::whole_heap_condemned>(space, root, slotname); 
-    }
-  }
-
-  template <condemned_set_status condemned_portion>
-  void visit_root_specialized(immix_heap* space, unchecked_ptr* root, const char* slotname) {
     gc_assert(root != NULL, "someone passed a NULL root addr!");
-    if (GCLOG_DETAIL > 1) {
-      fprintf(gclog, "\t\tSTACK SLOT %p (in (%u)) contains ptr %p, slot name = %s\n", root, frame15_id_of(root),
+    if (ENABLE_GCLOG) {
+      fprintf(gclog, "\t\tSTACK SLOT %p contains ptr %p, slot name = %s\n", root,
                         unchecked_ptr_val(*root),
                         (slotname ? slotname : "<unknown slot>"));
     }
-
-    ++gNumRootsScanned;
     
     // TODO-X determine when to use condemned set and when not to
-    immix_trace<condemned_portion>(space, root, kFosterGCMaxDepth);
-  }
-
-  uint64_t process_remsets(immix_heap* space, remset_t& incoming_ptr_addrs) {
-    // To boost tracing efficiency, pre-compile different variants of the tracing code
-    // (using templates) specialized to what portion of the heap is being traced.
-    switch (gcglobals.condemned_set.status) {
-    case                    condemned_set_status::single_subheap_condemned:
-      return process_remset<condemned_set_status::single_subheap_condemned>(space, incoming_ptr_addrs);
-    case                    condemned_set_status::per_frame_condemned:
-      return process_remset<condemned_set_status::per_frame_condemned>(space, incoming_ptr_addrs);
-    case                    condemned_set_status::whole_heap_condemned:
-      return process_remset<condemned_set_status::whole_heap_condemned>(space, incoming_ptr_addrs);
-    }
+    immix_trace<true>(space, root, kFosterGCMaxDepth);
   }
 
-  template <condemned_set_status condemned_portion>
-  uint64_t process_remset(immix_heap* space, remset_t& incoming_ptr_addrs) {
+  template <bool use_space>
+  uint64_t process_remset(immix_heap* space, std::set<tori**>& incoming_ptr_addrs) {
     uint64_t numRemSetRoots = 0;
-
-    //fprintf(gclog, "space %p has %d potentially-incoming slots\n", space, incoming_ptr_addrs.size());
-
-    std::vector<tori**> slots(incoming_ptr_addrs.begin(), incoming_ptr_addrs.end());
-    for (tori** loc: slots) {
+    for (tori** loc: incoming_ptr_addrs) {
       // We can ignore the remembered set root if the source is also getting collected.
-      if (is_condemned<condemned_portion>(loc, space,
-            (condemned_portion == condemned_set_status::per_frame_condemned)
-                ? frame15_info_for(loc) : nullptr )) {
-        if (GCLOG_DETAIL > 3) {
-          fprintf(gclog, "space %p skipping ptr %p, from remset, in co-condemned slot %p\n", space, *loc, loc);
-        }
+      if (is_condemned<use_space>(loc, space, use_space ? nullptr : frame15_info_for(loc) )) {
         continue;
       }
 
       tori* ptr = *loc;
       // Otherwise, we must check whether the source slot was modified;
       // if so, it might not point into our space.
-      if (is_condemned<condemned_portion>((void*)ptr, space,
-            (condemned_portion == condemned_set_status::per_frame_condemned)
-                ? frame15_info_for((void*)ptr) : nullptr )) {
-        const typemap* purported_typemap = heap_cell::for_tidy(assume_tori_is_tidy(untag(make_unchecked_ptr(ptr))))->get_meta();
-        if (gcglobals.typemap_memory.contains((void*) purported_typemap)) {
-          if (TRACK_NUM_REMSET_ROOTS) { numRemSetRoots++; }
-          //fprintf(gclog, "space %p examining remset ptr %p in slot %p with typemap %p\n", space, *loc, loc, purported_typemap); fflush(gclog);
-          visit_root_specialized<condemned_portion>(space, (unchecked_ptr*) loc, "remembered_set_root");
-        } else {
-          fprintf(gclog, "space %p skipping remset bad-typemap ptr %p in slot %p\n", space, *loc, loc);
-        }
-      } else {
-        if (GCLOG_DETAIL > 3) {
-          fprintf(gclog, "space %p skipping remset non-condemned ptr %p in slot %p\n", space, *loc, loc);
-        }
+      if (is_condemned<use_space>((void*)ptr, space, use_space ? nullptr : frame15_info_for((void*)ptr) )) {
+        if (TRACK_NUM_REMSET_ROOTS) { numRemSetRoots++; }
+        visit_root(space, (unchecked_ptr*) ptr, "remembered_set_root");
       }
     }
     return numRemSetRoots;
   }
-
-  void common_gc(immix_heap* active_space, remset_t& incoming_ptr_addrs, bool voluntary);
+    /*
+    uint64_t numRemSetLines = 0;
+    // Trace from remembered set roots
+    for (auto& fid : frames_pointing_here) {
+      auto frame_cards = cards_for_frame15_id(fid);
+      for (int i = 0; i < IMMIX_CARDS_PER_FRAME15; ++i) {
+        if (frame_cards[i] != 0) {
+          ++numRemSetLines;
+          // Scan card for pointers that point into this space.
+          unchecked_ptr** finger = (unchecked_ptr**) frame15_for_frame15_id(fid);
+          unchecked_ptr** limit  = (unchecked_ptr**) frame15_for_frame15_id(fid + 1);
+          for ( ; finger != limit; ++finger) {
+            unchecked_ptr* ptr = *finger;
+            if (owned_by((tori*)ptr, this)) {
+              // TODO pin values since they're being treated conservatively?
+              if (TRACK_NUM_REMSET_ROOTS) { numRemSetRoots++; }
+              common.visit_root(this, ptr, "remembered_set_root");
+            }
+          }
+        }
+      }
+    }
+    */
 };
 
+
+class immix_line_frame15;
+
 class immix_frame_tracking {
+  // We store the frame15 count separately so that we don't need to
+  // consult the map entries in fromglobal_frame15s.
+  size_t num_frame15s_total; // including both indvidual and coalesced.
+
   // Stores values returned from global_frame15_allocator.get_frame15();
   // Note we store a vector rather than a set because we maintain
   // the invariant that a given frame15 is only added once between clear()s.
@@ -1727,6 +1359,7 @@
 
   void release_clean_frames(byte_limit* lim) {
     lim->frame15s_left += frame15s_in_reserve_clean();
+    num_frame15s_total -= frame15s_in_reserve_clean();
 
     for (auto f15 : clean_frame15s) {
       global_frame15_allocator.give_frame15(f15);
@@ -1771,25 +1404,34 @@
 
   template<typename WasUncleanThunk>
   void iter_coalesced_frame21(WasUncleanThunk thunk) {
-    // Interestingly, we don't (directly) preserve any coalesced frame21s!
+    // Interestingly, we don't preserve any coalesced frame21s!
     // Unclean frame21s get split, and clean frame21s are returned to the global pool.
     // Coalescing is a net increase in work, but it's also a net reduction in
     // the critical path for large, mostly-empty subheaps, which is an overall win.
-    std::vector<frame21*> holder;
-    holder.swap(coalesced_frame21s);
-    // Avoid problems from the callback thunk indirectly modifying coalesced_frame21s,
-    // e.g. if the entire frame is dirty, it will be re-coalesced.
-    // Note that, at this point in execution, coalesced_frame21s is empty.
-    for (auto f21 : holder) {
+    for (auto f21 : coalesced_frame21s) {
       thunk(f21);
     }
+    coalesced_frame21s.clear();
+  }
+
+  void clear_tracking() {
+    num_frame15s_total = 0;
+#if COALESCE_FRAME15S
+    fromglobal_frame15s.clear();
+#else
+    uncoalesced_frame15s.clear();
+#endif
+    coalesced_frame21s.clear();
   }
 
   void add_frame21(frame21* f) {
+    num_frame15s_total += IMMIX_ACTIVE_F15_PER_F21;
     coalesced_frame21s.push_back(f);
   }
 
   void add_frame15(frame15* f) {
+    ++num_frame15s_total;
+
 #if COALESCE_FRAME15S
     auto x = frame21_id_of(f);
     std::vector<frame15*>& v = fromglobal_frame15s[x];
@@ -1804,11 +1446,8 @@
 #endif
   }
 
-  size_t logical_frame15s() {
-    return physical_frame15s() + (IMMIX_ACTIVE_F15_PER_F21 * coalesced_frame21s.size());
-  }
-
-  // Note: when COALESCE_FRAME15S is enabled, this method is O(n).
+  size_t logical_frame15s() { return num_frame15s_total; }
+
   size_t physical_frame15s() {
     size_t rv = 0;
 #if COALESCE_FRAME15S
@@ -1919,22 +1558,20 @@
 */
 
 
+#define IMMIX_LINE_FRAME15_START_LINE 5
+
 struct immix_line_frame15 {
-  // We set aside 5 (IMMIX_LINE_FRAME15_START_LINE)
-  // of the 128 lines in the frame, which is 3.9% overhead (1 KB + 256b out of 32 KB).
-
-  // One line (256 b) for condemned marks, though we only need/use IMMIX_LINES_PER_BLOCK.
+  // We set aside 5 of the 128 lines in the frame, which is 3.9% overhead
+  // (1 KB + 128b out of 32 KB).
   condemned_status condemned[IMMIX_BYTES_PER_LINE]; // half a line for per-line condemned bytemap
 
-  // Four lines (1024 bytes = (123+5) * 8 bytes) for owners and other metadata.
   // We can store per-line space pointers for the remaining 123 lines:
   immix_line_space* owners[123]; // 8 b * (123 + 5) = 1 KB = 4 lines
   // And have five words left over for bookkeeping:
   union {
     struct {
-      bump_allocator    line_bumper;
+      bump_allocator bumper;
       immix_line_space* last_user;
-      void*             bumper_start;
     };
     struct {
       uint64_t pad1;
@@ -1947,6 +1584,9 @@
 
   char begin_lines[0];
 
+
+  void mark_owner(immix_line_space* owner, int64_t nbytes);
+
   // The offset mediates between the logical and physical view of line numbering.
   // If we stored metadata at the end of the frame we could avoid it.
   immix_line_space* get_owner_for_line(int n) { return owners[n - IMMIX_LINE_FRAME15_START_LINE]; }
@@ -1954,18 +1594,7 @@
 
   condemned_status get_condemned_status_for_line(int line) { return condemned[line]; }
   void set_condemned_status_for_line(int line, condemned_status c) { condemned[line] = c; }
-  void reset_line_bumper() {
-    line_bumper.base = &begin_lines[0];
-    line_bumper.bound = offset(line_bumper.base, IMMIX_LINES_PER_LINE_FRAME15 * IMMIX_LINE_SIZE);
-  }
-  void clear_line_frame15() {
-    if (MEMSET_FREED_MEMORY) {
-      reset_line_bumper();
-      line_bumper.wipe_memory(0xDA);
-    }
-  }
-
-  void realign_and_split_line_bumper_if(bool do_split);
+
   //condemned_status  get_condemned_status_for_line(int n) { return condemned[n]; }
 };
 
@@ -1975,117 +1604,36 @@
             == (IMMIX_LINE_FRAME15_START_LINE * IMMIX_BYTES_PER_LINE),
             "our expectation for the positioning of begin_lines is broken!");
 
-void do_clear_line_frame15(immix_line_frame15* f) { f->clear_line_frame15(); }
-
-/*
-void* nth_line_of_line_frame15(immix_line_frame15* f) {
+void* first_line_of_line_frame15(immix_line_frame15* f) {
   return offset(f, IMMIX_LINE_FRAME15_START_LINE * IMMIX_BYTES_PER_LINE);
 }
-*/
-
-// Opportunistically merges when line groups are inserted in sorted order.
-void append_linegroup(std::vector<used_linegroup>& lines, used_linegroup u) {
-  if (lines.empty()) { lines.push_back(u); return; }
-
-  used_linegroup& last = lines.back();
-  if (frame15_id_of(last.base) == frame15_id_of(u.base)
-             && last.endline() == u.startline()) {
-    //fprintf(gclog, "append_linegroup() extending count %d by %d for base %p\n", last.count, u.count, last.base);
-    //fprintf(gclog, "   last: base %p, count %d, lines %d to %d\n", last.base, last.count, last.startline(), last.endline());
-    //fprintf(gclog, "      u: base %p, count %d, lines %d to %d\n", u.base, u.count, u.startline(), u.endline());
-    last.count += u.count;
-  } else {
-    lines.push_back(u);
-  }
-}
 
 
 class immix_line_allocator {
   immix_line_frame15* current_frame;
 
-  // We could use a single, implicitly linked free_linegroup structure here,
-  // which would save some memory, but it's clearer and simpler for now to use a vector.
-  std::vector<used_linegroup> avail_lines;
-
-  std::set<frame15_id> freeable_frames;
-
 public:
   immix_line_allocator() : current_frame(nullptr) {}
 
-  void ensure_sufficient_lines(immix_line_space* owner, int64_t cell_size, bool force_new_line = false);
+  void ensure_current_frame(immix_line_space* owner, int64_t cell_size, bool force_new_line = false);
 
   // For use as the last step in condemn().
   void ensure_no_line_reuse(immix_line_space* owner) {
     if (!current_frame) return;
-    ensure_sufficient_lines(owner, 0, true);
-  }
-
-  void* line_allocate_array(immix_line_space* owner, typemap* elt_typeinfo, int64_t n, int64_t req_bytes, uintptr_t mark_value, bool init) {
-    ensure_sufficient_lines(owner, req_bytes);
-    return helpers::allocate_array_prechecked(&current_frame->line_bumper, elt_typeinfo, n, req_bytes, mark_value, init);
+    ensure_current_frame(owner, 0, true);
   }
 
-  void* line_allocate_cell(immix_line_space* owner, int64_t cell_size, uintptr_t mark_value, typemap* typeinfo) {
-    ensure_sufficient_lines(owner, cell_size);
-    return helpers::allocate_cell_prechecked(&(current_frame->line_bumper), typeinfo, cell_size, mark_value);
+  void* allocate_array(immix_line_space* owner, typemap* elt_typeinfo, int64_t n, int64_t req_bytes, uintptr_t mark_value, bool init) {
+    ensure_current_frame(owner, req_bytes);
+    return helpers::allocate_array_prechecked(&current_frame->bumper, elt_typeinfo, n, req_bytes, mark_value, init);
   }
 
-  template <uint64_t cell_size>
-  void* line_allocate_cell_N(immix_line_space* owner, uintptr_t mark_value, typemap* typeinfo) {
-    ensure_sufficient_lines(owner, cell_size);
-    return helpers::allocate_cell_prechecked(&(current_frame->line_bumper), typeinfo, cell_size, mark_value);
+  void* allocate_cell(immix_line_space* owner, int64_t cell_size, uintptr_t mark_value, typemap* typeinfo) {
+    ensure_current_frame(owner, cell_size);
+    return helpers::allocate_cell_prechecked(&(current_frame->bumper), typeinfo, cell_size, mark_value);
   }
 
   bool owns(immix_line_frame15* f) { return f == current_frame; }
-
-  // Called by immix_line_frame15::immix_sweep().
-  void reclaim_linegroup(used_linegroup g) {
-    append_linegroup(avail_lines, g);
-
-    auto finfo = frame15_info_for(g.base);
-    finfo->num_available_lines_at_last_collection += g.size_in_lines();
-
-    gc_assert(finfo->num_available_lines_at_last_collection <= IMMIX_LINES_PER_LINE_FRAME15, "available-line metadata broken");
-
-    if (finfo->num_available_lines_at_last_collection == IMMIX_LINES_PER_LINE_FRAME15) {
-      freeable_frames.insert(frame15_id_of(g.base));
-    }
-  }
-
-  int64_t count_available_bytes() {
-    int64_t bytes = 0;
-    for (auto linegroup : avail_lines) { bytes += linegroup.size_in_bytes(); }
-    return bytes;
-  }
-
-  void reclaim_frames(byte_limit* lim) {
-    if (freeable_frames.empty()) { return; }
-
-    std::vector<used_linegroup> avail(avail_lines);
-    avail_lines.clear();
-
-    for (auto g : avail) {
-      frame15_id fid = frame15_id_of(g.base);
-      if (freeable_frames.count(fid) == 1) {
-        // Line no longer available because the whole frame will be reclaimed.
-      } else {
-        avail_lines.push_back(g);
-      }
-    }
-
-    for (frame15_id fid : freeable_frames) {
-      ++lim->frame15s_left;
-      global_frame15_allocator.give_line_frame15((immix_line_frame15*) frame15_for_frame15_id(fid));
-      gcglobals.lazy_mapped_frame15info[fid].num_available_lines_at_last_collection = 0;
-    }
-    freeable_frames.clear();
-  }
-
-  void realign_and_split_line_bumper() {
-    //fprintf(gclog, "GC %d: realign_and_split_line_bumper; current_frame: %p\n", gcglobals.num_gcs_triggered, current_frame);
-    //if (current_frame) { fprintf(gclog, "     current_frame: start %p, base %p\n", current_frame->bumper_start, current_frame->line_bumper.base); }
-    if (current_frame) { current_frame->realign_and_split_line_bumper_if(true); }
-  }
 };
 
 immix_line_allocator global_immix_line_allocator;
@@ -2111,71 +1659,57 @@
 class immix_line_space : public heap {
 public:
   immix_common common;
-  
 
 private:
+  // How many are we allowed to allocate before being forced to GC & reuse?
+  byte_limit* lim;
+
   large_array_allocator laa;
 
-  std::vector<used_linegroup> used_lines;
+  std::vector<immix_line_frame15*> used_frames;
+  immix_line_frame15* prev_used_frame;
 
   // The points-into remembered set
-  remset_t incoming_ptr_addrs;
+  std::set<tori**> incoming_ptr_addrs;
 
 public:
-  immix_line_space() {
-    if (GCLOG_DETAIL > 2) {
-      fprintf(gclog, "new immix_line_space %p, current byte limit: %zd f15s\n", this,
-          gcglobals.space_limit->frame15s_left);
-    }
-
-    incoming_ptr_addrs.set_empty_key(nullptr);
+  immix_line_space(byte_limit* lim) : lim(lim) {
+    fprintf(gclog, "new immix_line_space %p, byte limit: %p, current value: %zd f15s\n", this, lim, lim->frame15s_left);
   }
 
   virtual void dump_stats(FILE* json) {
     return;
   }
 
-  void establish_ownership_for_allocation(immix_line_frame15* lineframe, int64_t nbytes);
-
-  void note_used_linegroup(void* bumper_start, void* bound) {
-    used_lines.push_back(used_linegroup { .base = bumper_start, .count =
-          int(distance(bumper_start, realigned_to_line_flat(bound)) >> IMMIX_LINE_SIZE_LOG) });
-    if (GCLOG_DETAIL > 2) {
-      fprintf(gclog, "noting used linegroup for space %p, bumper_start is %p (line %d of frame %u), bound %p (size %d); #noted lines = %zd\n",
-          this, bumper_start, line_offset_within_f15(bumper_start),
-          frame15_id_of(bumper_start),
-          bound, used_lines.back().count,
-          used_lines.size());
-    }
-          //(int) roundUpToNearestMultipleWeak(distance(bumper_start, bound), IMMIX_BYTES_PER_LINE) });
+  void used_frame(immix_line_frame15* f) {
+    // We want to keep a set of used frames.
+    // Calls to this function will often have locality,
+    // which we capture with prev_used_frame.
+    // We ought to use a hash table instead of a vector
+    // so that we don't grow when reusing/recycling multiple frames.
+    if (f != prev_used_frame) { used_frames.push_back(f); prev_used_frame = f; }
   }
 
   immix_line_frame15* get_new_frame(bool secondtry = false) {
-    if (gcglobals.space_limit->frame15s_left == 0) {
-      {
-        condemned_set_status_manager tmp(condemned_set_status::whole_heap_condemned);
-        if (GCLOG_DETAIL > 2) { fprintf(gclog, "get_new_(line)frame triggering immix gc\n"); }
-        common.common_gc(this, incoming_ptr_addrs, false);
-      }
-
-      if (GCLOG_DETAIL > 2) {
-        fprintf(gclog, "forced line-frame gc reclaimed %zd frames\n", gcglobals.space_limit->frame15s_left);
-      }
-
+    if (lim->frame15s_left == 0) {
+      gcglobals.num_gcs_triggered_involuntarily++;
+
+      this->immix_line_gc();
+
+      fprintf(gclog, "forced smallgc reclaimed %zd frames\n", lim->frame15s_left);
       if (secondtry) {
         helpers::oops_we_died_from_heap_starvation();
       } else return get_new_frame(true);
     }
 
-    --gcglobals.space_limit->frame15s_left;
+    // The frame returned may be fragmented, which we don't yet account for.
+    --lim->frame15s_left;
     auto lineframe = global_frame15_allocator.get_line_frame15();
-    gc_assert(! is_in_metadata_frame(lineframe), "shouldn't line allocate into a metadata frame!");
-    designate_as_lineframe(lineframe);
-    //fprintf(gclog, "get_new_frame() updating last_user of %p from %p to %p\n", lineframe, lineframe->last_user, this);
+    mark_lineframe(lineframe);
     lineframe->last_user = this;
-    lineframe->reset_line_bumper();
-    lineframe->bumper_start = lineframe->line_bumper.base;
-    lineframe->line_bumper.base = realigned_for_allocation(lineframe->bumper_start);
+    lineframe->bumper.base = realigned_for_allocation(first_line_of_line_frame15(lineframe));
+    lineframe->bumper.bound = offset(first_line_of_line_frame15(lineframe),
+                                          ((1 << 15) - (1 << 10)));
     return lineframe;
   }
 
@@ -2194,7 +1728,7 @@
     if (req_bytes > (1 << 13)) {
       return laa.allocate_array(elt_typeinfo, n, req_bytes, init, common.prevent_const_prop(), this);
     } else {
-      return global_immix_line_allocator.line_allocate_array(this, elt_typeinfo, n, req_bytes, common.prevent_const_prop(), init);
+      return global_immix_line_allocator.allocate_array(this, elt_typeinfo, n, req_bytes, common.prevent_const_prop(), init);
     }
   }
 
@@ -2202,38 +1736,35 @@
   // Invariant: cell size is less than one line.
   virtual void* allocate_cell(typemap* typeinfo) {
     int64_t cell_size = typeinfo->cell_size; // includes space for cell header.
-    return global_immix_line_allocator.line_allocate_cell(this, cell_size, common.prevent_const_prop(), typeinfo);
+    return global_immix_line_allocator.allocate_cell(this, cell_size, common.prevent_const_prop(), typeinfo);
   }
 
   // Invariant: N is less than one line.
   template <int N>
   void* allocate_cell_N(typemap* typeinfo) {
-    return global_immix_line_allocator.line_allocate_cell_N<N>(this, common.prevent_const_prop(), typeinfo);
+    return global_immix_line_allocator.allocate_cell(this, N, common.prevent_const_prop(), typeinfo);
   }
 
   virtual void* allocate_cell_16(typemap* typeinfo) { return allocate_cell_N<16>(typeinfo); }
   virtual void* allocate_cell_32(typemap* typeinfo) { return allocate_cell_N<32>(typeinfo); }
   virtual void* allocate_cell_48(typemap* typeinfo) { return allocate_cell_N<48>(typeinfo); }
 
-  virtual void force_gc_for_debugging_purposes() {
-    if (GCLOG_DETAIL > 2) { fprintf(gclog, "force_gc_for_debugging_purposes triggering line gc\n"); }
-    common.common_gc(this, incoming_ptr_addrs, true);
-  }
-
-  // Marks lines we own as condemned; ignores lines owned by other spaces.
+
+  virtual byte_limit* get_byte_limit() { return lim; }
+  virtual void force_gc_for_debugging_purposes() { this->immix_line_gc(); }
+
   virtual void condemn() {
-    for (auto usedgroup : used_lines) {
-      int num_condemned_lines = usedgroup.size_in_lines();
-      auto lineframe = usedgroup.associated_lineframe();
-
-      int startline = usedgroup.startline();
-      int endline   = usedgroup.endline();
-      for (int i = startline; i < endline; ++i) {
-        lineframe->set_condemned_status_for_line(i, condemned_status::yes_condemned);
+    for (auto lineframe : used_frames) {
+      int num_condemned_lines = 0;
+      for (int i = IMMIX_LINE_FRAME15_START_LINE; i < IMMIX_LINES_PER_BLOCK; ++i) {
+        if (lineframe->get_owner_for_line(i) == this) {
+          lineframe->set_condemned_status_for_line(i, condemned_status::yes_condemned);
+          ++num_condemned_lines;
+        }
       }
 
       set_condemned_status_for_frame15_id(frame15_id_of(lineframe),
-          (num_condemned_lines == IMMIX_LINES_PER_LINE_FRAME15)
+          (num_condemned_lines == (IMMIX_LINES_PER_BLOCK - IMMIX_LINE_FRAME15_START_LINE))
             ? condemned_status::yes_condemned
             : condemned_status::mixed_condemned);
     }
@@ -2241,19 +1772,19 @@
     global_immix_line_allocator.ensure_no_line_reuse(this);
   }
 
-  virtual void uncondemn() {
-    for (auto usedgroup : used_lines) {
-      int num_uncondemned_lines = usedgroup.size_in_lines();
-      auto lineframe = usedgroup.associated_lineframe();
-
-      int startline = usedgroup.startline();
-      int endline   = usedgroup.endline();
-      for (int i = startline; i < endline; ++i) {
-        lineframe->set_condemned_status_for_line(i, condemned_status::not_condemned);
+  void uncondemn() {
+    for (auto lineframe : used_frames) {
+      int num_uncondemned_lines = 0;
+      for (int i = IMMIX_LINE_FRAME15_START_LINE; i < IMMIX_LINES_PER_BLOCK; ++i) {
+        auto owner = lineframe->get_owner_for_line(i);
+        if (owner == this || !owner) {
+          lineframe->set_condemned_status_for_line(i, condemned_status::not_condemned);
+          ++num_uncondemned_lines;
+        }
       }
 
       set_condemned_status_for_frame15_id(frame15_id_of(lineframe),
-          (num_uncondemned_lines == IMMIX_LINES_PER_LINE_FRAME15)
+          (num_uncondemned_lines == (IMMIX_LINES_PER_BLOCK - IMMIX_LINE_FRAME15_START_LINE))
             ? condemned_status::not_condemned
             : condemned_status::mixed_condemned);
     }
@@ -2263,67 +1794,135 @@
     common.visit_root(this, root, slotname);
   }
 
-  // Clear the line map for our used lines.
+  // TODO-X integrate with regular inspection
+  // Clear the line map for our frames -- but only for the lines we own!
   void clear_mark_bits_for_space() {
-    for (auto usedgroup : used_lines) {
-      auto lineframe = usedgroup.associated_lineframe();
+    for (auto lineframe : used_frames) {
       uint8_t* linemap = linemap_for_frame15_id(frame15_id_of(lineframe));
-
-      int startline = usedgroup.startline();
-      int endline   = usedgroup.endline();
-      for (int i = startline; i < endline; ++i) {
-        linemap[i] = 0;
+      for (int i = IMMIX_LINE_FRAME15_START_LINE; i < IMMIX_LINES_PER_BLOCK; ++i) {
+        if (lineframe->get_owner_for_line(i) == this) {
+          linemap[i] = 0;
+        }
       }
     }
+    // TODO-X clear granule bits too
   }
 
-  virtual bool is_empty() { return used_lines.empty() && laa.empty(); }
-
-  virtual uint64_t approx_size_in_bytes() {
-    uint64_t rv = 0;
-    for (auto usedgroup : used_lines) { rv += usedgroup.size_in_bytes(); }
-    return rv;
-  }
-
-  virtual void trim_remset() { helpers::do_trim_remset(incoming_ptr_addrs, this); }
-
-  // TODO should mark-clearing and sweeping be handled via condemned sets?
-  //
-  virtual void immix_sweep(clocktimer<false>& phase,
-                           clocktimer<false>& gcstart) { // immix_line_sweep / sweep_line_space
+  void immix_line_gc() {
+    auto num_marked_at_start = gcglobals.num_objects_marked_total;
+
+#if ENABLE_GC_TIMING
+    clocktimer<false> ct; ct.start();
+#endif
+
+    //common.flip_current_mark_bits_value();
+
+    // TODO check condemned set instead of assuming true
+    uint64_t numRemSetRoots = common.process_remset<true>(this, incoming_ptr_addrs);
+
+    visitGCRoots(__builtin_frame_address(0), this);
+
+    foster_bare_coro** coro_slot = __foster_get_current_coro_slot();
+    foster_bare_coro*  coro = *coro_slot;
+    if (coro) {
+      if (ENABLE_GCLOG) {
+        fprintf(gclog, "==========visiting current coro: %p\n", coro); fflush(gclog);
+      }
+      common.visit_root(this, (unchecked_ptr*)coro_slot, NULL);
+      if (ENABLE_GCLOG) {
+        fprintf(gclog, "==========visited current coro: %p\n", coro); fflush(gclog);
+      }
+    }
+
+    immix_worklist.process(this);
+
+    // The regular immix space would call clear_current_blocks() here
+    // because it marks frames as recycled.
+    {
+    
     laa.sweep_arrays();
 
-    // Split this space's lines into marked and unmarked buckets.
-    std::vector<used_linegroup> used(used_lines);
-    used_lines.clear();
-
-    for (auto usedgroup : used) {
-      int startline = usedgroup.startline();
-      int endline   = usedgroup.endline();
-
-      uint8_t* linemap = linemap_for_frame15_id(usedgroup.associated_frame15_id());
-
-      //fprintf(gclog, "immix_sweep processing group from lines %d to %d for frame (%u)\n",
-      //  startline, endline, frame15_id_of(usedgroup.base));
-
-      // Rather than constructing combined groups of marked & unmarked lines,
-      // we append one line at a time, in order, and let the helper do the merging.
-      for (int i = startline; i < endline; ++i) {
-        //fprintf(gclog, "looking at linemap entry %d for (%u)\n", i, usedgroup.associated_frame15_id());
+    // Get a copy of the used frames
+    auto all_used_frames = get_all_used_frames();
+    used_frames.clear();
+    prev_used_frame = nullptr;
+
+    for (auto lineframe : all_used_frames) {
+      this->inspect_line_frame15_postgc(lineframe);
+    }
+
+#if ENABLE_GC_TIMING
+    double delta_us = ct.elapsed_us();
+#endif
+#if ENABLE_GC_TIMING && ENABLE_GCLOG_ENDGC
+    fprintf(gclog, "used frames: %zu -> %zu, took %f us; frames left: %zd\n",
+        all_used_frames.size(), used_frames.size(),
+        delta_us,
+        lim->frame15s_left
+        );
+#endif
+#   if TRACK_NUM_OBJECTS_MARKED
+        fprintf(gclog, "\t%zu objects marked in this GC cycle, %zu marked overall\n",
+                gcglobals.num_objects_marked_total - num_marked_at_start,
+                gcglobals.num_objects_marked_total);
+#   endif
+#if (ENABLE_GCLOG || ENABLE_GCLOG_ENDGC)
+#   if TRACK_NUM_REMSET_ROOTS
+        fprintf(gclog, "\t%ld objects identified in remset\n", numRemSetRoots);
+#   endif
+      fprintf(gclog, "\t/endgc-small\n\n");
+      fflush(gclog);
+#endif
+
+    // TODO gcglobals.subheap_ticks
+#if ENABLE_GC_TIMING
+    gcglobals.gc_time_us += delta_us;
+#endif
+
+    }
+    gcglobals.num_gcs_triggered += 1;
+  }
+
+  // Note this returns a copy!
+  std::vector<immix_line_frame15*> get_all_used_frames() {
+    return used_frames;
+  }
+
+  void inspect_line_frame15_postgc(immix_line_frame15* lineframe) {
+
+    //frame15* f15 = frame15_for_frame15_id(fid);
+    //if (heap_for(f15) != this) { return; }
+
+    uint8_t* linemap = linemap_for_frame15_id(frame15_id_of(lineframe));
+
+    uint8_t num_marked_lines = 0;
+    for (int i = IMMIX_LINE_FRAME15_START_LINE; i < IMMIX_LINES_PER_BLOCK; ++i) {
+      if (lineframe->get_owner_for_line(i) == this) {
         if (linemap[i]) {
-          //fprintf(gclog, "immix_sweep for %p: line %d marked\n", usedgroup.base, i);
-          append_linegroup(used_lines, usedgroup.singleton(i - startline));
+          ++num_marked_lines;
         } else {
-          //fprintf(gclog, "immix_sweep for %p: line %d unmarked\n", usedgroup.base, i);
-          global_immix_line_allocator.reclaim_linegroup(usedgroup.singleton(i - startline));
+          lineframe->set_owner_for_line(i, nullptr);
         }
       }
-
-      usedgroup.clear_line_and_object_mark_bits();
     }
+
+    // We currently only recycle blocks when every line we own(ed) is left unmarked.
+    // Another possibility would be to explicitly manage lines instead of blocks,
+    // but that gets into segregated freelist designs, splitting/merging, etc.
+
+    if (num_marked_lines > 0) {
+      if (ENABLE_GCLOG || ENABLE_LINE_GCLOG) { fprintf(gclog, "immix_line_space reusing frame %p\n", lineframe); }
+      used_frame(lineframe);
+    } else if (!global_immix_line_allocator.owns(lineframe)) {
+      if (ENABLE_GCLOG || ENABLE_LINE_GCLOG) { fprintf(gclog, "immix_line_space returning frame %p\n", lineframe); }
+      lim->frame15s_left++;
+      global_frame15_allocator.give_line_frame15(lineframe);
+    } else {
+      if (ENABLE_GCLOG || ENABLE_LINE_GCLOG) { fprintf(gclog, "immix_line_space ignoring active-allocation frame %p\n", lineframe); }
+    }
+    // TODO update frame15_info? does it make sense for shared frame15s?
   }
 
-
   virtual void remember_outof(void** slot, void* val) {
     auto mdb = metadata_block_for_slot((void*) slot);
     uint8_t* cards = (uint8_t*) mdb->cardmap;
@@ -2345,336 +1944,54 @@
   return f->get_condemned_status_for_line(line);
 }
 
-// Precondition: the line frame's bumper is initialized and large enough
-// for the pending n-byte allocation.
-void immix_line_space::establish_ownership_for_allocation(immix_line_frame15* lineframe, int64_t nbytes) {
-  int startline = line_offset_within_f15(       lineframe->line_bumper.base);
-  int endline   = line_offset_within_f15(offset(lineframe->line_bumper.base, nbytes));
-
+
+void immix_line_frame15::mark_owner(immix_line_space* owner, int64_t nbytes) {
+  int startline = line_offset_within_f15(bumper.base);
+  int endline = line_offset_within_f15(offset(bumper.base, nbytes));
   if (endline == startline) {
     // mark just one, don't bother looping
-    lineframe->set_owner_for_line(startline, this);
+    set_owner_for_line(startline, owner);
   } else {
     if (endline == 0) { // wrapped around
-      endline = IMMIX_LINES_PER_LINE_FRAME15;
+      endline = IMMIX_LINES_PER_BLOCK - IMMIX_LINE_FRAME15_START_LINE;
     }
-    for (int i = startline; i <= endline; ++i) {
-      lineframe->set_owner_for_line(i, this);
+    for (int i = startline; i < endline; ++i) {
+      set_owner_for_line(i, owner);
     }
   }
+
+  owner->used_frame(this);
 }
 
-// When changing the actively allocating/owning line space, or triggering a collection,
-// we must "split" the allocation bumper and have the current space make note of the allocated-into portion.
-void immix_line_frame15::realign_and_split_line_bumper_if(bool do_split) {
-  if (distance(bumper_start, line_bumper.base) <= (FOSTER_GC_DEFAULT_ALIGNMENT - HEAP_CELL_HEADER_SIZE)) {
-    if (GCLOG_DETAIL > 1) { fprintf(gclog, "skippping realignment because line bumper was empty\n"); }
-    return; // No need to split or realign when the bumper hasn't been used yet.
-  }
-
-  void* old_base = line_bumper.base;
-  line_bumper.base = realigned_to_line_flat(line_bumper.base);
-  void* mid_base = line_bumper.base;
-  void* old_bumper_start = bumper_start;
-  if (do_split && last_user) {
-    last_user->note_used_linegroup(bumper_start, line_bumper.base);
-    bumper_start = line_bumper.base;
-  }
-  line_bumper.base = realigned_for_allocation(line_bumper.base);
-  if (GCLOG_DETAIL > 2) {
-    fprintf(gclog, "realign & split line bumper: old bumper_start was %p, old base was %p, mid base %p, final base %p\n",
-        old_bumper_start,
-        old_base,
-        mid_base,
-        line_bumper.base);
-  }
-}
-
-// Compared to the "regular" immix allocator, we have two sources of overhead here:
-// the last-owner tracking, which is needed to ensure each line has only allocations
-// coming from a single owner; and owner marking.
-void immix_line_allocator::ensure_sufficient_lines(immix_line_space* owner, int64_t cell_size, bool force_new_line) {
+void immix_line_allocator::ensure_current_frame(immix_line_space* owner, int64_t cell_size, bool force_new_line) {
   if (!current_frame) {
     current_frame = owner->get_new_frame();
-    if ((GCLOG_DETAIL > 0) || ENABLE_LINE_GCLOG) { fprintf(gclog, "immix_line_allocator acquired first frame %p\n", current_frame); }
+    if (ENABLE_GCLOG || ENABLE_LINE_GCLOG) { fprintf(gclog, "immix_line_allocator acquired first frame %p\n", current_frame); }
   }
 
   // Are we continuing to allocate to our own lines,
   // or taking ownership from another space?
-  bool ownership_changing = current_frame->last_user != owner
-                         && current_frame->last_user != nullptr;
-  if (force_new_line || ownership_changing) {
-    current_frame->realign_and_split_line_bumper_if(ownership_changing);
-    //fprintf(gclog, "ensure_sufficient_lines() updating last_user of %p from %p to %p\n", current_frame, current_frame->last_user, owner);
-    current_frame->last_user = owner; // Note this comes after the potential bumper split above.
-    // If realigning fails, the line bumper will be empty, so we'll grab a new frame.
+  if (current_frame->last_user != owner || force_new_line) {
+    if (current_frame->bumper.size() < IMMIX_LINE_SIZE) {
+      // If we're on the last line, we cannot realign to the next line.
+      current_frame->bumper.base = current_frame->bumper.bound;
+    } else {
+      // If we have room, we can move to a new frame
+      // and check below whether we have room to allocate.
+      current_frame->bumper.base = realigned_for_allocation(
+          realigned_to_line(current_frame->bumper.base));
+      current_frame->last_user = owner;
+    }
   }
 
   // Make sure we have enough space even after realignment.
-  while (current_frame->line_bumper.size() < cell_size) {
-    if (distance(current_frame->bumper_start, current_frame->line_bumper.bound) > 0) {
-      //fprintf(gclog, "noting too-small linegroup so we don't forget it; avail linegroups count: %zd\n", avail_lines.size());
-      current_frame->last_user->note_used_linegroup(current_frame->bumper_start,
-                                                    current_frame->line_bumper.bound);
-      current_frame = nullptr;
-      // The reasoning behind nullifying current_frame is a little bit subtle.
-      // First, note that in both branches below, current_frame is assigned before being further used.
-      // Second, consider what happens if avail_lines is empty, and trying to get a new frame triggers GC.
-      //   * We noted the too-small-to-use linegroup above.
-      //   * common_gc() calls realign_and_split_line_bumper() because, for example, it's necessary when the
-      //     active space is not a line space.
-      //   * That function re-notes the same group!
-      //      * Or, rather, it would except that nullifying current_frame turns splitting into a no-op.
-    }
-
-    // To avoid repeatedly scanning lots of small groups for a continuous sequence of medium-sized
-    // allocations, we only try the most recent group and then fall back to getting a whole new frame.
-    // No stats colleted yet but this seems OK on paper because the vast majority of allocations are small.
-    if ((!avail_lines.empty()) && avail_lines.back().size_in_bytes() >= cell_size) {
-      used_linegroup g = avail_lines.back(); avail_lines.pop_back();
-      current_frame = g.associated_lineframe();
-      frame15_info_for(current_frame)->num_available_lines_at_last_collection -= g.size_in_lines();
-      current_frame->bumper_start      = g.base;
-      current_frame->line_bumper.base  = realigned_for_allocation(g.base);
-      current_frame->line_bumper.bound = offset(g.base, g.count * IMMIX_BYTES_PER_LINE);
-      current_frame->last_user = owner;
-    } else {
-      current_frame = owner->get_new_frame();
-    }
-
-    /*
-    if (ENABLE_GCLOG_PREP || GCLOG_DETAIL > 2) { fprintf(gclog, "immix_line_allocator acquired new frame %p with bumper size %zu at line %d, alloc# %zd\n",
-        current_frame, current_frame->line_bumper.size(),
-        line_offset_within_f15(current_frame->line_bumper.base),
-        gcglobals.num_allocations); }
-          */
+  if (current_frame->bumper.size() < cell_size) {
+    current_frame = owner->get_new_frame();
+    if (ENABLE_GCLOG || ENABLE_LINE_GCLOG) { fprintf(gclog, "immix_line_allocator acquired new frame %p with bumper size %zu\n",
+        current_frame, current_frame->bumper.size()); }
   }
 
-  owner->establish_ownership_for_allocation(current_frame, cell_size);
-}
-
-void immix_common::common_gc(immix_heap* active_space,
-                             remset_t& incoming_ptr_addrs,
-                             bool voluntary) {
-    gcglobals.num_gcs_triggered += 1;
-    if (!voluntary) { gcglobals.num_gcs_triggered_involuntarily++; }
-    if (PRINT_STDOUT_ON_GC) { fprintf(stdout, "                        start GC #%d\n", gcglobals.num_gcs_triggered); fflush(stdout); }
-    //{ fprintf(gclog, "                        start GC #%d; space %p; voluntary? %d\n", gcglobals.num_gcs_triggered, active_space, voluntary); }
-
-    clocktimer<false> gcstart; gcstart.start();
-    clocktimer<false> phase;
-#if ENABLE_GC_TIMING_TICKS
-    int64_t t0 = __foster_getticks_start();
-#endif
-
-    auto num_marked_at_start   = gcglobals.num_objects_marked_total;
-    auto bytes_marked_at_start = gcglobals.alloc_bytes_marked_total;
-
-    global_immix_line_allocator.realign_and_split_line_bumper();
-
-    phase.start();
-#if FOSTER_GC_TIME_HISTOGRAMS && ENABLE_GC_TIMING_TICKS
-    int64_t phaseStartTicks = __foster_getticks_start();
-#endif
-
-    //clocktimer<false> ct; ct.start();
-    // Remembered sets would be ignored for full-heap collections, because
-    // remembered sets skip co-condemned pointers, and everything is condemned.
-    uint64_t numRemSetRoots =
-        voluntary ? process_remsets(active_space, incoming_ptr_addrs) : 0;
-
-    //double roots_ms = ct.elapsed_ms();
-
-/*
-    fprintf(gclog, "gc %zd, voluntary %d; space %p of size %zu bytes had %zu potential incoming ptrs, %zu remset roots\n",
-      gcglobals.num_gcs_triggered, int(voluntary), active_space,
-      active_space->approx_size_in_bytes(), incoming_ptr_addrs.size(), numRemSetRoots);
-      */
-
-    //ct.start();
-    visitGCRoots(__builtin_frame_address(0), active_space);
-    //double trace_ms = ct.elapsed_ms();
-
-#if FOSTER_GC_TIME_HISTOGRAMS && ENABLE_GC_TIMING_TICKS
-    hdr_record_value(gcglobals.hist_gc_rootscan_ticks, __foster_getticks_elapsed(phaseStartTicks, __foster_getticks_end()));
-#endif
-
-    hdr_record_value(gcglobals.hist_gc_stackscan_roots, gNumRootsScanned);
-    gNumRootsScanned = 0;
-
-
-
-    foster_bare_coro** coro_slot = __foster_get_current_coro_slot();
-    foster_bare_coro*  coro = *coro_slot;
-    if (coro) {
-      if (GCLOG_DETAIL > 1) {
-        fprintf(gclog, "==========visiting current coro: %p\n", coro); fflush(gclog);
-      }
-      visit_root(active_space, (unchecked_ptr*)coro_slot, NULL);
-      if (GCLOG_DETAIL > 1) {
-        fprintf(gclog, "==========visited current coro: %p\n", coro); fflush(gclog);
-      }
-    }
-
-    //ct.start();
-    immix_worklist.process(active_space, *this);
-    //double worklist_ms = ct.elapsed_ms();
-
-    auto deltaRecursiveMarking_us = phase.elapsed_us();
-    phase.start();
-#if FOSTER_GC_TIME_HISTOGRAMS && ENABLE_GC_TIMING_TICKS
-    phaseStartTicks = __foster_getticks_start();
-#endif
-
-    // After marking finishes, and before we sweep, we can uncondemn.
-    //ct.start();
-    gcglobals.condemned_set.uncondemn_all();
-    //double uncondemn_ms = ct.elapsed_ms();
-
-    //ct.start();
-    gcglobals.condemned_set.sweep_condemned(active_space, phase, gcstart, deltaRecursiveMarking_us);
-    //double sweep_ms = ct.elapsed_ms();
-
-/*
-    if (!voluntary) {
-      fprintf(gclog, "phase times:\n");
-      fprintf(gclog, "   roots: %.2f ms\n", roots_ms);
-      fprintf(gclog, "   trace: %.2f ms\n", trace_ms);
-      fprintf(gclog, "   worklist: %.2f ms\n", worklist_ms);
-      fprintf(gclog, "   uncondemn: %.2f ms\n", uncondemn_ms);
-      fprintf(gclog, "   sweep: %.2f ms\n", sweep_ms);    
-    }
-    */
-
-    if (GC_ASSERTIONS) { gcglobals.marked_in_current_gc.clear(); }
-
-#if FOSTER_GC_TIME_HISTOGRAMS && ENABLE_GC_TIMING_TICKS
-    hdr_record_value(gcglobals.hist_gc_postgc_ticks, __foster_getticks_elapsed(phaseStartTicks, __foster_getticks_end()));
-#endif
-
-#if ((GCLOG_DETAIL > 1) || ENABLE_GCLOG_ENDGC)
-#   if TRACK_NUM_OBJECTS_MARKED
-        fprintf(gclog, "\t%zu objects marked in this GC cycle, %zu marked overall; %zu bytes live\n",
-                gcglobals.num_objects_marked_total - num_marked_at_start,
-                gcglobals.num_objects_marked_total,
-                gcglobals.alloc_bytes_marked_total - bytes_marked_at_start);
-#   endif
-      if (TRACK_NUM_REMSET_ROOTS) {
-        fprintf(gclog, "\t%lu objects identified in remset\n", numRemSetRoots);
-      }
-      if (ENABLE_GC_TIMING) {
-        double delta_us = gcstart.elapsed_us();
-        fprintf(gclog, "\ttook %zd us which was %f%% marking\n",
-                          int64_t(delta_us),
-                          (deltaRecursiveMarking_us * 100.0)/delta_us);      }
-      fprintf(gclog, "       num_free_lines (line spaces only): %d, num avail bytes: %zd\n", num_free_lines,
-                                       global_immix_line_allocator.count_available_bytes()); num_free_lines = 0;
-      fprintf(gclog, "\t/endgc %d of immix heap %p, voluntary? %d; gctype %d\n\n", gcglobals.num_gcs_triggered,
-                                                active_space, int(voluntary), int(gcglobals.condemned_set.status));
-
-      fflush(gclog);
-#endif
-
-  if (PRINT_STDOUT_ON_GC) { fprintf(stdout, "                              endgc\n"); fflush(stdout); }
-
-  if (ENABLE_GC_TIMING) {
-    double delta_us = gcstart.elapsed_us();
-    if (FOSTER_GC_TIME_HISTOGRAMS) {
-      hdr_record_value(gcglobals.hist_gc_pause_micros, int64_t(delta_us));
-    }
-    gcglobals.gc_time_us += delta_us;
-  }
-
-#if ENABLE_GC_TIMING_TICKS
-    int64_t t1 = __foster_getticks_end();
-    if (FOSTER_GC_TIME_HISTOGRAMS) {
-      hdr_record_value(gcglobals.hist_gc_pause_ticks, __foster_getticks_elapsed(t0, t1));
-    }
-    gcglobals.subheap_ticks += __foster_getticks_elapsed(t0, t1);
-#endif
-  }
-
-template<typename Allocator>
-void condemned_set<Allocator>::sweep_condemned(Allocator* active_space,
-             clocktimer<false>& phase, clocktimer<false>& gcstart,
-             double deltaRecursiveMarking_us) {
-  std::vector<heap_handle<immix_heap>*> subheap_handles;
-
-  switch (status) {
-    case condemned_set_status::single_subheap_condemned: {
-      active_space->immix_sweep(phase, gcstart);
-      break;
-    }
-
-    case condemned_set_status::per_frame_condemned: {
-      for (auto space : spaces) {
-        space->immix_sweep(phase, gcstart);
-      }
-      spaces.clear();
-      break;
-    }
-
-    case condemned_set_status::whole_heap_condemned: {
-      subheap_handles.swap(gcglobals.all_subheap_handles_except_default_allocator);
-
-      if (GC_ASSERTIONS) {
-        std::set<immix_heap*> subheaps;
-        for (auto handle : subheap_handles) { subheaps.insert(handle->body); }
-        if (subheaps.size() != subheap_handles.size()) {
-          fprintf(gclog, "INVARIANT VIOLATED: subheap handles contains duplicates!\n");
-        }
-      }
-
-      // Before we clear line marks, remove any stale remset entries.
-      // If we don't do this, the following bad thing can happen:
-      //   * Object A stored in slot B, so A's space records slot B in its remset.
-      //   * Slot B becomes dead.
-      //      (keep in mind B's space doesn't know what other spaces have B in their remsets)
-      //   * Whole-heap GC leaves A unmarked, because whole-heap GCs ignore remsets,
-      //     and B was (one of) the last supporters of A.
-      //   * Allocation in A puts an arbitrary bit pattern in B's referent
-      //     (especially the header/typemap)
-      //   * Single-subheap GC of A follows the remset entry for B and goes off the rails.
-      /*
-      gcglobals.default_allocator->trim_remset();
-      for (auto handle : subheap_handles) {
-        handle->body->trim_remset();
-      }
-      */
-
-      gcglobals.default_allocator->immix_sweep(phase, gcstart);
-      for (auto handle : subheap_handles) {
-        handle->body->immix_sweep(phase, gcstart);
-      }
-
-      break;
-    }
-  }
-
-  // Invariant: line spaces have returned unmarked linegroups to the global line allocator.
-  global_immix_line_allocator.reclaim_frames(gcglobals.space_limit);
-
-  // Subheap deallocation effectively only happens for whole-heap collections.
-  for (auto handle : subheap_handles) {
-    // A space should be deallocated only if it is both inaccessible (meaning unmarked)
-    // and empty. A marked space, empty or not, might be activated in the future.
-    // A non-empty unmarked space won't be activated, but it's not dead until the objects
-    // within it become inaccessible.
-    heap_cell* handle_cell = handle->as_cell();
-    auto space = handle->body;
-    if ((!obj_is_marked(handle_cell)) && space->is_empty()) {
-      //fprintf(gclog, "DELETING SPACE %p\n", space);
-      //delete space;
-    } else {
-      gcglobals.all_subheap_handles_except_default_allocator.push_back(handle);
-    }
-  }
-
-  // Handles (and other unframed allocations) must be unmarked too.
-  for (auto c : unframed_and_marked) {
-    do_unmark_granule(c);
-  }
-  unframed_and_marked.clear();
+  current_frame->mark_owner(owner, cell_size);
 }
 
 // }}}
@@ -2769,21 +2086,20 @@
 
 class immix_space : public heap {
 public:
-  immix_space() {
-    if (GCLOG_DETAIL > 2) { fprintf(gclog, "new immix_space %p, current space limit: %zd f15s\n", this, gcglobals.space_limit->frame15s_left); }
-
-    incoming_ptr_addrs.set_empty_key(nullptr);
+  immix_space(byte_limit* lim) : lim(lim) {
+    fprintf(gclog, "new immix_space %p, byte limit ptr: %p, current value: %zd f15s\n", this, lim, lim->frame15s_left);
   }
+  // TODO take a space limit. Use a combination of local & global
+  // frame21_allocators to service requests for frame15s.
 
   virtual void dump_stats(FILE* json) {
     return;
   }
 
-  // Marks all frames (including clean ones) as condemned.
+  virtual byte_limit* get_byte_limit() { return lim; }
+
   virtual void condemn() {
-    if (GCLOG_DETAIL > 0) {
-      fprintf(gclog, "condemning %zu frames...\n", tracking.logical_frame15s()); fflush(gclog);
-    }
+    fprintf(gclog, "condemning %zu frames...\n", tracking.logical_frame15s()); fflush(gclog);
     int n = 0;
     int m = 0;
     clocktimer<false> ct; ct.start();
@@ -2800,35 +2116,12 @@
     });
     // TODO condemn array frames
 
-    if (GCLOG_DETAIL > 0) {
-      fprintf(gclog, "condemned (%d + %d = %d) / %zu frames in %f microseconds\n",
-          n, m, n + m, tracking.logical_frame15s(),
-          ct.elapsed_us());
-    }
-  }
-
-  virtual void uncondemn() {
-    tracking.iter_frame15( [&](frame15* f15) {
-      set_condemned_status_for_frame15_id(frame15_id_of(f15), condemned_status::not_condemned);
-      return true;
-    });
-    tracking.iter_coalesced_frame21( [&](frame21* f21) {
-      // The fact that we own the entire frame21 indicates that none of its frame15s are line-based.
-      set_condemned_status_for_frame21(f21, condemned_status::not_condemned);
-    });
-
-    // TODO uncondemn array frames
+    fprintf(gclog, "condemned (%d + %d = %d) / %zu frames in %f microseconds\n",
+        n, m, n + m, tracking.logical_frame15s(),
+        ct.elapsed_us());
   }
 
   void clear_current_blocks() {
-    if (GCLOG_DETAIL > 3) {
-      fprintf(gclog, "clear_current_blocks: small %p (%u), medium %p (%u)\n",
-          small_bumper.base,
-          frame15_id_of(small_bumper.base),
-          medium_bumper.base,
-          frame15_id_of(medium_bumper.base)
-          );
-    }
     // TODO clear mem to avoid conservative pointer leaks
     small_bumper.base = small_bumper.bound;
     medium_bumper.base = medium_bumper.bound;
@@ -2838,10 +2131,7 @@
     common.visit_root(this, root, slotname);
   }
 
-  virtual void force_gc_for_debugging_purposes() {
-    if (GCLOG_DETAIL > 2) { fprintf(gclog, "force_gc_for_debugging_purposes triggering immix gc\n"); }
-    common.common_gc(this, incoming_ptr_addrs, true);
-  }
+  virtual void force_gc_for_debugging_purposes() { this->immix_gc(); }
 
   // {{{ Prechecked allocation functions
   template <int N>
@@ -2871,40 +2161,33 @@
     // The immix paper uses a policy of expansion -> recycled -> clean used.
     // The order below is different.
 
-    // Recycled frames are only used if we just need one free line.
-    // Using recycled frames for medium allocations raises
+    // Recycled frames are only used for small allocations, for which we only
+    // need one free line. Using recycled frames for medium allocations raises
     // the risk for fragmentation to require searching many recycled frames.
-    if (!recycled_lines.empty() && cell_size <= IMMIX_LINE_SIZE) {
-      free_linegroup* g = recycled_lines.back();
+
+    if (!recycled.empty() && cell_size <= IMMIX_LINE_SIZE) {
+      free_linegroup* g = recycled.back();
 
       if (g->next) {
-        recycled_lines.back() = g->next;
-      } else { recycled_lines.pop_back(); }
+        recycled.back() = g->next;
+      } else { recycled.pop_back(); }
 
       bumper->bound = g->bound;
       bumper->base  = realigned_for_allocation(g);
 
-      if (MEMSET_FREED_MEMORY) { memset(g, 0xef, distance(g, g->bound)); }
-
-      if ((GCLOG_DETAIL > 0) || ENABLE_GCLOG_PREP) {
-        fprintf(gclog, "after GC# %d, using recycled line group in line %d of full frame (%u); # lines %.2f (bytes %d); # groups left: %zu\n",
-            gcglobals.num_gcs_triggered,
-            line_offset_within_f15(bumper->base),
-            frame15_id_of(g),
-            double(bumper->size()) / double(IMMIX_LINE_SIZE),
-            bumper->size(), recycled_lines.size());
+      if (ENABLE_GCLOG || ENABLE_GCLOG_PREP) {
+        fprintf(gclog, "using recycled line group in frame %p; # left: %zu\n", (void*)(uintptr_t(frame15_id_of(g)) << 15), recycled.size());
         //for (int i = 0; i < IMMIX_LINES_PER_BLOCK; ++i) { fprintf(gclog, "%c", linemap[i] ? 'd' : '_'); } fprintf(gclog, "\n");
       }
       return true;
     }
 
-    if (gcglobals.space_limit->frame15s_left > 0) {
-      --gcglobals.space_limit->frame15s_left;
+    if (lim->frame15s_left > 0) {
+      --lim->frame15s_left;
       // Note: cannot call clear() on global allocator until
       // all frame15s it has distributed are relinquished.
       frame15* f = global_frame15_allocator.get_frame15();
-      gc_assert(! is_in_metadata_frame(f), "shouldn't allocate into a metadata frame!");
-      if (ENABLE_GCLOG_PREP) { fprintf(gclog, "grabbed global frame15: %p (%u) into space %p\n", f, frame15_id_of(f), this); }
+      if (ENABLE_GCLOG_PREP) { fprintf(gclog, "grabbed global frame15: %p into space %p\n", f, this); }
       tracking.add_frame15(f);
       set_parent_for_frame(this, f);
       bumper->base  = realigned_for_allocation(f);
@@ -2963,23 +2246,11 @@
   {
     int64_t cell_size = typeinfo->cell_size; // includes space for cell header.
 
-    if (GCLOG_DETAIL > 2) { fprintf(gclog, "allocate_cell_slowpath triggering immix gc\n"); }
-
-    // When we run out of memory, we should collect the whole heap, regardless of
-    // what the active subheap happens to be -- the underlying principle being that
-    // subheap-enabled code shouldn't needlessly diverge from more traditional
-    // runtime's behavior in these cases.
-    // An alternative would be to collect the current subheap and then, if that
-    // doesn't reclaim "enough", to try the whole heap, but that's a shaky heuristic
-    // that can easily lead to nearly-doubled wasted work.
-    {
-      condemned_set_status_manager tmp(condemned_set_status::whole_heap_condemned);
-      common.common_gc(this, incoming_ptr_addrs, false);
-    }
-
-    if (GCLOG_DETAIL > 2) {
-      fprintf(gclog, "forced heap-frame gc reclaimed %zd frames\n", gcglobals.space_limit->frame15s_left);
-    }
+    //fprintf(gclog, "allocate_cell_slowpath triggering immix gc\n");
+    gcglobals.num_gcs_triggered_involuntarily++;
+    this->immix_gc();
+    //fprintf(gclog, "allocate_cell_slowpath triggered immix gc\n");
+    //printf("allocate_cell_slowpath trying to establish cell precond\n"); fflush(stdout);
 
     if (!try_establish_alloc_precondition(&small_bumper, cell_size)) {
       helpers::oops_we_died_from_heap_starvation(); return NULL;
@@ -3003,7 +2274,7 @@
     int64_t slot_size = elt_typeinfo->cell_size; // note the name change!
     int64_t req_bytes = array_size_for(n, slot_size);
 
-    //fprintf(gclog, "immix space allocating array, %d elts * %d b = %d bytes\n", n, slot_size, req_bytes);
+    //fprintf(gclog, "allocating array, %d elts * %d b = %d bytes\n", n, slot_size, req_bytes);
 
     if (false && FOSTER_GC_ALLOC_HISTOGRAMS) {
       hdr_record_value(gcglobals.hist_gc_alloc_array, req_bytes);
@@ -3025,12 +2296,8 @@
     if (try_establish_alloc_precondition(bumper, req_bytes)) {
       return helpers::allocate_array_prechecked(bumper, elt_typeinfo, n, req_bytes, common.prevent_const_prop(), init);
     } else {
-      if (GCLOG_DETAIL > 2) { fprintf(gclog, "allocate_array_into_bumper triggering immix gc\n"); }
-      {
-        condemned_set_status_manager tmp(condemned_set_status::whole_heap_condemned);
-        common.common_gc(this, incoming_ptr_addrs, false);
-      }
-
+      gcglobals.num_gcs_triggered_involuntarily++;
+      this->immix_gc();
       if (try_establish_alloc_precondition(bumper, req_bytes)) {
         //fprintf(gclog, "gc collection freed space for array, now have %lld\n", curr->free_size());
         //fflush(gclog);
@@ -3064,26 +2331,68 @@
       }
       */
 
-  virtual bool is_empty() { return recycled_lines.empty() && laa.empty()
-                                    && tracking.logical_frame15s() == 0; }
-
-  virtual uint64_t approx_size_in_bytes() {
-    return tracking.logical_frame15s() * (IMMIX_BYTES_PER_LINE * IMMIX_LINES_PER_FRAME15);
-  }
-
-  virtual void trim_remset() { helpers::do_trim_remset(incoming_ptr_addrs, this); }
-  
-  virtual void immix_sweep(clocktimer<false>& phase,
-                           clocktimer<false>& gcstart) {
+  void immix_gc() {
+
+    //printf("GC\n");
+    //fprintf(gclog, "GC\n");
+
+    clocktimer<false> gcstart; gcstart.start();
+    clocktimer<false> phase;
+#if ENABLE_GC_TIMING_TICKS
+    int64_t t0 = __foster_getticks_start();
+#endif
+
+    auto num_marked_at_start = gcglobals.num_objects_marked_total;
+    if (ENABLE_GCLOG) {
+      fprintf(gclog, ">>>>>>> visiting gc roots on current stack\n"); fflush(gclog);
+    }
+
+    //worklist.initialize();
+    //common.flip_current_mark_bits_value(); // TODO-X
+
+    phase.start();
+#if FOSTER_GC_TIME_HISTOGRAMS && ENABLE_GC_TIMING_TICKS
+    int64_t phaseStartTicks = __foster_getticks_start();
+#endif
+
+
+    // TODO check condemned set instead of forcibly using this space
+    uint64_t numRemSetRoots = common.process_remset<true>(this, incoming_ptr_addrs);
+
+    visitGCRoots(__builtin_frame_address(0), this);
+
+#if FOSTER_GC_TIME_HISTOGRAMS && ENABLE_GC_TIMING_TICKS
+    hdr_record_value(gcglobals.hist_gc_rootscan_ticks, __foster_getticks_elapsed(phaseStartTicks, __foster_getticks_end()));
+#endif
+
+    foster_bare_coro** coro_slot = __foster_get_current_coro_slot();
+    foster_bare_coro*  coro = *coro_slot;
+    if (coro) {
+      if (ENABLE_GCLOG) {
+        fprintf(gclog, "==========visiting current coro: %p\n", coro); fflush(gclog);
+      }
+      this->visit_root((unchecked_ptr*)coro_slot, NULL);
+      if (ENABLE_GCLOG) {
+        fprintf(gclog, "==========visited current coro: %p\n", coro); fflush(gclog);
+      }
+    }
+
+    immix_worklist.process(this);
+
+    auto deltaRecursiveMarking_us = phase.elapsed_us();
+    phase.start();
+#if FOSTER_GC_TIME_HISTOGRAMS && ENABLE_GC_TIMING_TICKS
+    phaseStartTicks = __foster_getticks_start();
+#endif
+    // Coarse grained sweep, post-collection
+    {
     // The current block will probably get marked recycled;
     // rather than trying to stop it, we just accept it and reset the base ptr
     // so that the next allocation will trigger a fetch of a new block to use.
     clear_current_blocks();
 
-    // This vector will get filled by the calls to inspect_*_postgc().
-    recycled_lines.clear();
-
-    //// TODO how/when do we sweep arrays from "other" subheaps for full-heap collections?
+    // These vectors will get filled by the calls to inspect_*_postgc().
+    recycled.clear();
     laa.sweep_arrays();
 
     clocktimer<false> insp_ct; insp_ct.start();
@@ -3099,56 +2408,82 @@
     auto inspectFrame21Time_us = insp_ct.elapsed_us();
 
     auto deltaPostMarkingCleanup_us = phase.elapsed_us();
-
-
-#if ((GCLOG_DETAIL > 1) && ENABLE_GCLOG_ENDGC)
+#if FOSTER_GC_TIME_HISTOGRAMS && ENABLE_GC_TIMING_TICKS
+    hdr_record_value(gcglobals.hist_gc_postgc_ticks, __foster_getticks_elapsed(phaseStartTicks, __foster_getticks_end()));
+#endif
+    //if (TRACK_BYTES_KEPT_ENTRIES) { hpstats.bytes_kept_per_gc.record_sample(next->used_size()); }
+
+
+#if (ENABLE_GCLOG || ENABLE_GCLOG_ENDGC)
       size_t frame15s_total = tracking.logical_frame15s();
       auto total_heap_size = foster::humanize_s(double(frame15s_total * (1 << 15)), "B");
-      //size_t frame15s_recycled = frame15s_in_reserve_recycled();
-      //size_t frame15s_kept = frame15s_total - (frame15s_recycled + tracking.frame15s_in_reserve_clean());
-      //auto total_live_size = foster::humanize_s(double(frame15s_kept) * (1 << 15), "B");
-
-      fprintf(gclog, "logically %zu frame15s, comprised of %zu frame21s and %zu actual frame15s; %zd frames left\n",
-          frame15s_total,
-          tracking.count_frame21s(), tracking.physical_frame15s(), gcglobals.space_limit->frame15s_left);
+      size_t frame15s_kept = frame15s_total - (recycled.size() + tracking.frame15s_in_reserve_clean());
+      auto total_live_size = foster::humanize_s(double(frame15s_kept) * (1 << 15), "B");
+
+      fprintf(gclog, "logically %zu frame15s, comprised of %zu frame21s and %zu actual frame15s\n", frame15s_total,
+          tracking.count_frame21s(), tracking.physical_frame15s());
       describe_frame15s_count("tracking  ", frame15s_total);
-      //describe_frame15s_count("  recycled", frame15s_recycled);
+      describe_frame15s_count("  recycled", frame15s_in_reserve_recycled());
       describe_frame15s_count("  clean   ", tracking.frame15s_in_reserve_clean());
       fprintf(gclog, "tracking %zu f21s, ended with %zu clean f21s\n", tracking.count_frame21s(), tracking.count_clean_frame21s());
 
-      // %zu recycled, 
-      fprintf(gclog, "%zu clean f15 + %zu clean f21; ",
-          //frame15s_recycled,
-          tracking.count_clean_frame15s(),
-          tracking.count_clean_frame21s());
-      //fprintf(gclog, "%zd total (%zd f21) => (%zd f15 @ %s kept / %s)",
-      //    frame15s_total, tracking.count_frame21s(),
-      //    frame15s_kept, total_live_size.c_str(), total_heap_size.c_str());
-
+      fprintf(gclog, "%zu recycled, %zu clean f15 + %zu clean f21; %zd total (%zd f21) => (%zu f15 @ %s kept / %s)",
+          recycled.size(), tracking.count_clean_frame15s(), tracking.count_clean_frame21s(),
+          frame15s_total, tracking.count_frame21s(),
+          frame15s_kept, total_live_size.c_str(), total_heap_size.c_str());
+      if (ENABLE_GC_TIMING) {
+        double delta_us = gcstart.elapsed_us();
+        fprintf(gclog, ", took %zd us which was %f%% marking, %f%% post-mark",
+          int64_t(delta_us),
+          (deltaRecursiveMarking_us * 100.0)/delta_us,
+          (deltaPostMarkingCleanup_us * 100.0)/delta_us);
+      }
       fprintf(gclog, "\n");
 
-      //if (ENABLE_GC_TIMING) {
-      //  fprintf(gclog, "\ttook %f us inspecting frame15s, %f us inspecting frame21s\n",
-      //      inspectFrame15Time_us, inspectFrame21Time_us);
-      //}
+      if (ENABLE_GC_TIMING) {
+        fprintf(gclog, "\ttook %f us inspecting frame15s, %f us inspecting frame21s\n",
+            inspectFrame15Time_us, inspectFrame21Time_us);
+      }
+
+#   if TRACK_NUM_OBJECTS_MARKED
+        fprintf(gclog, "\t%zu objects marked in this GC cycle, %zu marked overall\n",
+                gcglobals.num_objects_marked_total - num_marked_at_start,
+                gcglobals.num_objects_marked_total);
+#   endif
+#   if TRACK_NUM_REMSET_ROOTS
+        fprintf(gclog, "\t%lu objects identified in remset\n", numRemSetRoots);
+#   endif
+      fprintf(gclog, "\t/endgc\n\n");
+      fflush(gclog);
 #endif
-
-    tracking.release_clean_frames(gcglobals.space_limit);
+    }
+
+    tracking.release_clean_frames(lim);
+
+    if (ENABLE_GC_TIMING) {
+      double delta_us = gcstart.elapsed_us();
+      if (FOSTER_GC_TIME_HISTOGRAMS) {
+        hdr_record_value(gcglobals.hist_gc_pause_micros, int64_t(delta_us));
+      }
+      gcglobals.gc_time_us += delta_us;
+    }
+
+#if ENABLE_GC_TIMING_TICKS
+    int64_t t1 = __foster_getticks_end();
+    if (FOSTER_GC_TIME_HISTOGRAMS) {
+      hdr_record_value(gcglobals.hist_gc_pause_ticks, __foster_getticks_elapsed(t0, t1));
+    }
+    gcglobals.subheap_ticks += __foster_getticks_elapsed(t0, t1);
+#endif
+
+    gcglobals.num_gcs_triggered += 1;
   }
 
   void describe_frame15s_count(const char* start, size_t f15s) {
     auto h = foster::humanize_s(double(f15s) * (1 << 15), "B");
     fprintf(gclog, "%s: %6zd f15s == %s\n", start, f15s, h.c_str());
   }
-
-  // Note: O(n) (in the number of recycled line groups).
-  size_t frame15s_in_reserve_recycled() {
-    std::set<frame15_id> recycled;
-    for (auto g : recycled_lines) {
-      recycled.insert(frame15_id_of(g));
-    }
-    return recycled.size();
-  }
+  size_t frame15s_in_reserve_recycled() { return recycled.size(); }
 
   void inspect_frame21_postgc(frame21* f21) {
     bool is_frame21_entirely_clear = is_linemap_clear(f21);
@@ -3158,16 +2493,13 @@
     } else {
       // Handle the component frame15s individually.
       frame15_id fid = frame15_id_of(f21);
-      if (GCLOG_DETAIL > 1) {
-        fprintf(gclog, "   inspect_frame21_postgc: iterating frames of f21 %p (%d)\n", f21, frame15_id_within_f21(frame15_id_of(f21)));
-      }
-
+      fprintf(gclog, "   iterating frames of f21 %p\n", f21);
       for (int i = 1; i < IMMIX_F15_PER_F21; ++i) {
         frame15* f15 = frame15_for_frame15_id(fid + i);
         bool unclean = inspect_frame15_postgc(fid + i, f15);
         if (unclean) { // Clean frames already noted;
           // We don't want to re-track regular frame15s, only split ones.
-          if (GCLOG_DETAIL > 1) { fprintf(gclog, "  adding f15 %p of f21 %p\n", f15, f21); }
+          fprintf(gclog, "  adding f15 %p of f21 %p\n", f15, f21);
           tracking.add_frame15(f15);
         }
       }
@@ -3179,7 +2511,7 @@
     uint8_t* linemap = linemap_for_frame15_id(fid);
     int num_marked_lines = count_marked_lines_for_frame15(f15, linemap);
 
-    if (GCLOG_DETAIL > 2) {
+    if (ENABLE_GCLOG) {
       fprintf(gclog, "frame %u: ", fid);
       for(int i = 0;i < IMMIX_LINES_PER_BLOCK; ++i) { fprintf(gclog, "%c", (linemap[i] == 0) ? '_' : 'd'); }
       fprintf(gclog, "\n");
@@ -3202,20 +2534,28 @@
 
     // One or more holes left to process?
     while (num_available_lines > 0) {
+      //fprintf(gclog, "for %p, num_avail_lines: %d\n", f15, num_available_lines); fflush(gclog);
+
       // At least one available line means this loop will terminate before cursor == 0
       // Precondition: cursor is marked
       while (line_is_marked(cursor - 1, linemap)) --cursor;
       // Postcondition: cursor is marked; cursor - 1 is unmarked.
       int rightmost_unmarked_line = --cursor;
+      //fprintf(gclog, "rightmost unmarked line: %d\n", rightmost_unmarked_line); fflush(gclog);
+      //fprintf(gclog, "cursor(%d) marked? %d\n", cursor, line_is_marked(cursor, linemap)); fflush(gclog);
 
       while (cursor >= 0 && line_is_unmarked(cursor, linemap)) --cursor;
       // Postcondition: line_is_marked(cursor), line_is_unmarked(cursor + 1), cursor >= -1
       int leftmost_unmarked_line = cursor + 1;
+      //fprintf(gclog, "leftmost unmarked line: %d\n", leftmost_unmarked_line); fflush(gclog);
+      //fprintf(gclog, "cursor(%d) marked? %d\n", cursor, line_is_marked(cursor, linemap)); fflush(gclog);
+      //fprintf(gclog, "cursor+1 marked? %d\n", line_is_marked(cursor + 1, linemap)); fflush(gclog);
+
+      //fprintf(gclog, "free linegroup between lines %d and %d\n", leftmost_unmarked_line, rightmost_unmarked_line);
 
       free_linegroup* g = (free_linegroup*) offset(f15,   leftmost_unmarked_line      * IMMIX_BYTES_PER_LINE);
       g->bound =                            offset(f15, (rightmost_unmarked_line + 1) * IMMIX_BYTES_PER_LINE);
-
-      if (MEMSET_FREED_MEMORY) { memset(offset(g, 16), 0xdd, distance(g, g->bound) - 16); }
+      //fprintf(gclog, "linegroup size in bytes: %d\n", ((uint8_t*)g->bound) - ((uint8_t*)g));
       g->next = nextgroup;
       nextgroup = g;
 
@@ -3227,13 +2567,17 @@
     // Postcondition: nextgroup refers to leftmost hole, if any
 
     if (nextgroup) {
-      if (GCLOG_DETAIL > 0) { fprintf(gclog, "Adding frame %p to recycled list; n marked = %d\n", f15, num_marked_lines); }
-      recycled_lines.push_back(nextgroup);
+      if (ENABLE_GCLOG) { fprintf(gclog, "Adding frame %p to recycled list; n marked = %d\n", f15, num_marked_lines); }
+      recycled.push_back(nextgroup);
     }
 
     // Clear line and object mark bits.
     memset(linemap, 0, IMMIX_LINES_PER_BLOCK);
-    clear_object_mark_bits_for_frame15(f15);
+    if (MARK_OBJECT_WITH_BITS) {
+      memset(&gcglobals.lazy_mapped_granule_marks[global_granule_for(f15) / 8], 0, IMMIX_GRANULES_PER_BLOCK / 8);
+    } else {
+      memset(&gcglobals.lazy_mapped_granule_marks[global_granule_for(f15)], 0, IMMIX_GRANULES_PER_BLOCK);
+    }
 
     // TODO increment mark histogram
 
@@ -3241,6 +2585,10 @@
     return true;
   }
 
+  void add_subheap_handle(heap_handle<immix_heap>* subheap) {
+    subheaps.push_back(subheap);
+  }
+
   virtual void remember_outof(void** slot, void* val) {
     auto mdb = metadata_block_for_slot((void*) slot);
     uint8_t* cards = (uint8_t*) mdb->cardmap;
@@ -3249,17 +2597,13 @@
 
   virtual void remember_into(void** slot) {
     //frames_pointing_here.insert(frame15_id_of((void*) slot));
-    /*
-    fprintf(gclog, "remember_into, before have remembered pointers for space %p:\n", this);
-    for (auto p : incoming_ptr_addrs) {
-      fprintf(gclog, "       %p\n",  p);
-    }
-    */
     incoming_ptr_addrs.insert((tori**) slot);
-    //fprintf(gclog, "remember_into, after: count of %p is %d, size %zd\n", slot, incoming_ptr_addrs.count((tori**) slot), incoming_ptr_addrs.size());
   }
 
 public:
+  // How many are we allowed to allocate before being forced to GC & reuse?
+  byte_limit* lim;
+
   immix_common common;
 
 private:
@@ -3274,29 +2618,32 @@
 
   // Stores the empty spaces that the previous collection
   // identified as being viable candidates for allocation into.
-  std::vector<free_linegroup*> recycled_lines;
+  std::vector<free_linegroup*> recycled;
 
   // The points-into remembered set; each frame in this set needs to have its
   // card table inspected for pointers that actually point here.
   //std::set<frame15_id> frames_pointing_here;
-  remset_t incoming_ptr_addrs;
+  std::set<tori**> incoming_ptr_addrs;
+
+  std::vector<heap_handle<immix_heap>*> subheaps;
 
   // immix_space_end
 };
 
-void immix_worklist_t::process(immix_heap* target, immix_common& common) {
+void immix_worklist::process(immix_heap* target) {
   while (!empty()) {
     heap_cell* cell = peek_front();
     advance();
 
-    switch (gcglobals.condemned_set.status) {
-    case               condemned_set_status::single_subheap_condemned:
-      common.scan_cell<condemned_set_status::single_subheap_condemned>(target, cell, kFosterGCMaxDepth);
-    case               condemned_set_status::per_frame_condemned:
-      common.scan_cell<condemned_set_status::per_frame_condemned>(target, cell, kFosterGCMaxDepth);
-    case               condemned_set_status::whole_heap_condemned:
-      common.scan_cell<condemned_set_status::whole_heap_condemned>(target, cell, kFosterGCMaxDepth);
+    gc_assert(false, "TODO 2727");
+    exit(1);
+    /*
+    if (target) {
+      common.scan_cell<true>(target, cell, kFosterGCMaxDepth);
+    } else {
+      common.scan_cell<false>(nullptr, cell, kFosterGCMaxDepth);
     }
+    */
   }
   initialize();
 }
@@ -3305,7 +2652,7 @@
 
 // {{{ Walks the call stack, calling visitor->visit_root() along the way.
 void visitGCRoots(void* start_frame, immix_heap* visitor) {
-  enum { MAX_NUM_RET_ADDRS = 4024 };
+  enum { MAX_NUM_RET_ADDRS = 1024 };
   // Garbage collection requires 16+ KB of stack space due to these arrays.
   ret_addr  retaddrs[MAX_NUM_RET_ADDRS];
   frameinfo frames[MAX_NUM_RET_ADDRS];
@@ -3313,13 +2660,9 @@
   // Collect frame pointers and return addresses
   // for the current call stack.
   int nFrames = foster_backtrace((frameinfo*) start_frame, frames, MAX_NUM_RET_ADDRS);
-  if (nFrames > 500) {
+  if (nFrames == MAX_NUM_RET_ADDRS) {
     gcglobals.num_big_stackwalks += 1;
   }
-  if (nFrames == MAX_NUM_RET_ADDRS) {
-    fprintf(gclog, "ERROR: backtrace is probably incomplete due to oversized stack! (%d frames)\n", nFrames); fflush(gclog);
-    exit(2);
-  }
   if (FOSTER_GC_TIME_HISTOGRAMS) {
     hdr_record_value(gcglobals.hist_gc_stackscan_frames, int64_t(nFrames));
   }
@@ -3328,7 +2671,7 @@
   if (SANITY_CHECK_CUSTOM_BACKTRACE) {
     // backtrace() fails when called from a coroutine's stack...
     int numRetAddrs = backtrace((void**)&retaddrs, MAX_NUM_RET_ADDRS);
-    if (GCLOG_DETAIL > 2) {
+    if (ENABLE_GCLOG) {
       for (int i = 0; i < numRetAddrs; ++i) {
         fprintf(gclog, "backtrace: %p\n", retaddrs[i]);
       }
@@ -3340,7 +2683,7 @@
     for (int i = 0; i < numRetAddrs; ++i) {
       if (frames[i].retaddr != retaddrs[diff + i]) {
         fprintf(gclog, "custom + system backtraces disagree: %p vs %p, diff %d\n", frames[diff + i].retaddr, retaddrs[i], diff);
-        exit(11);
+        exit(1);
       }
     }
   }
@@ -3375,35 +2718,20 @@
     frameptr fp = (frameptr) frames[i].frameptr;
     frameptr sp = (i == 0) ? fp : offset(frames[i - 1].frameptr, 2 * sizeof(void*));
 
-    const StkMapRecord* smr = gcglobals.stackMapRecords[safePointAddr];
-    if (!smr) {
-      if (GCLOG_DETAIL > 3) {
+    const stackmap::PointCluster* pc = gcglobals.clusterForAddress[safePointAddr];
+    if (!pc) {
+      if (ENABLE_GCLOG) {
         fprintf(gclog, "no point cluster for address %p with frame ptr%p\n", safePointAddr, fp);
       }
       continue;
     }
 
-    if (GCLOG_DETAIL > 3) {
-      fprintf(gclog, "\nframe %d: retaddr %p, frame ptr %p: loc count %d\n",
-        i, safePointAddr, fp, smr->numlocs);
+    if (ENABLE_GCLOG) {
+      fprintf(gclog, "\nframe %d: retaddr %p, frame ptr %p: live count w/meta %d, w/o %d\n",
+        i, safePointAddr, fp,
+        pc->liveCountWithMetadata, pc->liveCountWithoutMetadata);
     }
 
-    // Skip first three entries, which are (I think) stack and frame pointers.
-    // We hacked in fake base pointers as copies of the "real" pointers,
-    // so just ignore those.
-    for (auto z = 3; z < smr->numlocs; z += 2) {
-      const Location* loc = &(smr->locs[z]);
-    //fprintf(gclog, "             loc %d: encoding %d, offset/const %d, regnum %x, size %d\n",
-    //  z, loc->encoding, loc->offset_or_smallconstant, loc->dwarf_regnum, loc->loc_size);
-      int32_t off = loc->offset_or_smallconstant;
-      void*  metadata = nullptr;
-      void*  rootaddr = (loc->dwarf_regnum == 7) ? offset(sp, off)
-                                                 : offset(fp, off);
-      visitor->visit_root(static_cast<unchecked_ptr*>(rootaddr),
-                          static_cast<const char*>(metadata));
-    }
-
-#if 0
     const void* const* ms = pc->getMetadataStart();
     const int32_t    * lo = pc->getLiveOffsetWithMetaStart();
     int32_t frameSize = pc->frameSize;
@@ -3419,7 +2747,6 @@
 
     gc_assert(pc->liveCountWithoutMetadata == 0,
                   "TODO: scan pointer w/o metadata");
-#endif
   }
 }
 // }}}
@@ -3476,7 +2803,7 @@
 void coro_dump(foster_bare_coro* coro) {
   if (!coro) {
     fprintf(gclog, "cannot dump NULL coro ptr!\n");
-  } else if (GCLOG_DETAIL > 2) {
+  } else if (ENABLE_GCLOG) {
     coro_print(coro);
   }
 }
@@ -3507,7 +2834,7 @@
   void* frameptr = coro_topmost_frame_pointer(coro);
   gc_assert(frameptr != NULL, "coro frame ptr shouldn't be NULL!");
 
-  if (GCLOG_DETAIL > 1) {
+  if (ENABLE_GCLOG) {
     fprintf(gclog, "========= scanning coro (%p, fn=%p, %s) stack from %p\n",
         coro, foster::runtime::coro_fn(coro), coro_status_name(coro), frameptr);
   }
@@ -3515,7 +2842,7 @@
   fprintf(gclog, "coro_visitGCRoots\n"); fflush(gclog);
   visitGCRoots(frameptr, visitor);
 
-  if (GCLOG_DETAIL > 1) {
+  if (ENABLE_GCLOG) {
     fprintf(gclog, "========= scanned coro stack from %p\n", frameptr);
     fflush(gclog);
   }
@@ -3524,7 +2851,7 @@
 //////////////////////////////////////////////////////////////}}}
 
 // {{{ GC startup & shutdown
-void register_stackmaps(std::map<void*, const StkMapRecord*>&); // see foster_gc_stackmaps.cpp
+void register_stackmaps(ClusterMap&); // see foster_gc_stackmaps.cpp
 
 int address_space_prefix_size_log() {
   if (sizeof(void*) == 4) { return 32; }
@@ -3547,9 +2874,6 @@
 #endif
 }
 
-size_t lazy_mapped_frame15_condemned_size() { return (size_t(1) << (address_space_prefix_size_log() - 15)); }
-size_t lazy_mapped_granule_marks_size() {     return (size_t(1) <<  address_space_prefix_size_log()     ) / (16 * 1); } 
-
 void initialize(void* stack_highest_addr) {
   gcglobals.init_start.start();
   gclog = fopen("gclog.txt", "w");
@@ -3557,25 +2881,20 @@
 
   pages_boot();
 
-  gcglobals.space_limit = new byte_limit(gSEMISPACE_SIZE());
-  gcglobals.allocator = new immix_space();
+  gcglobals.allocator = new immix_space(new byte_limit(gSEMISPACE_SIZE()));
   gcglobals.default_allocator = gcglobals.allocator;
-  gcglobals.allocator_handle = nullptr;
-
-  gcglobals.condemned_set.status = condemned_set_status::single_subheap_condemned;
 
   gcglobals.had_problems = false;
-  gcglobals.logall = false;
-
-  register_stackmaps(gcglobals.stackMapRecords);
+
+  register_stackmaps(gcglobals.clusterForAddress);
 
   gcglobals.lazy_mapped_frame15info             = allocate_lazily_zero_mapped<frame15info>(     size_t(1) << (address_space_prefix_size_log() - 15));
   gcglobals.lazy_mapped_coarse_marks            = allocate_lazily_zero_mapped<uint8_t>(         size_t(1) << (address_space_prefix_size_log() - COARSE_MARK_LOG));
   //gcglobals.lazy_mapped_coarse_condemned        = allocate_lazily_zero_mapped<condemned_status>(size_t(1) << (address_space_prefix_size_log() - COARSE_MARK_LOG));
-  gcglobals.lazy_mapped_frame15_condemned       = allocate_lazily_zero_mapped<condemned_status>(lazy_mapped_frame15_condemned_size());
+  gcglobals.lazy_mapped_frame15_condemned       = allocate_lazily_zero_mapped<condemned_status>(size_t(1) << (address_space_prefix_size_log() - 15));
   //gcglobals.lazy_mapped_frame15info_associated  = allocate_lazily_zero_mapped<void*>(      size_t(1) << (address_space_prefix_size_log() - 15));
   //
-  gcglobals.lazy_mapped_granule_marks           = allocate_lazily_zero_mapped<uint8_t>(lazy_mapped_granule_marks_size()); // byte marks
+  gcglobals.lazy_mapped_granule_marks           = allocate_lazily_zero_mapped<uint8_t>((size_t(1) << address_space_prefix_size_log()) / (16 * 1)); // byte marks
 
   gcglobals.gc_time_us = 0.0;
   gcglobals.num_gcs_triggered = 0;
@@ -3584,16 +2903,11 @@
   gcglobals.subheap_ticks = 0.0;
   gcglobals.num_allocations = 0;
   gcglobals.num_alloc_bytes = 0;
-  gcglobals.num_subheaps_created = 0;
-  gcglobals.num_subheap_activations = 0;
   gcglobals.write_barrier_phase0_hits = 0;
   gcglobals.write_barrier_phase1_hits = 0;
-  gcglobals.write_barrier_slow_path_ticks = 0;
   gcglobals.num_objects_marked_total = 0;
-  gcglobals.alloc_bytes_marked_total = 0;
 
   hdr_init(1, 6000000, 2, &gcglobals.hist_gc_stackscan_frames);
-  hdr_init(1, 6000000, 2, &gcglobals.hist_gc_stackscan_roots);
   hdr_init(1, 600000000, 2, &gcglobals.hist_gc_postgc_ticks);
   hdr_init(1, 600000000, 3, &gcglobals.hist_gc_pause_micros); // 600M us => 600 seconds => 10 minutes
   hdr_init(1, 600000000, 2, &gcglobals.hist_gc_pause_ticks);
@@ -3601,20 +2915,6 @@
   hdr_init(1, 1000000000000, 3, &gcglobals.hist_gc_alloc_array); // 1 TB
   hdr_init(129, 1000000, 3, &gcglobals.hist_gc_alloc_large); // 1 MB
   memset(gcglobals.enum_gc_alloc_small, 0, sizeof(gcglobals.enum_gc_alloc_small));
-
-  if (foster__typeMapList[0]) {
-    void* min_addr_typemap = foster__typeMapList[0];
-    void* max_addr_typemap = foster__typeMapList[0];
-    // Determine the bounds of the typemap
-    for (void** typemap = &foster__typeMapList[1]; *typemap; ++typemap) {
-      min_addr_typemap = std::min(min_addr_typemap, *typemap);
-      max_addr_typemap = std::max(max_addr_typemap, *typemap);
-    }
-    gcglobals.typemap_memory.base = min_addr_typemap;
-    gcglobals.typemap_memory.bound = offset(max_addr_typemap, 8);
-  } else {
-    fprintf(gclog, "skipping type map list registration\n");
-  }
 }
 
 
@@ -3687,11 +2987,6 @@
 
     struct hdr_iter_percentiles * percentiles = &iter.specifics.percentiles;
 
-    if (h->total_count == 0) {
-      fprintf(stream, "\t(no samples recorded)\n");
-      return 0;
-    }
-
     if (fprintf(
             stream, "%22s %12s %12s %12s %12s     (chart, linear scale)\n\n",
             "Range", "Percentile", "BucketCount", "TotalCount", "1/(1-Percentile)") < 0)
@@ -3805,9 +3100,6 @@
 
     fprintf(gclog, "gc_stackscan_frames:\n");
     foster_hdr_percentiles_print(gcglobals.hist_gc_stackscan_frames, gclog, 4);
-
-    fprintf(gclog, "gc_stackscan_roots:\n");
-    foster_hdr_percentiles_print(gcglobals.hist_gc_stackscan_roots, gclog, 4);
   }
 
   if (!json && FOSTER_GC_EFFIC_HISTOGRAMS) {
@@ -3822,14 +3114,6 @@
   fprintf(gclog, "'Num_Big_Stackwalks': %d\n", gcglobals.num_big_stackwalks);
   fprintf(gclog, "'Num_GCs_Triggered': %d\n", gcglobals.num_gcs_triggered);
   fprintf(gclog, "'Num_GCs_Involuntary': %d\n", gcglobals.num_gcs_triggered_involuntarily);
-  {
-    auto s = foster::humanize_s(double(gcglobals.num_subheaps_created), "");
-    fprintf(gclog, "'Num_Subheaps_Created': %s\n", s.c_str());
-  }
-  {
-    auto s = foster::humanize_s(double(gcglobals.num_subheap_activations), "");
-    fprintf(gclog, "'Num_Subheap_Activations': %s\n", s.c_str());
-  }
   if (TRACK_NUM_ALLOCATIONS) {
     auto s = foster::humanize_s(double(gcglobals.num_allocations), "");
     fprintf(gclog, "'Num_Allocations': %s\n", s.c_str());
@@ -3846,23 +3130,13 @@
     fprintf(gclog, "'MarkCons_Obj_div_Bytes': %e\n",
         double(gcglobals.num_objects_marked_total) / double(gcglobals.num_alloc_bytes));
   }
-  if (TRACK_NUM_OBJECTS_MARKED) {
-    fprintf(gclog, "'MarkCons_Bytes_div_Bytes': %e\n",
-        double(gcglobals.alloc_bytes_marked_total) / double(gcglobals.num_alloc_bytes));
-  }
   if (TRACK_WRITE_BARRIER_COUNTS) {
     fprintf(gclog, "'Num_Write_Barriers_Fast': %lu\n", (gcglobals.write_barrier_phase0_hits - gcglobals.write_barrier_phase1_hits));
     fprintf(gclog, "'Num_Write_Barriers_Slow': %lu\n",  gcglobals.write_barrier_phase1_hits);
   }
   if (ENABLE_GC_TIMING_TICKS) {
-    {
-      auto s = foster::humanize_s(gcglobals.write_barrier_slow_path_ticks, "");
-      fprintf(gclog, "'Write_Barrier_Slow_Path_Ticks': %s\n", s.c_str());
-    }
-    {
-      auto s = foster::humanize_s(gcglobals.subheap_ticks, "");
-      fprintf(gclog, "'Subheap_Ticks': %s\n", s.c_str());
-    }
+    auto s = foster::humanize_s(gcglobals.subheap_ticks, "");
+    fprintf(gclog, "'Subheap_Ticks': %s\n", s.c_str());
     if (gcglobals.num_gcs_triggered > 0) {
       auto v = foster::humanize_s(gcglobals.subheap_ticks / double(gcglobals.num_gcs_triggered), "");
       fprintf(gclog, "'Avg_GC_Ticks': %s\n", v.c_str());
@@ -3902,12 +3176,12 @@
 /////////////////////////////////////////////////////////////////
 
 //  {{{ Debugging utilities
-void gc_assert(bool cond_expected_true, const char* msg) {
+void gc_assert(bool cond, const char* msg) {
   if (GC_ASSERTIONS) {
-    if (!cond_expected_true) {
+    if (!cond) {
       gcglobals.allocator->dump_stats(NULL);
     }
-    foster_assert(cond_expected_true, msg);
+    foster_assert(cond, msg);
   }
 }
 
@@ -3980,26 +3254,19 @@
 // {{{ Pointer classification utilities
 const typemap* tryGetTypemap(heap_cell* cell) {
   if (uint64_t(cell->cell_size()) < uint64_t(1<<16)) return nullptr;
-  // Reminder: cell_size() and map are the same bit pattern.
   const typemap* map = cell->get_meta();
   if (GC_ASSERTIONS) {
     bool is_corrupted = (
           ((map->isCoro  != 0) && (map->isCoro  != 1))
        || ((map->isArray != 0) && (map->isArray != 1))
        || (map->numOffsets < 0)
-       || (map->cell_size  < 0)
-       || (map->cell_size  > (uint64_t(1)<<31)));
+       || (map->cell_size  < 0));
     if (is_corrupted) {
-      fprintf(gclog, "Found corrupted type map for cell %p (body %p):\n", cell, cell->body_addr()); fflush(gclog);
+      fprintf(gclog, "Found corrupted type map:\n"); fflush(gclog);
       inspect_typemap(map);
       gc_assert(!is_corrupted, "tryGetTypemap() found corrupted typemap");
     }
   }
-  
-  if (!gcglobals.typemap_memory.contains((void*) map)) {
-    return nullptr;
-  }
-
   return map;
 }
 // }}}
@@ -4066,21 +3333,19 @@
 __attribute((noinline))
 void foster_write_barrier_slowpath(immix_heap* hv, immix_heap* hs, void* val, void** slot) {
     if (TRACK_WRITE_BARRIER_COUNTS) { ++gcglobals.write_barrier_phase1_hits; }
-    if (GCLOG_DETAIL > 3) { fprintf(gclog, "space %p remembering slot %p with inc ptr %p and old pointer %p; slot heap is %p\n", hv, slot, val, *slot, hs); }
     hv->remember_into(slot);
+    //hs->remember_outof(slot, val);
 }
 
 __attribute__((always_inline))
 void foster_write_barrier_generic(void* val, void** slot) /*__attribute((always_inline))*/ {
-//void __attribute((always_inline)) foster_write_barrier_generic(void* val, void** slot) {}
+//void __attribute((always_inline)) foster_write_barrier_generic(void* val, void** slot) {
   //immix_heap* hv = heap_for_tidy((tidy*)val);
   //immix_heap* hs = heap_for_tidy((tidy*)slot);
-
   immix_heap* hv = heap_for_wb(val);
   immix_heap* hs = heap_for_wb((void*)slot);
   if (TRACK_WRITE_BARRIER_COUNTS) { ++gcglobals.write_barrier_phase0_hits; }
-  //fprintf(gclog, "write barrier (%zu) writing ptr %p from heap %p into slot %p in heap %p\n",
-  //    gcglobals.write_barrier_phase0_hits, val, hv, slot, hs); fflush(gclog);
+  //fprintf(gclog, "write barrier writing ptr %p from heap %p into slot %p in heap %p\n", val, hv, slot, hs); fflush(gclog);
   if (hv == hs) {
     *slot = val;
     return;
@@ -4098,13 +3363,7 @@
   // since statically allocated data will never be deallocated, and can never
   // point into the program heap (by virtue of its immutability).
   if (hv) {
-    if (false && ENABLE_GC_TIMING_TICKS) {
-      int64_t t0 = __foster_getticks_start();
-      foster_write_barrier_slowpath(hv, hs, val, slot);
-      gcglobals.write_barrier_slow_path_ticks += __foster_getticks_elapsed(t0, __foster_getticks_end());
-    } else {
-      foster_write_barrier_slowpath(hv, hs, val, slot);
-    }
+    foster_write_barrier_slowpath(hv, hs, val, slot);
   }
   *slot = val;
 }
@@ -4116,70 +3375,54 @@
 // aligned (though I guess it's not strictly necessary for types without any
 // constructors).
 void* foster_subheap_create_raw() {
-  ++gcglobals.num_subheaps_created;
-  immix_space* subheap = new immix_space();
+  immix_space* subheap = new immix_space(gcglobals.allocator->get_byte_limit());
+  fprintf(gclog, "created subheap %p\n", subheap);
   void* alloc = malloc(sizeof(heap_handle<immix_space>));
-  heap_handle<immix_heap>* h = (heap_handle<immix_heap>*) realigned_for_heap_handle(alloc);
-  h->header           = 32;
+  heap_handle<immix_space>* h = (heap_handle<immix_space>*)
+    realigned_for_allocation(alloc);
+  h->header           = 0;
   h->unaligned_malloc = alloc;
   h->body             = subheap;
-  gcglobals.all_subheap_handles_except_default_allocator.push_back(h);
-  return h->as_tidy();
+  //gcglobals.allocator->add_subheap_handle(h); // TODO XXXX
+  return h;
 }
 
 void* foster_subheap_create_small_raw() {
-  ++gcglobals.num_subheaps_created;
-  immix_line_space* subheap = new immix_line_space();
+  immix_line_space* subheap = new immix_line_space(gcglobals.allocator->get_byte_limit());
+  fprintf(gclog, "created small subheap %p\n", subheap);
   void* alloc = malloc(sizeof(heap_handle<heap>));
-  heap_handle<heap>* h = (heap_handle<heap>*) realigned_for_heap_handle(alloc);
-  h->header           = 32;
+  heap_handle<heap>* h = (heap_handle<heap>*)
+    realigned_for_allocation(alloc);
+  h->header           = 0;
   h->unaligned_malloc = alloc;
   h->body             = subheap;
-  gcglobals.all_subheap_handles_except_default_allocator.push_back(h);
-  return h->as_tidy();
+  //gcglobals.allocator->add_subheap_handle(h); // TODO XXXX
+  return h;
 }
 
-void* foster_subheap_activate_raw(void* generic_subheap) {
-  ++gcglobals.num_subheap_activations;
+void foster_subheap_activate_raw(void* generic_subheap) {
   // TODO make sure we properly retain (or properly remove!)
   //      a subheap that is created, installed, and then silently dropped
   //      without explicitly being destroyed.
-  //fprintf(gclog, "subheap_activate: generic %p\n", generic_subheap); fflush(gclog);
-  heap_handle<immix_heap>* handle = heap_handle<immix_heap>::for_tidy((tidy*) generic_subheap);
-  // Clang appears to assume handle is non-null; handle will be null if generic_subheap is
-  // the tidy pointer for the null heap cell.
-  immix_heap* subheap = (uintptr_t(generic_subheap) <= FOSTER_GC_DEFAULT_ALIGNMENT)
-                          ? gcglobals.default_allocator
-                          : handle->body;
-  //fprintf(gclog, "subheap_activate: subheap %p)\n", subheap); fflush(gclog);
-  heap_handle<immix_heap>* prev = gcglobals.allocator_handle;
-  //fprintf(gclog, "subheap_activate(generic %p, handle %p, subheap %p, prev %p)\n", generic_subheap, handle, subheap, prev);
+  immix_heap* subheap = ((heap_handle<immix_heap>*) generic_subheap)->body;
   gcglobals.allocator = subheap;
-  gcglobals.allocator_handle = handle;
-  //fprintf(gclog, "subheap_activate: prev %p (tidy %p))\n", prev, prev->as_tidy()); fflush(gclog);
-  return prev ? prev->as_tidy() : nullptr;
   //fprintf(gclog, "activated subheap %p\n", subheap);
 }
 
 void foster_subheap_collect_raw(void* generic_subheap) {
-  heap_handle<immix_heap>* handle = heap_handle<immix_heap>::for_tidy((tidy*) generic_subheap);
-  auto subheap = handle->body;
+  auto subheap = ((heap_handle<immix_heap>*) generic_subheap)->body;
   //fprintf(gclog, "collecting subheap %p\n", subheap);
   subheap->force_gc_for_debugging_purposes();
   //fprintf(gclog, "subheap-collect done\n");
 }
 
 void foster_subheap_condemn_raw(void* generic_subheap) {
-  heap_handle<immix_heap>* handle = heap_handle<immix_heap>::for_tidy((tidy*) generic_subheap);
-  auto subheap = handle->body;
+  auto subheap = ((heap_handle<immix_heap>*) generic_subheap)->body;
   fprintf(gclog, "condemning subheap %p\n", subheap);
   subheap->condemn();
   fprintf(gclog, "condemned subheap %p\n", subheap);
-  gcglobals.condemned_set.status = condemned_set_status::per_frame_condemned;
-  gcglobals.condemned_set.spaces.insert(subheap);
 }
 
-void foster_subheap_ignore_raw(void*) { return; }
 
 } // extern "C"
 
@@ -4190,19 +3433,13 @@
 } // namespace foster::runtime::gc
 
 uint8_t ctor_id_of(void* constructed) {
-  uintptr_t i = uintptr_t(constructed);
-  if (i < 64) {
-    return uint8_t(i);
-  }
-
   gc::heap_cell* cell = gc::heap_cell::for_tidy((gc::tidy*) constructed);
   const gc::typemap* map = tryGetTypemap(cell);
   gc_assert(map, "foster_ctor_id_of() was unable to get a usable typemap");
   int8_t ctorId = map->ctorId;
   if (ctorId < 0) {
-    fprintf(gc::gclog, "foster_ctor_id_of inspected bogus ctor id %d from cell %p in line %d of frame %u\n", ctorId, cell, line_offset_within_f15(cell), frame15_id_of(cell));
+    fprintf(gc::gclog, "foster_ctor_id_of inspected bogus ctor id %d\n", ctorId);
     gc::inspect_typemap(map);
-    exit(3);
   }
   return ctorId;
 }
diff --git a/runtime/gc/foster_gc_backtrace_x86-inl.h b/runtime/gc/foster_gc_backtrace_x86-inl.h
--- a/runtime/gc/foster_gc_backtrace_x86-inl.h
+++ b/runtime/gc/foster_gc_backtrace_x86-inl.h
@@ -39,7 +39,7 @@
 int foster_backtrace(frameinfo* frame, frameinfo* frames, size_t frames_sz) {
   int i = 0;
   while (frame && not_bogus(frame->retaddr) && frames_sz --> 0) {
-    if (GCLOG_DETAIL > 2) {
+    if (ENABLE_GCLOG) {
       fprintf(gclog, "...... frame: %p, frameptr: %p, retaddr: %p\n", frame, frame->frameptr, frame->retaddr);
       fflush(gclog);
     }
diff --git a/runtime/gc/foster_gc_stackmaps.cpp b/runtime/gc/foster_gc_stackmaps.cpp
--- a/runtime/gc/foster_gc_stackmaps.cpp
+++ b/runtime/gc/foster_gc_stackmaps.cpp
@@ -20,108 +20,14 @@
 // This symbol is emitted by the fostergc LLVM GC plugin to the
 // final generated assembly.
 extern "C" {
-  extern void* __LLVM_StackMaps;
+  extern stackmap_table foster__gcmaps;
 }
 
 static bool is_odd(int64_t x) { return 1 == (x % 2); }
 
-struct StkSizeRecord {
-  uint64_t fn_addr;
-  uint64_t stacksize;
-  uint64_t recordcount; // version 3
-};
-
-struct StackMapHeader {
-  uint8_t version;
-  uint8_t reserved_01;
-  uint16_t reserved_02;
-  uint32_t num_functions;
-  uint32_t num_constants;
-  uint32_t num_records;
-  StkSizeRecord records[0];
-  // constants
-  // StkMapRecords
-};
-
 // Stack map registration walks through the stack maps emitted
 // by the Foster LLVM GC plugin
-void register_stackmaps(std::map<void*, const StkMapRecord*>& stackMapRecords) {
-  StackMapHeader* smh = (StackMapHeader*) &__LLVM_StackMaps;
-  fprintf(gclog, "version: %d\n", smh->version);
-  fprintf(gclog, "num_functions: %d\n", smh->num_functions);
-  fprintf(gclog, "num_constants: %d\n", smh->num_constants);
-  fprintf(gclog, "num_records  : %d\n", smh->num_records);
-  StkSizeRecord* fn_records = (StkSizeRecord*) smh->records;
-  uint64_t* constants = (uint64_t*) &fn_records[smh->num_functions];
-
-  // We can't do "regular" array indexing because there are embedded
-  // indefinite-size arrays for StkMapLiveOuts.
-  StkMapRecord* curr_record = (StkMapRecord*) &constants[smh->num_constants];
-
-  for (auto i = 0; i < smh->num_functions; ++i) {
-    fprintf(gclog, "  fn %d has recordcount: %d\n", i, fn_records[i].recordcount);
-  }
-
-  if (smh->num_functions == 0) {
-    return;
-  }
-
-  int fnoffset = 0;
-  int fnrecords_left = fn_records[fnoffset].recordcount;
-  uintptr_t fnaddr = fn_records[fnoffset].fn_addr;
-
-  for (auto i = 0; i < smh->num_records; ++i) {
-    fprintf(gclog, "  record %d:\n", i);
-    fprintf(gclog, "       fn id: %d:\n", fnoffset);
-    //fprintf(gclog, "       patchpoint : %" PRIx64 "\n", curr_record->patchpoint_id);
-    fprintf(gclog, "       insn offset: %d\n", curr_record->insn_offset);
-
-    uintptr_t safepoint_addr = fnaddr + uint64_t(curr_record->insn_offset);
-
-    stackMapRecords[(void*) safepoint_addr] = curr_record;
-
-    fprintf(gclog, "       insn addr: %p\n", (void*) safepoint_addr);
-
-    if (--fnrecords_left == 0) {
-      ++fnoffset;
-      fnrecords_left = fn_records[fnoffset].recordcount;
-      fnaddr         = fn_records[fnoffset].fn_addr;
-    }
-
-    uint16_t numlocs = curr_record->numlocs;
-    fprintf(gclog, "       %d locs\n", numlocs);
-    // Skip first three entries, which are (I think) stack and frame pointers.
-    // We hacked in fake base pointers as copies of the "real" pointers,
-    // so just ignore those.
-    for (auto z = 0; z < numlocs; ++z) {
-      Location* loc = &(curr_record->locs[z]);
-    fprintf(gclog, "             loc %d: encoding %d, offset/const %d, regnum %x, size %d\n",
-      z, loc->encoding, loc->offset_or_smallconstant, loc->dwarf_regnum, loc->loc_size);
-/*
-      if (loc->encoding == 3) {
-        gc_assert(loc->dwarf_regnum == 7, "can't yet handle indirections off of non-stack-pointer registers");
-      } else {
-        gc_assert(false, "can't yet handle non-indirect stack map locations");
-      }
-      */
-    }
-
-    StkMapLiveOuts* louts = (StkMapLiveOuts*) &(curr_record->locs[numlocs]);
-    louts = roundUpToNearestMultipleWeak(louts, 8); // version 3
-
-    fprintf(gclog, "       # live outs: %d\n", louts->numLiveOuts);
-    //gc_assert(louts->numLiveOuts == 0, "can't yet handle stackmaps with live outs!");
-
-    curr_record = (StkMapRecord*) &(louts->liveouts[louts->numLiveOuts]);
-    curr_record = roundUpToNearestMultipleWeak(curr_record, 8);
-  }
-
-  if (smh->version != 3) {
-    fprintf(gclog, "Unknown stackmap version (%d)...\n", smh->version);
-    exit(1);
-  }
-
-#if 0
+void register_stackmaps(std::map<void*, const stackmap::PointCluster*>& clusterForAddress) {
   int32_t numStackMaps = foster__gcmaps.numStackMaps;
   //fprintf(gclog, "num stack maps: %d\n", numStackMaps); fflush(gclog);
 
@@ -189,7 +95,6 @@
       // fprintf(gclog, "\n");
     }
   }
-  #endif
   fprintf(gclog, "--------- gclog stackmap registration complete ----------\n");
   fflush(gclog);
 }
diff --git a/runtime/gc/foster_gc_utils.h b/runtime/gc/foster_gc_utils.h
--- a/runtime/gc/foster_gc_utils.h
+++ b/runtime/gc/foster_gc_utils.h
@@ -154,11 +154,6 @@
   uint8_t padding[16];
 
   heap_cell* as_cell() { return (heap_cell*) &header; }
-  tidy     * as_tidy() { return (tidy*)      &body; }
-
-  static heap_handle* for_tidy(tidy* ptr) {
-    return (heap_handle*) offset((void*)ptr, -(intptr_t(sizeof(void*) + HEAP_CELL_HEADER_SIZE)));
-  }
 };
 
 
@@ -220,43 +215,10 @@
   stackmap stackmaps[0];
 };
 
-
-struct Location {
-  uint8_t encoding;
-  uint8_t reserved_01;
-  uint16_t loc_size; // version 3
-  uint16_t dwarf_regnum;
-  uint16_t reserved_02; // version 3
-  int32_t offset_or_smallconstant;
-};
-
-struct StkMapRecord {
-  uint64_t patchpoint_id;
-  uint32_t insn_offset;
-  uint16_t reserved;
-  uint16_t numlocs;
-  Location locs[0];
-  // padding to 8-byte boundary
-  // StkMapLiveOuts
-};
-
-struct LiveOut {
-  uint16_t dwarf_regnum;
-  uint8_t  reserved;
-  uint8_t  size_bytes;
-};
-
-struct StkMapLiveOuts {
-  uint16_t padding;
-  uint16_t numLiveOuts;
-  LiveOut liveouts[0];
-};
-
 } } } // namespace foster::runtime::gc
 
 // This symbol is emitted by the fostergc LLVM GC plugin to the
 // final generated assembly.
 extern "C" {
   extern foster::runtime::gc::stackmap_table foster__gcmaps;
-  extern void* __LLVM_StackMaps;
 }
diff --git a/runtime/libfoster.cpp b/runtime/libfoster.cpp
--- a/runtime/libfoster.cpp
+++ b/runtime/libfoster.cpp
@@ -473,12 +473,12 @@
 
 void cstr_free(char* s) { free(s); }
 
-FILE* c2f_stdin() { return stdin; }
-FILE* c2f_stdout() { return stdout; }
-FILE* c2f_stderr() { return stderr; }
+FILE* c2f_stdin__autowrap() { return stdin; }
+FILE* c2f_stdout__autowrap() { return stdout; }
+FILE* c2f_stderr__autowrap() { return stderr; }
 
 
-int8_t foster_crypto_hash_sha256(foster_bytes* output, foster_bytes* input) {
+int8_t foster_crypto_hash_sha256__autowrap(foster_bytes* output, foster_bytes* input) {
   if (output->cap != crypto_hash_sha256_BYTES) {
     return 1;
   }
@@ -487,19 +487,6 @@
                             input->cap); // returns zero
 }
 
-double foster_strtof64(foster_bytes* b, int32_t roundmode) {
-  char* c = cstr(b);
-  double f = atof(c);
-  free(c);
-  return f;
-}
-
-void* foster_gdtoa64__autowrap(double f, int32_t mode, int32_t ndig, int32_t rounding, int32_t* decpt) {
-  char buf[64];
-  sprintf(buf, "%g", f);
-  return foster_emit_string_of_cstring(buf, strlen(buf));
-}
-
 void print_float_f32(float f) { return fprint_f32(stdout, f); }
 void expect_float_f32(float f) { return fprint_f32(stderr, f); }
 
@@ -607,7 +594,7 @@
   else                 { snprintf(ptr, 62, "%.3f %s%s", val, prefix, unit); }
 }
 
-void* foster_humanize_s(double val) {
+void* foster_humanize_s__autowrap(double val) {
   char buf[64] = { 0 };
   foster__humanize_s_ptr(val, &buf[0], "");
   return foster_emit_string_of_cstring(buf, strlen(buf));
@@ -622,10 +609,6 @@
   return double(later - early) / 1e6;
 }
 
-void foster_gc_safepoint_poll() {
-  //fprintf(stderr, "foster gc safepoint?\n");
-}
-
 // We want to perform aggressive link time optimization of
 // foster code + stdlib, without having runtime::initialize()
 // and ::cleanup() discarded. This function is hardcoded to be
@@ -643,10 +626,8 @@
 
   if (!tru) {
     // kung-fu grip to prevent LTO from being too mean.
-    //foster_coro_delete_self_reference((void*)&foster__gcmaps);
-    foster_coro_delete_self_reference((void*)&__LLVM_StackMaps);
+    foster_coro_delete_self_reference((void*)&foster__gcmaps);
     printf("%p\n", &foster_write_barrier_generic);
-    printf("%p\n", &foster_gc_safepoint_poll);
   }
 
   return foster::runtime::cleanup();
diff --git a/runtime/libfoster_coro.cpp b/runtime/libfoster_coro.cpp
--- a/runtime/libfoster_coro.cpp
+++ b/runtime/libfoster_coro.cpp
@@ -65,26 +65,20 @@
 }
 
 // corofn :: void* -> void
-void foster_coro_create(coro_func corofn, void* arg) {
-   // 100 MB stacks just in case; but note it's virtual addr space only
-  long ssize = 100*1024*1024*sizeof(void*);
-
+void foster_coro_create(coro_func corofn,
+                        void* arg) {
+  long ssize = 8*1024*sizeof(void*);
   // TODO allocate small stacks that grow on demand
   // (via reallocation or stack segment chaining).
   // TODO use mark-sweep GC for coro stacks.
-
-  coro_stack sinfo;
-  if (!coro_stack_alloc(&sinfo, ssize / sizeof(void*))) {
-    fprintf(stderr, "Coro stack allocation failed!\n"); fflush(stderr);
-    exit(2);
-  }
-  //fprintf(stderr, "allocating a coro stack of size %ld (0x%lx): %p\n", ssize, ssize, sinfo.sptr);
-  //memset(sinfo.sptr, 0xEE, ssize); // for debugging only
+  void* sptr = malloc(ssize);
+  memset(sptr, 0xEE, ssize); // for debugging only
   foster_bare_coro* coro = (foster_bare_coro*) arg;
   coro->indirect_self = (foster_bare_coro**) malloc(sizeof(coro));
   foster_coro_ensure_self_reference(coro);
   libcoro__coro_create(&coro->ctx, corofn, coro->indirect_self,
-                       sinfo.sptr, sinfo.ssze);
+                       sptr, ssize);
+  //printf("foster_coro_create(%p, %p)\n", corofn, arg);
 }
 
 // This is a no-op for the CORO_ASM backend,
diff --git a/runtime/libfoster_main.cpp b/runtime/libfoster_main.cpp
--- a/runtime/libfoster_main.cpp
+++ b/runtime/libfoster_main.cpp
@@ -28,7 +28,6 @@
 extern "C" int64_t opaquely_i64(int64_t n) { return n; }
 
 extern "C" {
-  void* foster__get_ctor_as_ptr(int8_t n) { return (void*) n; }
   void foster_pin_hook_memalloc_cell(uint64_t nbytes) { return; }
   void foster_pin_hook_memalloc_array(uint64_t nbytes) { return; }
 }
diff --git a/runtime/libfoster_posix.cpp b/runtime/libfoster_posix.cpp
--- a/runtime/libfoster_posix.cpp
+++ b/runtime/libfoster_posix.cpp
@@ -9,7 +9,7 @@
 #include <cstdio> // for fread(), etc.
 #include <cerrno>
 #include <climits> // for SSIZE_MAX
-#include <unistd.h> // for read(), write(), access()
+#include <unistd.h> // for read(), write()
 
 #ifdef OS_LINUX
 #include <fcntl.h> // for O_RDWR
@@ -22,8 +22,6 @@
 #include <fcntl.h> // for O_RDWR
 #endif
 
-#include <math.h> // for fpclassify
-
 // Definitions of functions which are meant to be exposed to Foster code
 // (at least for bootstrapping/utility purposes) rather than an implementation
 // of runtime services themselves.
@@ -158,15 +156,4 @@
   return int64_t(fd);
 }
 
-int32_t foster_posix_access__autowrap(foster_bytes* path, int32_t mode) {
-  return int32_t(access((const char*) &path->bytes[0], mode));
 }
-
-int32_t foster_posix_open3__autowrap(foster_bytes* path, int32_t flags, int32_t mode) {
-  return int32_t(open((const char*) &path->bytes[0], flags, mode));
-}
-
-
-int32_t foster_Float32_classify(float  f) { return int32_t(fpclassify(f)); }
-int32_t foster_Float64_classify(double f) { return int32_t(fpclassify(f)); }
-}
diff --git a/scripts/fosterc.py b/scripts/fosterc.py
--- a/scripts/fosterc.py
+++ b/scripts/fosterc.py
@@ -61,7 +61,7 @@
                                 '-lobjc'],
     'Linux': lambda: common + ['-lrt']
   }[platform.system()]()
-  return ' '.join(flags + ['-l%s' % f for f in options.nativelibs])
+  return ' '.join(flags)
 
 def rpath(path):
   import platform
@@ -98,7 +98,7 @@
   return [x for v in vals for x in [val, v]]
 
 def get_ghc_rts_args():
-  ghc_rts_args = ["-smeGCstats.txt", "-M2G", "-K64M"]
+  ghc_rts_args = ["-smeGCstats.txt", "-M2G", "-K640K"]
 
   if options and options.stacktraces:
     ghc_rts_args.append('-xc') # Produce stack traces
@@ -174,10 +174,6 @@
             print compilelog.read()
       raise StopAfterMiddle()
 
-    for arg in options.bitcodelibs:
-        options.beargs.append("-link-against")
-        options.beargs.append(arg)
-
     # running fosterlower on the Protobuf CFG produces an LLVM
     # bitcode Module; linking a bunch of Modules produces a Module
     e3 = crun(['fosterlower', check_output, '-o', finalname,
@@ -366,10 +362,6 @@
                     help="Add import path")
   parser.add_option("-o", dest="exepath", action="store", default=None,
                     help="Set path for output result (executable, etc)")
-  parser.add_option("--nativelib", dest="nativelibs", action="append", default=[],
-                    help="Add native library to link against")
-  parser.add_option("--bitcode", dest="bitcodelibs", action="append", default=[],
-                    help="Add LLVM bitcode object to link against")
   return parser
 
 def fosterc_set_options(opts):
diff --git a/scripts/install-llvm.sh b/scripts/install-llvm.sh
--- a/scripts/install-llvm.sh
+++ b/scripts/install-llvm.sh
@@ -8,18 +8,6 @@
 LLVM_V=${LLVM_VERSION}.src
 LLVM_ROOT=${HOME}/llvm
 
-# $0 is either a path to the script,
-# implying `which` will return an empty string,
-# or it's a path, in which case we should use `which`.
-if [ "x$(which $0)" == "x" ]; then
-SD=$(dirname $0)/../scripts
-else
-SD=$(dirname `which $0`)/../scripts
-fi
-
-FOSTER_ROOT=$($SD/normpath.py $SD/..)
-MAKEJ=$(${FOSTER_ROOT}/third_party/vstinner_perf/makej)
-
 # invoke from LLVM_ROOT
 checkout_source() {
 pushd src
@@ -66,7 +54,7 @@
         #CC=clang CXX=clang++
         cmake .. -G "Unix Makefiles" -DCMAKE_INSTALL_PREFIX=${LLVM_ROOT}/${LLVM_VERSION} -DCMAKE_BUILD_TYPE="RelWithDebInfo" -DLLVM_TARGETS_TO_BUILD="host" -DLLVM_ENABLE_ASSERTIONS=ON -DLLVM_LINK_LLVM_DYLIB=ON
 
-	time make -j${MAKEJ}
+	time make -j
 
 	make install
 popd
diff --git a/scripts/ubuntu-17.10.sh b/scripts/ubuntu-17.10.sh
--- a/scripts/ubuntu-17.10.sh
+++ b/scripts/ubuntu-17.10.sh
@@ -1,8 +1,5 @@
 #!/bin/sh
 
-set -e
-set -x
-
 sudo apt-get install --yes build-essential g++ g++-multilib git gnuplot \
                        python-pygments python-matplotlib  python-scipy python-sphinx \
                        python-pandas python-pip python-numpy python-qt4 \
diff --git a/scripts/ubuntu-18.04.sh b/scripts/ubuntu-18.04.sh
deleted file mode 100644
--- a/scripts/ubuntu-18.04.sh
+++ /dev/null
@@ -1,124 +0,0 @@
-#!/bin/sh
-
-set -e
-set -x
-
-sudo apt-get install --yes build-essential g++ g++-multilib git gnuplot \
-                       python3-pygments python3-matplotlib  python3-scipy python3-sphinx \
-                       python3-pandas python3-pip python3-numpy python3-pyqt4 \
-                       z3 libdw-dev
-sudo apt-get install --yes mercurial vim libsparsehash-dev \
-              curl exuberant-ctags aptitude libcairo2-dev libc6-dev default-jdk
-sudo apt-get install --yes m4 ministat meld \
-         linux-tools-virtual linux-tools-generic \
-         libffi-dev libedit-dev cmake cmake-curses-gui
-
-    sudo apt-get install --yes libncurses5-dev
-
-  # GMP is needed for GHC to build Cabal from source.
-  sudo apt-get install --yes libgmp3-dev libgmp-dev
-
-
-
-  # TortoiseHG from source, since the PPA packages have been taken down.
-  hg clone https://bitbucket.org/tortoisehg/thg ~/.local/tortoisehg
-  # Assuming ~/.local/bin is on $PATH, which it was in Ubuntu MATE 16.04
-  mkdir -p ~/.local/bin
-  ln -s ~/.local/tortoisehg/thg ~/.local/bin/thg
-
-curl https://beyondgrep.com/ack-2.20-single-file > ~/.local/bin/ack && chmod 0755 ~/.local/bin/ack
-
-
-  # Python packages, mostly used by benchmarking infrastructure
-  pip install pyyaml jinja2 statsmodels mpld3 seaborn
-
-  mkdir tmp
-  cd tmp
-  wget http://thrysoee.dk/editline/libedit-20170329-3.1.tar.gz
-  tar xf libedit*.gz
-  rm libedit*.gz
-  cd libedit*
-  ./configure --prefix=$HOME/.local && make -j && make install
-  cd ..
-
-
-  wget http://prdownloads.sourceforge.net/swig/swig-3.0.12.tar.gz
-  tar xf swig-*.gz
-  rm swig-*.gz
-  cd swig-*
-  ./configure --prefix=$HOME/.local && make -j && make install
-  cd ..
-
-  wget https://downloads.haskell.org/~ghc/8.4.3/ghc-8.4.3-x86_64-deb8-linux-dwarf.tar.xz
-  tar xf ghc-*.xz
-  rm ghc-*.xz
-  cd ghc-* && ./configure --prefix=$HOME/.local/ghc-8.4.3 && make -j install && cd ..
-
-  wget https://www.haskell.org/cabal/release/cabal-install-2.2.0.0/cabal-install-2.2.0.0-x86_64-unknown-linux.tar.gz
-  tar xf cabal-*.gz
-  rm cabal-*.gz
-  mv cabal ~/.local/bin
-
-  curl -O https://capnproto.org/capnproto-c++-0.6.1.tar.gz
-  tar xf capnproto-*.gz
-  rm     capnproto-*.gz
-  cd capnproto-c++-0.6.1
-  ./configure --prefix=$HOME/.local/capnp-c++-0.6.1
-  make -j check
-  make install
-  cd ..
-
-
-
-  cd ..
-  rm -rf tmp/
-
-
-	ANTLR_VERSION=3.4
-	ANTLR_DIR=~/antlr/${ANTLR_VERSION}
-	mkdir tmp
-	cd tmp
-		wget http://antlr3.org/download/C/libantlr3c-${ANTLR_VERSION}.tar.gz
-		tar xzvf libantlr3c-${ANTLR_VERSION}.tar.gz
-                cd libantlr3c-${ANTLR_VERSION}
-		./configure --prefix=${ANTLR_DIR} --enable-64bit && make -j && make install
-                cd ..
-	cd ..
-	rm -rf ./tmp
-	pushd ${ANTLR_DIR}
-	wget http://antlr3.org/download/antlr-${ANTLR_VERSION}-complete.jar
-        mv antlr-${ANTLR_VERSION}-complete.jar antlr-${ANTLR_VERSION}.jar
-	popd
-
-
-export PATH=$PATH:$HOME/.local/bin:$HOME/.local/capnp-c++-0.6.1/bin:$HOME/.local/ghc-8.4.2/bin
-cabal update
-
-cat ~/.cabal/config | sed 's/-- library-profiling:/library-profiling: True/' > tmp.txt
-mv tmp.txt ~/.cabal/config
-
-
-hg clone https://bitbucket.org/b/sw ~/sw
-hg clone https://bitbucket.org/b/foster ~/foster
-hg clone https://bitbucket.org/b/minuproto ~/sw/local/minuproto
-
-cd ~/foster/compiler/me
-cabal sandbox init
-cabal update
-cabal sandbox add-source ~/sw/local/minuproto
-cabal install --only-dependencies
-
-echo Okay, starting to install LLVM...
-. ~/foster/scripts/install-llvm.sh
-
-
-pushd ~/foster/third_party/nacl-20110221
-echo "Building NaCl, this will take several minutes."
-./do
-popd
-
-echo Time to build Foster itself!
-cd ~/foster
-mkdir _obj
-cd _obj
-cmake .. && make
diff --git a/stdlib/bytes/bytes.foster b/stdlib/bytes/bytes.foster
--- a/stdlib/bytes/bytes.foster
+++ b/stdlib/bytes/bytes.foster
@@ -134,9 +134,6 @@
 bytesTake32 :: { Bytes => Int32 => Bytes };
 bytesTake32 = { ba => n => bytesTake ba (zext_i32_to_i64 n) };
 
-bytesSlice32 :: { Bytes => Int32 => Int32 => Bytes };
-bytesSlice32 = { ba => off => len => bytesTake32 (bytesDrop32 ba off) len };
-
 bytesDropWord :: { Bytes => Word => Bytes };
 bytesDropWord = { ba => n => bytesDrop ba (zext_Word_to_i64 n) };
 
@@ -280,7 +277,6 @@
   rv
 };
 
-
 bytesFlatten :: { Bytes => Bytes };
 bytesFlatten = { bytes => bytes |> bytesFlattenRaw |> bytesOfRawArray };
 
@@ -312,10 +308,6 @@
   end
 };
 
-// Precondition: o is a valid index for b.
-bytesGet32!    :: { Int32 => Bytes => Int8 };
-bytesGet32! = { o => ba => bytesGet! (zext_i32_to_i64 o) ba };
-
 // Precondition: 0 is a valid index for b.
 bytesGet0! :: { Bytes => Int8 };
 bytesGet0! = { ba =>
@@ -497,51 +489,6 @@
   go 0
 };
 
-// Return true if b1 starts with b2.
-bytesHasPrefix :: { Bytes => Bytes => Bool };
-bytesHasPrefix = { b1 => b2 =>
-  bytesEqual (bytesTake b1 (bytesLength b2)) b2
-};
-
-
-bytesCmpRaw :: { Bytes => Array Int8 => Ord };
-bytesCmpRaw = { b1 => a2 =>
-  defaultVerdict = cmp-UInt64 (bytesLength b1) (arrayLength a2);
-  len            = min-UInt64 (bytesLength b1) (arrayLength a2);
-  REC go = { idx =>
-    if idx ==Int64 len
-      then defaultVerdict
-      else res = cmp-UInt8 (bytesGet! idx b1) a2[idx];
-           if isEQ res then go (idx +Int64 1) else res end
-    end
-  };
-  go 0
-};
-
-bytesEqualRaw :: { Bytes => Array Int8 => Bool };
-bytesEqualRaw = { b1 => a2 => isEQ (bytesCmpRaw b1 a2) };
-
-bytesSliceFromTo :: { Bytes => Int64 => Int64 => Bytes };
-bytesSliceFromTo = { b => src => tgt =>
-  len = tgt -Int64 src;
-  bytesTake (bytesDrop b src) len
-};
-
-bytesIndexFrom :: { Bytes => Int64 => Int8 => Int64 };
-bytesIndexFrom = { b => idx => c =>
-  len = bytesLength b;
-  REC loop = { idx =>
-    if idx ==Int64 len
-      then idx
-      else if bytesGet! idx b ==Int8 c
-             then idx
-             else loop (idx +Int64 1)
-            end
-    end
-  };
-  loop idx
-};
-
 /*
 bytesDescribe :: { Bytes => Text };
 bytesDescribe = { bytes =>
@@ -566,127 +513,3 @@
 };
 */
 
-
-// BytesChunk is a simplified form of Bytes which does not support efficient
-// concatenation. In exchange, other operations (especially indexing) are
-// more efficient.
-type case BytesChunk
-  of $BytesChunk (Array Int8) Int64 Int64 // offset, fragment length
-  ;
-
-bytesChunk :: { Array Int8 => BytesChunk };
-bytesChunk = { arr => BytesChunk arr 0 (arrayLength arr) };
-
-bytesChunkTake :: { BytesChunk => Int64 => BytesChunk };
-bytesChunkTake = { b => i =>
-  case b of $BytesChunk arr off len ->
-    BytesChunk arr off (min-SInt64 i len)
-  end
-};
-
-bytesChunkDrop :: { BytesChunk => Int64 => BytesChunk };
-bytesChunkDrop = { b => i =>
-  case b of $BytesChunk arr off len ->
-    n = (min-SInt64 i len);
-    BytesChunk arr (off +Int64 n) (len -Int64 n)
-  end
-};
-
-bytesChunkGet! :: { BytesChunk => Int64 => Int8 };
-bytesChunkGet! = { b => i =>
-  case b of $BytesChunk arr off _ ->
-    arr[off +Int64 i]
-  end
-};
-
-bytesChunkLength :: { BytesChunk => Int64 };
-bytesChunkLength = { b =>
-  case b of $BytesChunk _ _ len -> len end
-};
-
-bytesChunkHasPrefix :: { BytesChunk => BytesChunk => Bool };
-bytesChunkHasPrefix = { b1 => b2 =>
-  bytesChunkEqual (bytesChunkTake b1 (bytesChunkLength b2)) b2
-};
-
-bytesChunkCmp :: { BytesChunk => BytesChunk => Ord };
-bytesChunkCmp = { b1 => b2 =>
-  defaultVerdict = cmp-UInt64 (bytesChunkLength b1) (bytesChunkLength b2);
-  len            = min-UInt64 (bytesChunkLength b1) (bytesChunkLength b2);
-  REC go = { idx =>
-    if idx ==Int64 len
-      then defaultVerdict
-      else res = cmp-UInt8 (bytesChunkGet! b1 idx) (bytesChunkGet! b2 idx);
-           if isEQ res then go (idx +Int64 1) else res end
-    end
-  };
-  go 0
-};
-
-bytesChunkCmpRaw :: { BytesChunk => Array Int8 => Ord };
-bytesChunkCmpRaw = { b1 => a2 =>
-  defaultVerdict = cmp-UInt64 (bytesChunkLength b1) (arrayLength a2);
-  len            = min-UInt64 (bytesChunkLength b1) (arrayLength a2);
-  REC go = { idx =>
-    if idx ==Int64 len
-      then defaultVerdict
-      else res = cmp-UInt8 (bytesChunkGet! b1 idx) a2[idx];
-           if isEQ res then go (idx +Int64 1) else res end
-    end
-  };
-  go 0
-};
-
-bytesChunkEmpty = { bytesChunk b"" };
-
-bytesChunkEqual :: { BytesChunk => BytesChunk => Bool };
-bytesChunkEqual = { b1 => b2 =>
-  if bytesChunkLength b1 ==Int64 bytesChunkLength b2
-    then isEQ (bytesChunkCmp b1 b2)
-    else False
-  end
-};
-
-bytesChunkEqualRaw :: { BytesChunk => Array Int8 => Bool };
-bytesChunkEqualRaw = { b => a =>
-  if bytesChunkLength b ==Int64 arrayLength a
-    then isEQ (bytesChunkCmpRaw b a)
-    else False
-  end
-};
-
-bytesChunkFromTo :: { BytesChunk => Int64 => Int64 => BytesChunk };
-bytesChunkFromTo = { b => src => tgt =>
-  len = tgt -Int64 src;
-  bytesChunkTake (bytesChunkDrop b src) len
-};
-
-bytesChunkIndexFrom :: { BytesChunk => Int64 => Int8 => Int64 };
-bytesChunkIndexFrom = { b => idx => c =>
-  len = bytesChunkLength b;
-  REC loop = { idx =>
-    if idx ==Int64 len
-      then idx
-      else if bytesChunkGet! b idx ==Int8 c
-             then idx
-             else loop (idx +Int64 1)
-            end
-    end
-  };
-  loop idx
-};
-
-bytesChunkOfBytes :: { Bytes => BytesChunk };
-bytesChunkOfBytes = { b =>
-  case b
-    of $BytesOffset ($BytesFragment arr _) offset len -> BytesChunk arr offset len
-    of _ -> b |> bytesFlattenRaw |> bytesChunk
-  end
-};
-
-bytesOfBytesChunk :: { BytesChunk => Bytes };
-bytesOfBytesChunk = { b =>
-  case b of $BytesChunk arr offset len ->
-    BytesOffset (BytesFragment arr (offset +Int64 len)) offset len
-  end
-};
diff --git a/stdlib/hash/sha256/sha256.foster b/stdlib/hash/sha256/sha256.foster
--- a/stdlib/hash/sha256/sha256.foster
+++ b/stdlib/hash/sha256/sha256.foster
@@ -3,26 +3,8 @@
 
 // based on http://pastebin.com/raw.php?i=ZmkCyJmU
 
-// Should be automatically lifted, but appears not to be (yet);
-// not lifting produces a 50% performance degradation.
-ror = { x => n => bitor-Int32 (x `bitlshr-Int32` n) (x `bitshl-Int32` (32 -Int32 n)) };
-
-// Lifting these reduces runtime by ~30%
-s2 = { x => x `bitlshr-Int32` 2 };
-sv = { x => 3 -Int32 (x `bitand-Int32` 3) };
-
-abyte = { val : Int8 => a : Array Int32 => q : Ref Int32 => w => h =>
-  bitpos = sv q^ `bitshl-Int32` 3;
-  q2 = s2 q^;
-  v = a[q2];
-  mask1 = bitnot-Int32 (0xff `bitshl-Int32` bitpos);
-  mask2 = (zext_i8_to_i32 val) `bitshl-Int32` bitpos;
-  ((v `bitand-Int32` mask1) `bitor-Int32` mask2) >^ a[q2];
-  if q^ ==Int32 63
-    then block w a q h; 0 >^ q;
-    else (q^ +Int32 1) >^ q;
-  end;
-};
+sha256aa :: { Array Int8 => Array Int8 };
+sha256aa = { inp =>
 
   cube = prim mach-array-literal
       0x428a2f98 0x71374491 0xb5c0fbcf 0xe9b5dba5 0x3956c25b 0x59f111f1 0x923f82a4 0xab1c5ed5
@@ -36,41 +18,16 @@
       ;
   sqr = prim mach-array-literal 0x6a09e667 0xbb67ae85 0x3c6ef372 0xa54ff53a
                                 0x510e527f 0x9b05688c 0x1f83d9ab 0x5be0cd19;
-
-  block = { w => a => q => h =>
-    enumRange32 0 16 { n => a[n] >^ w[n] };
-    enumRange32 16 64 { n =>
-      aa = ((ror w[n -Int32 15]  7) `bitxor-Int32` (ror w[n -Int32 15] 18)) `bitxor-Int32` (w[n -Int32 15] `bitlshr-Int32`  3);
-      bb = ((ror w[n -Int32  2] 17) `bitxor-Int32` (ror w[n -Int32  2] 19)) `bitxor-Int32` (w[n -Int32  2] `bitlshr-Int32` 10);
-      (w[n -Int32 16] +Int32 w[n -Int32 7] +Int32 aa +Int32 bb) >^ w[n];
-    };
-
-    t = newDArray0 8 { i => h[i] };
-    enumRange32 0 64 { n =>
-      xa = (ror t[0] 2) `bitxor-Int32` ((ror t[0] 13) `bitxor-Int32` (ror t[0] 22));
-      xb = (ror t[4] 6) `bitxor-Int32` ((ror t[4] 11) `bitxor-Int32` (ror t[4] 25));
-      xc = (t[0] `bitand-Int32` t[1]) `bitxor-Int32` ((t[0] `bitand-Int32` t[2]) `bitxor-Int32` (t[1] `bitand-Int32` t[2]));
-      xd = (t[4] `bitand-Int32` t[5]) `bitxor-Int32` ((bitnot-Int32 t[4]) `bitand-Int32` t[6]);
-      xe = t[7] +Int32 w[n] +Int32 cube[n] +Int32 xb +Int32 xd;
-      t[6] >^ t[7]; t[5] >^ t[6]; t[4] >^ t[5]; (t[3] +Int32 xe) >^ t[4];
-      t[2] >^ t[3]; t[1] >^ t[2]; t[0] >^ t[1]; (xa +Int32 xc +Int32 xe) >^ t[0];
-    };
-    enumRange32 0 8 { n => (t[n] +Int32 h[n]) >^ h[n] };
-  };
-
-
-sha256aa :: { Array Int8 => Array Int8 };
-sha256aa = { inp =>
-
   w = newDArray0 64 { i => 0 };
   a = newDArray0 16 { i => 0 };
   h = newDArray0 8  { i => sqr[i] };
 
+  ror = { x => n => bitor-Int32 (x `bitlshr-Int32` n) (x `bitshl-Int32` (32 -Int32 n)) };
+
   ////////////////////////////////////////
 
   q = prim ref 0;
 
-/*
   block = {
     enumRange32 0 16 { n => a[n] >^ w[n] };
     enumRange32 16 64 { n =>
@@ -92,6 +49,9 @@
     enumRange32 0 8 { n => (t[n] +Int32 h[n]) >^ h[n] };
   };
 
+  s2 = { x => x `bitlshr-Int32` 2 };
+  sv = { x => 3 -Int32 (x `bitand-Int32` 3) };
+
   byte = { val : Int8 =>
     bitpos = sv q^ `bitshl-Int32` 3;
     q2 = s2 q^;
@@ -104,21 +64,20 @@
       else (q^ +Int32 1) >^ q;
     end;
   };
-  */
 
   /////////////////////////////////
 
-  arrayEnum inp { b => i => abyte b a q w h };
+  arrayEnum inp { b => i => byte b };
 
   extract-byte-Int64-offset = { x => n =>
     ((x `bitshl-Int64` 3) `bitlshr-Int64` (n `bitshl-Int64` 3)) |> trunc_i64_to_i8
   };
 
   // finish
-  abyte 0x80 a q w h;
-  while { q^ !=Int32 56 } { abyte 0 a q w h };
+  byte 0x80;
+  while { q^ !=Int32 56 } { byte 0 };
   enumRange64 0 8 { n =>
-    v = extract-byte-Int64-offset (arrayLength inp) (7 -Int64 n); abyte v a q w h;
+    extract-byte-Int64-offset (arrayLength inp) (7 -Int64 n) |> byte;
   };
 
   newDArray0 32 { n => extract-byte-Int32 h[s2 n] (sv n) };
diff --git a/stdlib/io/posix/posix.foster b/stdlib/io/posix/posix.foster
--- a/stdlib/io/posix/posix.foster
+++ b/stdlib/io/posix/posix.foster
@@ -69,10 +69,10 @@
   end
 };
 
-write_bytes_chunk_to_fd_from :: { Int64 => Array Int8 => Int64 => Int64 => Int64 };
-write_bytes_chunk_to_fd_from = { fd => bytes => offset => size =>
+write_bytes_chunk_to_fd :: { Int64 => Array Int8 => Int64 };
+write_bytes_chunk_to_fd = { fd => bytes =>
   wstatus = (prim ref 0);
-  nwrote = foster_posix_write_bytes fd bytes offset size  wstatus;
+  nwrote = foster_posix_write_bytes fd bytes 0 (arrayLength bytes) wstatus;
   case wstatus^
     of 0 -> nwrote
     of 1 -> prim kill-entire-process "write_bytes_chunk_to_fd returned MORE";
@@ -81,11 +81,6 @@
   end
 };
 
-write_bytes_chunk_to_fd :: { Int64 => Array Int8 => Int64 };
-write_bytes_chunk_to_fd = { fd => bytes =>
-  write_bytes_chunk_to_fd_from fd bytes 0 (arrayLength bytes);
-};
-
 write_bytes_to_fd :: { Int64 => Bytes => Int64 };
 write_bytes_to_fd = { fd => bytes =>
   wstatus = (prim ref 0);
diff --git a/stdlib/pairing-heap/pairing-heap.foster b/stdlib/pairing-heap/pairing-heap.foster
--- a/stdlib/pairing-heap/pairing-heap.foster
+++ b/stdlib/pairing-heap/pairing-heap.foster
@@ -90,14 +90,6 @@
   end
 };
 
-pairingHeapFindAndDeleteMin :: forall (a:Type) { { a => a => Ord } => PairingHeap a => Maybe (a, PairingHeap a) };
-pairingHeapFindAndDeleteMin = { forall a:Type, cmp => h =>
-  case h
-    of $PairingHeapE -> None
-    of $PairingHeapT e l -> Some (e, pairingHeapMergePairs cmp l)
-  end
-};
-
 pairingHeapToSortedList :: forall (a:Type) { { a => a => Ord } => PairingHeap a => List a };
 pairingHeapToSortedList = { forall a:Type, cmp => h =>
   case h
diff --git a/stdlib/prng/xorshift/xorshift.foster b/stdlib/prng/xorshift/xorshift.foster
--- a/stdlib/prng/xorshift/xorshift.foster
+++ b/stdlib/prng/xorshift/xorshift.foster
@@ -45,16 +45,6 @@
   end
 };
 
-xor1x32HOF :: { Int32 => { Int32 } };
-xor1x32HOF = { x =>
-  r = (prim ref (xor1x32MakeState x));
-  {
-    let (v, s) = xor1x32gen r^;
-    s >^ r;
-    v
-  }
-}; 
-
 //////////
 
 type case Xor1x64State
diff --git a/test/bootstrap/arrays-static/arrays-static.foster b/test/bootstrap/arrays-static/arrays-static.foster
--- a/test/bootstrap/arrays-static/arrays-static.foster
+++ b/test/bootstrap/arrays-static/arrays-static.foster
@@ -1,12 +1,6 @@
 main = {
     barBARARBARBAARA 42;
     fooBARBAZ___QUUUX !;
-    part3 !;
-    part4 !;
-    part5 !;
-    noinline_llvm_part6 !;
-    case noinline_llvm_None ! of $None -> ()
-                              of $Some _ _ -> () end;
 };
 
 fooBARBAZ___QUUUX = {
@@ -18,71 +12,3 @@
 };
 
 aAAAARRRRRRAAAYYYYYY = prim mach-array-literal 42 43 44;
-
-
-sa :: Array Int8;
-sa = b" ";
-
-ascii_space :: Int8;
-ascii_space = 32;
-
-part3 = {
-  expect_i8 ascii_space;
-  print_i8 sa[0];
-};
-
-pair = (20, ascii_space);
-part4 = {
-  let (a, b) = pair;
-  expect_i8 52;
-  print_i8 (a +Int8 b);
-};
-
-type case List of $None of $Some Int32 List;
-nil = None;
-one = Some 1 nil;
-
-two :: List;
-two = (Some 2 one);
-
-alsotwo = two;
-
-sum = { list =>
-  case list of $None -> 0
-            of $Some n tail -> n +Int32 sum tail
-  end
-};
-
-part5 = {
-  expect_i32 3;
-  print_i32 (sum alsotwo);
-  expect_i32 3;
-  print_i32 (sum two);
-  expect_i32 3;
-  print_i32 (sum alsotwo);
-  expect_i32 3;
-  print_i32 (sum two);
-};
-
-noinline_llvm_None = {
-  None
-};
-
-
-noinline_llvm_part6 = {
-  REC forcecont_foo = { n : Int32 =>
-    if n ==Int32 0 then 0
-      else print_i32 (opaquely_i32 n); forcecont_bar (n -Int32 1);
-    end
-  };
-  REC forcecont_bar = { n : Int32 =>
-    if n ==Int32 0 then 0
-      else print_i32 (opaquely_i32 n); forcecont_foo (n -Int32 1);
-    end
-  };
-
-  expect_i32 3;
-  expect_i32 2;
-  expect_i32 1;
-  forcecont_foo 3;
-};
diff --git a/test/bootstrap/basic/fixnum-primitives/fixnum-primitives.foster b/test/bootstrap/basic/fixnum-primitives/fixnum-primitives.foster
--- a/test/bootstrap/basic/fixnum-primitives/fixnum-primitives.foster
+++ b/test/bootstrap/basic/fixnum-primitives/fixnum-primitives.foster
@@ -28,26 +28,6 @@
 
   expect_i1 True;
   print_i1 (0 <UInt64 n1);
-
-  // Literals (of sized integer types) are just syntax for bit patterns.
-  expect_i8b 0xff;
-  print_i8b 255;
-
-  expect_i8b 0b`1111`1111;
-  print_i8b -1;
-
-  expect_i1 True;
-  print_i1 (255 ==Int8 -1);
-
-  expect_i1 False;
-  print_i1 (255 ==Int32 -1);
-
-  expect_i32b 0x0000`00ff;
-  print_i32b 255;
-
-  expect_i32b 0xffff`ffff;
-  print_i32b -1;
-
 };
 
 sub8 = { n : Int8 => n -Int8 (trunc32to8 2); };
diff --git a/test/bootstrap/basic/syntax-num/syntax-num.foster b/test/bootstrap/basic/syntax-num/syntax-num.foster
--- a/test/bootstrap/basic/syntax-num/syntax-num.foster
+++ b/test/bootstrap/basic/syntax-num/syntax-num.foster
@@ -19,41 +19,5 @@
 
   l = 0xFEEDFACE;
   m = 0xFEED`FACE;
-
-  expect_i1 True;
-  print_i1 (l ==Int32 m);
-
-  expect_i1 True;
-  print_i1 (0x1.8p1 ==f64 3.0);
-
-  expect_i64x 0x40ad380000000000;
-  0x3.a7p10  |> f64-as-i64 |> print_i64x;
-  expect_i64x 0x3fd0000000000000;
-  0x1.0p-2   |> f64-as-i64 |> print_i64x;
-  expect_i64x 0x3810000000000000;
-  0x1.0p-126 |> f64-as-i64 |> print_i64x;
-
-  expect_i64x 0x41f0000000000000;
-  0x1.0p32 |> f64-as-i64 |> print_i64x;
-  expect_i64x 0x4200000000000000;
-  0x2.0p32 |> f64-as-i64 |> print_i64x;
-
-
-  expect_i64x 0x5fefffffffffffff;
-  0x1.fffffffffffffp511  |> f64-as-i64 |> print_i64x;
-
-  expect_i64x 0x7fcfffffffffffff;
-  0x1.fffffffffffffp1021  |> f64-as-i64 |> print_i64x;
-  expect_i64x 0x7fdfffffffffffff;
-  0x1.fffffffffffffp1022  |> f64-as-i64 |> print_i64x;
-  expect_i64x 0x7fefffffffffffff;
-  0x1.fffffffffffffp1023  |> f64-as-i64 |> print_i64x;
-  expect_i64x 0x0000000000000001;
-  0x1.0P-1074             |> f64-as-i64 |> print_i64x;
-  expect_i64x 0x0000000000000001;
-  0x0.0000000000001P-1022 |> f64-as-i64 |> print_i64x;
-
-  expect_i1 False;
-  print_i1 (__COMPILES__ 0x1.0p1e);
   ()
 };
diff --git a/test/bootstrap/closureconversion/globalization-01/globalization-01.foster b/test/bootstrap/closureconversion/globalization-01/globalization-01.foster
deleted file mode 100644
--- a/test/bootstrap/closureconversion/globalization-01/globalization-01.foster
+++ /dev/null
@@ -1,18 +0,0 @@
-main = {
-  localized !;
-  globalized !;
-};
-
-localized = {
-  n = 3;
-  expect_i32 n;
-  noinline_llvm_foo { print_i32 n };
-};
-
-g = 3;
-globalized = {
-  expect_i32 g;
-  noinline_llvm_foo { print_i32 g };
-};
-
-noinline_llvm_foo = { f => f ! };
diff --git a/test/bootstrap/datatypes/ctor-elimination/ctor-elimination.foster b/test/bootstrap/datatypes/ctor-elimination/ctor-elimination.foster
--- a/test/bootstrap/datatypes/ctor-elimination/ctor-elimination.foster
+++ b/test/bootstrap/datatypes/ctor-elimination/ctor-elimination.foster
@@ -27,38 +27,6 @@
   end
 };
 
-f3PlusSingleton = { f3a => f3b =>
-  case (f3a, f3b) of ($F3 xa ya za, $F3 xb yb zb) ->
-    F3 (xa +f32 xb) (ya +f32 yb) (za +f32 zb)
-  end
-};
-
-// The F3 constructors will be eliminated, but only if f3Plus is inlined.
-noinline_mustnotalloc_lenplus = {
-  expect_float_f32 12;
-  print_float_f32 (case (f3PlusSingleton (F3 1 2 3) (F3 1 2 3)) of $F3 a b c -> a +f32 b +f32 c end);
-};
-
-type case I2
-  of $I2 Int32 Int32
-  ;
-
-noinline_mustnotalloc_example2 = { a =>
-  case (a, a)
-    of (1, 2) -> 0
-    of (2, 2) -> 2
-    of (_, _) -> 3
-  end
-};
-
-noinline_mustnotallo_example3 = { a =>
-  case (I2 a a)
-    of ($I2 1 2) -> 0
-    of ($I2 2 2) -> 2
-    of ($I2 _ _) -> 3
-  end
-};
-
 main = {
   expect_float_f32 56;
   print_float_f32 (f3SqrLen (F3 2 4 6));
@@ -66,17 +34,6 @@
   expect_float_f32 56;
   print_float_f32 (f3SqrLen (f3Plus (F3 1 2 3) (F3 1 2 3)));
 
-  expect_float_f32 56;
-  print_float_f32 (f3SqrLen (f3Plus (F3 2 1 3) (F3 2 1 3)));
-
-  noinline_mustnotalloc_lenplus !;
-
-  expect_i32 2;
-  noinline_mustnotalloc_example2 2 |> print_i32;
-
-  expect_i32 2;
-  noinline_mustnotallo_example3 2 |> print_i32;
-
   expect_float_f32 100000002004087734272.000000;
   print_float_f32 1.e20;
 
diff --git a/test/bootstrap/inference/inferred-lambdas/inferred-lambdas.foster b/test/bootstrap/inference/inferred-lambdas/inferred-lambdas.foster
--- a/test/bootstrap/inference/inferred-lambdas/inferred-lambdas.foster
+++ b/test/bootstrap/inference/inferred-lambdas/inferred-lambdas.foster
@@ -4,14 +4,12 @@
   expect_i32 5;
   print_i32 (f 3);
 
-/*
   // Each binding (such as x) must be assigned a unique type,
   // either implicitly or explicitly. Here, x's type is underconstrained;
   // unlike ML and Haskell, we treat this as an error, rather than
   // triggering automatic generalization of its type.
   expect_i1 False;
   print_i1 (__COMPILES__ { x => x } );
-*/
 
   expect_i1 True;
   print_i1 (__COMPILES__ { x : () => x } );
diff --git a/test/bootstrap/refinements/refinements-basic-1/refinements-basic-1.foster b/test/bootstrap/refinements/refinements-basic-1/refinements-basic-1.foster
--- a/test/bootstrap/refinements/refinements-basic-1/refinements-basic-1.foster
+++ b/test/bootstrap/refinements/refinements-basic-1/refinements-basic-1.foster
@@ -2,16 +2,9 @@
   test-assert-bool-lits !;
   test-assert-bool-unrefined True False;
   //test-assert-bool-refined True False;
-
-  print_i32 2;
-  expect_i32 2;
-
   test-assert-bool-refined-alt !;
   test-more-bools !;
 
-  print_i32 4;
-  expect_i32 4;
-
   test-refined-array-literals !;
   test-refined-array-nonlits !;
 
@@ -79,10 +72,8 @@
 
 test-refined-array-literals = {
   // Unconstrained type; should it be Int32 or Int64 or ... ?
-  /*
   expect_i1 False;
   print_i1 (__COMPILES__ prim mach-array-literal 1 2 3);
-  */
 
   expect_i1 True;
   print_i1 (__COMPILES__ prim mach-array-literal :[Int32] 1 2 3);
diff --git a/test/bootstrap/testcases/gc-root-02/gc-root-02.foster b/test/bootstrap/testcases/gc-root-02/gc-root-02.foster
deleted file mode 100644
--- a/test/bootstrap/testcases/gc-root-02/gc-root-02.foster
+++ /dev/null
@@ -1,53 +0,0 @@
-
-main = {
-    existsFile_EF3F635E = { x_7A5ED037 : { Int32 } => // must a fn typed arg
-        sep_DB598668 = prim ref 5; // must be defined outside of loop (obv)
-
-        REC loop = { x_C9BA77B0 : Ref Int8 => // loop must have an arg...
-              (loop sep_DB598668 as ()); // must pass as loop arg; other use doesn't count.
-        };
-
-        expect_i8 5;
-        print_i8 sep_DB598668^;
-
-        _ = (ign = {
-          loop (prim ref 4); // call must be in a function body
-        }; ign);
-
-        // a call here de-triggers the bug: loop sep_DB598668;
-
-        ()
-    };
-    // Need two calls.
-    foo = existsFile_EF3F635E { 1 };
-    bar = existsFile_EF3F635E { 2 };
-
-
-    foo = existsFile_top { 1 };
-    bar = existsFile_top { 2 };
-    ()
-};
-
-
-
-
-
-
-    existsFile_top = { x_7A5ED037 : { Int32 } => // must a fn typed arg
-        sep_DB598668 = prim ref (); // must be defined outside of loop (obv)
-
-        REC loop = { x_C9BA77B0 : Ref () => // loop must have an arg...
-              (loop sep_DB598668 as ()); // must pass as loop arg; other use doesn't count.
-        };
-
-        _ = {
-          //loop sep_DB598668 // call must be in a function body
-          loop (prim ref ()); // call must be in a function body
-        };
-
-
-        // a call here de-triggers the bug: loop sep_DB598668;
-
-        ()
-    };
-
diff --git a/test/bootstrap/testcases/reloc-doms-01/reloc-doms-01.foster b/test/bootstrap/testcases/reloc-doms-01/reloc-doms-01.foster
deleted file mode 100644
--- a/test/bootstrap/testcases/reloc-doms-01/reloc-doms-01.foster
+++ /dev/null
@@ -1,10 +0,0 @@
-main = {
-    outer = { f : { () } =>
-        REC loop = { (loop ! as ()); };
-        _ = (ign = { loop !  }; ign);
-        ()
-    };
-    outer {()};
-    outer {()};
-};
-
diff --git a/test/bootstrap/tuples/tuple-arity-raising/tuple-arity-raising.foster b/test/bootstrap/tuples/tuple-arity-raising/tuple-arity-raising.foster
deleted file mode 100644
--- a/test/bootstrap/tuples/tuple-arity-raising/tuple-arity-raising.foster
+++ /dev/null
@@ -1,59 +0,0 @@
-main = {
-  //part1 !;
-  part2 !;
-  part3 !;
-};
-
-/*
-part1 = {
-  expect_i32 3;
-  expect_i32 4;
-  noinline_llvm_foo (3, 4);
-  expect_i32 4;
-  expect_i32 5;
-  noinline_llvm_foo (4, 5);
-};
-
-noinline_llvm_foo = { p =>
-  p1 = case p of (a, _) -> a end;
-  p2 = case p of (_, a) -> a end;
-  print_i32 p1;
-  print_i32 p2;
-};
-*/
-
-part2 = {
-  r1 = Ref_ref (2, 3);
-  r2 = Ref_ref (2, 3, 4);
-  (3,4) >^ r1;
-  (3,4,5) >^ r2;
-  ()
-};
-
-Ref_ref :: forall (a:Type) { a => Ref a };
-Ref_ref = { v => prim ref v };
-
-
-part3 = {
-  zorba = { x : (Int64, Int64) =>
-    f = case x of (a, _) -> a end;
-    s = case x of (_, a) -> a end;
-    f ==Int64 s
-  };
-
-  expect_i1 True;
-  print_i1 (zorba (2, 2));
-
-  noinline_llvm_baz = { x =>
-    zorba (1,2) ==Bool zorba (1,x)
-  };
-
-  expect_i1 True;
-  noinline_llvm_baz 2 |> print_i1;
-  expect_i1 True;
-  noinline_llvm_baz 3 |> print_i1;
-  expect_i1 False;
-  noinline_llvm_baz 1 |> print_i1;
-
-  ()
-};
diff --git a/test/bootstrap/tuples/tuple-gc/tuple-gc.foster b/test/bootstrap/tuples/tuple-gc/tuple-gc.foster
--- a/test/bootstrap/tuples/tuple-gc/tuple-gc.foster
+++ b/test/bootstrap/tuples/tuple-gc/tuple-gc.foster
@@ -11,7 +11,7 @@
   test3 !;
 
   s = foster_subheap_create !;
-  prev = foster_subheap_activate s;
+  foster_subheap_activate s;
 
   test3 !;
 
diff --git a/test/speed/gc/adexp/adexp.foster b/test/speed/gc/adexp/adexp.foster
deleted file mode 100644
--- a/test/speed/gc/adexp/adexp.foster
+++ /dev/null
@@ -1,585 +0,0 @@
-snafuinclude Prelude "prelude";
-snafuinclude Map "map";
-snafuinclude Measure "bench/measure";
-
-// This program re-implements the ADEXP parsing code from the following blog posts:
-//
-//   https://medium.com/@teivah/good-code-vs-bad-code-in-golang-84cb3c5da49d
-//   https://medium.com/@val_deleplace/go-code-refactoring-the-23x-performance-hunt-156746b522f7
-//
-// Compared to the "improved" Go code (go 1.10.3), this code is about 13x faster.
-//   https://github.com/Deleplace/forks-golang-good-code-bad-code   branch: performance
-//   $ go test -bench=. | grep 'ns/op'
-//   go tool pprof suggests that nearly 40% of runtime is spend in   runtime.mallocgc.
-//   I think it's due to slow allocation + synchronization;
-//     actual GC appears to be less than 1% of runtime.
-//
-//
-// Using BytesChunk instead of Bytes increases runtime by a factor of 4,
-// because this code spends much of its time indexing byte-by-byte, and
-// the code for bytesGet is many dozens of instructions. In contrast,
-// the indexing operation for BytesChunk is three instructions (two loads + an add).
-//
-// Bounds checking is about 20% of the remaining runtime.
-
-
-main2 = {
-  bytes = b"""-TITLE IFPL
--ADEP CYYZ
--ALTNZ EASTERN :CREEK'()+,./
--ADES AFIL
--ARCID ACA878
--ARCTYP A333
--CEQPT SDE3FGHIJ3J5LM1ORVWXY
--EETFIR KZLC 0035
--EETFIR KZDV 0131
--EETFIR KZMP 0200
--EETFIR CZWG 0247
--EETFIR CZUL 0349
--EETFIR CZQX 0459
--EETFIR EGGX 0655
--EETFIR EGPX 0800
--EETFIR EGTT 0831
--EETFIR EHAA 0853
--EETFIR EBBU 0908
--EETFIR EDGG 0921
--EETFIR EDUU 0921
--ESTDATA -PTID XETBO -ETO 170302032300 -FL F390
--ESTDATA -PTID ARKIL -ETO 170302032300 -FL F390
--GEO -GEOID GEO01 -LATTD 490000N -LONGTD 0500000W
--GEO -GEOID GEO02 -LATTD 500000N -LONGTD 0400000W
--GEO -GEOID GEO04 -LATTD 520000N -LONGTD 0200000W
--BEGIN RTEPTS
--PT -PTID CYYZ -FL F000 -ETO 170301220429
--PT -PTID JOOPY -FL F390 -ETO 170302002327
--PT -PTID GEO01 -FL F390 -ETO 170302003347
--PT -PTID BLM -FL F171 -ETO 170302051642
--PT -PTID LSZH -FL F014 -ETO 170302052710
--END RTEPTS
--SPEED N0456 ARKIL
--SPEED N0457 LIZAD
--MSGTXT (ACH-BEL20B-LIML1050-EBBR-DOF/150521-14/HOC/1120F320 -18/PBN/B1 DOF/150521 REG/OODWK RVR/150 OPR/BEL ORGN/LSAZZQZG SRC/AFP RMK/AGCS EQUIPPED)
--COMMENT ???FPD.F15: N0410F300 ARLES UL153 PUNSA/N0410F300 UL153
-VADEM/N0400F320 UN853 PENDU/N0400F330 UN853 IXILU/N0400F340 UN853
-DIK/N0400F320 UY37 BATTY""";
-
-
-  msg = parseAdexpMessage bytes;
-  case msg
-    of $Message
-          typ
-          msgTitle
-          msgAdep
-          msgAdes
-          msgAlternate
-          msgArcid
-          msgArctyp
-          msgCeqpt
-          msgMessageText
-          msgComment
-          msgEetfir
-          msgSpeed
-          msgEstdata
-          msgGeo
-          msgRoutePoints
-        -> print_text "parsed message";
-  end;
-
-  ff = elapsed_time_n { msg = parseAdexpMessage bytes; () } 100000;
-  print_text "elapsed:";
-  print_text (foster_fmttime_secs ff);
-  print_text "per-op:";
-  print_text (foster_fmttime_secs (ff `div-f64` 100000.0));
-
-  ()
-};
-
-parseAdexpMessage = { bytes : Array Int8 => bytes |> preprocess |> process };
-
-preprocess :: { Array Int8 => List BytesChunk };
-preprocess = {  bytes =>
-  lines = rawSplitByte bytes "\n";
-  result = prim ref Nil:[BytesChunk];
-
-  bytesBegin  = bytesOfRawArray b"-BEGIN";
-  bytesEnd    = bytesOfRawArray b"-END";
-  bytesDash   = bytesOfRawArray b"-";
-  bytesEmpty_ = bytesOfRawArray b" ";
-  bytesComment = bytesOfRawArray b"//";
-
-  currentLine = listFoldl lines { line : Bytes => currentLine : Bytes =>
-    case ()
-      of _ if bytesHasPrefix line bytesEnd ->
-        currentLine
-
-      of _ if bytesHasPrefix line bytesBegin ->
-       (Cons (bytesChunkOfBytes currentLine) result^) >^ result;
-       bytesDash `bytesConcat` (bytesDrop (trim line) (bytesLength bytesBegin +Int64 1))
-
-      of _ if bytesHasPrefix line bytesDash ->
-       (Cons (bytesChunkOfBytes currentLine) result^) >^ result;
-       trim line
-
-      of _ if bytesHasPrefix line bytesEmpty_ ->
-        (currentLine `bytesConcat` bytesEmpty_) `bytesConcat` trim line
-
-      of _ ->
-        (currentLine `bytesConcat` bytesEmpty_) `bytesConcat` trim line
-    end
-  } (bytesEmpty !);
-
-  listReverse (Cons (bytesChunkOfBytes currentLine) result^)
-};
-
-type case Message
-  of $Message
-        MessageType
-        (Ref BytesChunk) // title
-        (Ref BytesChunk) // adep
-        (Ref BytesChunk) // ades
-        (Ref BytesChunk) // alternate
-        (Ref BytesChunk) // arcid
-        (Ref BytesChunk) // arctyp
-        (Ref BytesChunk) // ceqpt
-        (Ref BytesChunk) // messagetext
-        (Ref BytesChunk) // comment
-
-        (Ref (List BytesChunk)) // eetfir
-        (Ref (List BytesChunk)) // speed
-
-        (Ref (List Estdata))
-        (Ref (List Geo))
-        (Ref (List RoutePoints))
-  ;
-
-type case LineSummary
-  of $Skip
-  of $SimpleToken BytesChunk BytesChunk
-  of $ComplexToken BytesChunk (List (Map BytesChunk BytesChunk))
-  ;
-
-type case Estdata
-  of $Estdata BytesChunk BytesChunk Int32
-  ;
-
-type case Geo
-  of $Geo BytesChunk BytesChunk BytesChunk
-  ;
-type case RoutePoints
-  of $RoutePoints BytesChunk Int32 BytesChunk
-  ;
-
-type case MessageType
-  of $AdexpType
-  ;
-
-mapLine :: { BytesChunk => BytesChunk => LineSummary };
-mapLine = { line => bytesComment =>
-  case ()
-    of _ if bytesChunkLength line ==Int64 0 -> Skip
-    of _ if bytesChunkHasPrefix line bytesComment -> Skip
-    of _ if bytesChunkGet! line 0 !=Int8 '-' -> Skip
-    of _ ->
-      case parseLine (bytesChunkDrop line 1)
-        of ($Some token, $Some value) -> parseToken token value
-        of _ -> Skip
-      end
-  end
-};
-
-parseLine :: { BytesChunk => (Maybe BytesChunk, Maybe BytesChunk) };
-parseLine = { b =>
-  if bytesChunkLength b ==Int64 0
-    then (None, None)
-    else
-      // 'OR WHAT'
-      //    ^ i = 2
-      //  bytesTake 2 => OR
-      //  bytesDrop 3 => WHAT
-
-      i = bytesChunkIndexFrom b 0 ' ';
-      if i ==Int64 bytesChunkLength b
-        then (Some b, None)
-        else (Some (bytesChunkTake b i), Some (bytesChunkDrop b (i +Int64 1)))
-      end
-  end
-};
-
-parseToken :: { BytesChunk => BytesChunk => LineSummary };
-parseToken = { token => value =>
-  if bytesChunkLength token <SInt64 2 then Skip else
-    case bytesChunkGet! token 0
-      of c if c ==Int8 'A' -> // ADEP, ALTNZ, ADES, ARCID, ARCTYP
-        case ()
-         of _ if bytesChunkEqualRaw token b"ADEP"   -> SimpleToken token value
-         of _ if bytesChunkEqualRaw token b"ADES"   -> SimpleToken token value
-         of _ if bytesChunkEqualRaw token b"ALTNZ"  -> SimpleToken token value
-         of _ if bytesChunkEqualRaw token b"ARCID"  -> SimpleToken token value
-         of _ if bytesChunkEqualRaw token b"ARCTYP" -> SimpleToken token value
-         of _ -> Skip
-        end
-
-      of c if c ==Int8 'C' -> // CEQPT, COMMENT
-        case ()
-         of _ if bytesChunkEqualRaw token b"CEQPT"   -> SimpleToken token value
-         of _ if bytesChunkEqualRaw token b"COMMENT" -> SimpleToken token value
-         of _ -> Skip
-        end
-
-      of c if c ==Int8 'E' -> // ETO, EETFIR, ESTDATA
-        case ()
-         of _ if bytesChunkEqualRaw token b"ETO"     -> SimpleToken token value
-         of _ if bytesChunkEqualRaw token b"EETFIR"  -> SimpleToken token value
-         of _ if bytesChunkEqualRaw token b"ESTDATA" -> parseComplexToken token value
-         of _ -> Skip
-        end
-      of c if c ==Int8 'F' -> // FL
-        if bytesChunkEqualRaw token b"FL"
-          then SimpleToken token value
-          else Skip
-        end
-      of c if c ==Int8 'L' -> // LATTD, LONGTD
-        case ()
-          of _ if bytesChunkEqualRaw token b"LATTD"  -> SimpleToken token value
-          of _ if bytesChunkEqualRaw token b"LONGTD" -> SimpleToken token value
-          of _ -> Skip
-        end
-      of c if c ==Int8 'G' -> // GEO, GEOID
-        case ()
-         of _ if bytesChunkEqualRaw token b"GEO"   -> parseComplexToken token value
-         of _ if bytesChunkEqualRaw token b"GEOID" -> SimpleToken token value
-         of _ -> Skip
-        end
-      of c if c ==Int8 'M' -> // MSGTXT
-        if bytesChunkEqualRaw token b"MSGTXT"
-          then SimpleToken token value
-          else Skip
-        end
-      of c if c ==Int8 'S' -> // SPEED
-        if bytesChunkEqualRaw token b"SPEED"
-          then SimpleToken token value
-          else Skip
-        end
-      of c if c ==Int8 'R' -> // RTEPTS
-        if bytesChunkEqualRaw token b"RTEPTS"
-          then parseComplexToken token value
-          else Skip
-        end
-      of c if c ==Int8 'P' -> // PTID
-        if bytesChunkEqualRaw token b"PTID"
-          then SimpleToken token value
-          else Skip
-        end
-      of c if c ==Int8 'T' -> // TITLE
-        if bytesChunkEqualRaw token b"TITLE"
-          then SimpleToken token value
-          else Skip
-        end
-      of _ -> Skip
-    end
-  end
-};
-
-trim :: { Bytes => Bytes };
-trim = { b =>
-  REC countSpacesFwd = { idx : Int64 => count : Int64 =>
-    if idx <SInt64 bytesLength b
-      then if bytesGet! idx b ==Int8 ' '
-            then countSpacesFwd (idx +Int64 1) (count +Int64 1)
-            else count
-           end
-      else count
-    end
-  };
-  REC countSpacesBwd = { idx : Int64 => count : Int64 =>
-    if idx >=SInt64 0
-      then if bytesGet! idx b ==Int8 ' '
-            then countSpacesBwd (idx -Int64 1) (count +Int64 1)
-            else count
-           end
-      else count
-    end
-  };
-
-  ftrim = (bytesDrop b (countSpacesFwd 0 0));
-  len = bytesLength ftrim;
-  numbad = countSpacesBwd (len -Int64 1) 0;
-  bytesTake ftrim (len -Int64 numbad)
-};
-
-trimAlt :: { Bytes => Bytes };
-trimAlt = { b =>
-  REC countSpacesFwd = { idx : Int64 => count : Int64 =>
-    if idx <SInt64 bytesLength b
-      then if bytesGet! idx b ==Int8 ' '
-            then countSpacesFwd (idx +Int64 1) (count +Int64 1)
-            else count
-           end
-      else count
-    end
-  };
-  REC countSpacesBwd = { idx : Int64 => count : Int64 =>
-    if idx >=SInt64 0
-      then if bytesGet! idx b ==Int8 ' '
-            then countSpacesBwd (idx -Int64 1) (count +Int64 1)
-            else count
-           end
-      else count
-    end
-  };
-
-  ftrim = (bytesDrop b (countSpacesFwd 0 0));
-  len = bytesLength ftrim;
-  numbad = countSpacesBwd (len -Int64 1) 0;
-  bytesTake ftrim (len -Int64 numbad)
-};
-
-trimChunk :: { BytesChunk => BytesChunk };
-trimChunk = { b =>
-  REC countSpacesFwd = { idx : Int64 => count : Int64 =>
-    if idx <SInt64 bytesChunkLength b
-      then if bytesChunkGet! b idx ==Int8 ' '
-            then countSpacesFwd (idx +Int64 1) (count +Int64 1)
-            else count
-           end
-      else count
-    end
-  };
-  REC countSpacesBwd = { idx : Int64 => count : Int64 =>
-    if idx >=SInt64 0
-      then if bytesChunkGet! b idx ==Int8 ' '
-            then countSpacesBwd (idx -Int64 1) (count +Int64 1)
-            else count
-           end
-      else count
-    end
-  };
-
-  ftrim = (bytesChunkDrop b (countSpacesFwd 0 0));
-  len = bytesChunkLength ftrim;
-  numbad = countSpacesBwd (len -Int64 1) 0;
-  bytesChunkTake ftrim (len -Int64 numbad)
-};
-
-parseComplexToken :: { BytesChunk => BytesChunk => LineSummary };
-parseComplexToken = { token => value =>
-  matches = findSubfields value;
-  maps = prim ref Nil;
-
-  currentMap = listFoldl matches { sub => currentMap =>
-    case parseLine sub
-      of ($Some h, $Some l) ->
-        case mapLookup currentMap h bytesChunkCmp
-          of $None   -> mapInsert h (trimChunk l) currentMap bytesChunkCmp
-          of $Some v -> (Cons currentMap maps^) >^ maps;
-                        mapEmpty !
-        end
-      of _ -> currentMap
-    end
-  } (mapEmpty !);
-
-  ComplexToken token (listReverse (Cons currentMap maps^))
-};
-
-rawBytesFromTo = { arr => src => tgt =>
-  len = tgt -Int64 src;
-  bytesTake (bytesDrop (bytesOfRawArray arr) src) len
-};
-
-rawIndexFrom :: { Array Int8 => Int64 => Int8 => Int64 };
-rawIndexFrom = { arr => idx => c =>
-  len = arrayLength arr;
-  REC loop = { idx =>
-    if idx ==Int64 len
-      then idx
-      else if arr[idx] ==Int8 c
-             then idx
-             else loop (idx +Int64 1)
-            end
-    end
-  };
-  loop idx
-};
-
-rawSplitByte :: { Array Int8 => Int8 => List Bytes };
-rawSplitByte = { arr => sep =>
-  len = arrayLength arr;
-  REC loop = { pos => chunks =>
-    tgt = rawIndexFrom arr pos sep;
-    if tgt ==Int64 len
-      then      (Cons (rawBytesFromTo arr pos tgt) chunks)
-      else loop (tgt +Int64 1)
-                (Cons (rawBytesFromTo arr pos tgt) chunks)
-    end
-  };
-  loop 0 Nil
-};
-
-bytesChunkSplitByte :: { BytesChunk => Int8 => List BytesChunk };
-bytesChunkSplitByte = { b => sep =>
-  len = bytesChunkLength b;
-  REC loop = { pos => chunks =>
-    tgt = bytesChunkIndexFrom b pos sep;
-    if tgt ==Int64 len
-      then      (Cons (bytesChunkFromTo b pos tgt) chunks)
-      else loop (tgt +Int64 1)
-                (Cons (bytesChunkFromTo b pos tgt) chunks)
-    end
-  };
-  loop 0 Nil
-};
-
-findSubfields :: { BytesChunk => List BytesChunk };
-findSubfields = { value =>
-  subfields = bytesChunkSplitByte value '-';
-  case subfields
-    of $Nil -> subfields
-    of $Cons f rest ->
-      if bytesChunkLength (trimChunk f) ==Int64 0
-        then rest
-        else subfields
-      end
-  end
-};
-
-process :: { List BytesChunk => Message };
-process = { mbytes =>
-  emp = bytesChunk b"";
-  msgTitle = prim ref emp;
-  msgAdep  = prim ref emp;
-  msgAdes  = prim ref emp;
-  msgAlternate = prim ref emp;
-  msgArcid = prim ref emp;
-  msgArctyp = prim ref emp;
-  msgCeqpt = prim ref emp;
-  msgMessageText = prim ref emp;
-  msgComment = prim ref emp;
-
-  msgEetfir = prim ref Nil;
-  msgSpeed = prim ref Nil;
-  msgEstdata = prim ref Nil;
-  msgGeo = prim ref Nil;
-  msgRoutePoints = prim ref Nil;
-
-  bytesComment = bytesChunk b"//";
-
-  listIter mbytes { idx => line =>
-    data = mapLine line bytesComment;
-    case data
-      of $Skip -> ()
-      of $SimpleToken token value ->
-        case ()
-          of _ if bytesChunkEqualRaw token b"TITLE" -> value >^ msgTitle;
-          of _ if bytesChunkEqualRaw token b"ADEP"  -> value >^ msgAdep;
-          of _ if bytesChunkEqualRaw token b"ALTNZ" -> value >^ msgAlternate;
-          of _ if bytesChunkEqualRaw token b"ADES"  -> value >^ msgAdes;
-          of _ if bytesChunkEqualRaw token b"ARCID" -> value >^ msgArcid;
-          of _ if bytesChunkEqualRaw token b"ARCTYP" -> value >^ msgArctyp;
-          of _ if bytesChunkEqualRaw token b"CEQPT" -> value >^ msgCeqpt;
-          of _ if bytesChunkEqualRaw token b"MSGTXT" -> value >^ msgMessageText;
-          of _ if bytesChunkEqualRaw token b"EETFIR" -> (Cons value msgEetfir^) >^ msgEetfir;
-          of _ if bytesChunkEqualRaw token b"SPEED"  -> (Cons value msgSpeed^) >^ msgSpeed;
-          of _ -> prim kill-entire-process "process saw unxepcted simple token"
-        end
-
-      of $ComplexToken token maps ->
-        case ()
-          of _ if bytesChunkEqualRaw token b"ESTDATA" ->
-            listIter maps { idx => map =>
-
-              case mapLookupWithDefault map (bytesChunk b"FL") (bytesChunk b"") bytesChunkCmp
-                      |> extractFlightLevel
-                of $Some lvl ->
-                  ptid = mapLookupWithDefault map (bytesChunk b"PTID") (bytesChunk b"") bytesChunkCmp;
-                  eto  = mapLookupWithDefault map (bytesChunk b"ETO" ) (bytesChunk b"") bytesChunkCmp;
-                  (Cons (Estdata ptid eto lvl) msgEstdata^) >^ msgEstdata;
-
-                of $None -> prim kill-entire-process "unable to find flight level"
-              end
-            };
-
-          of _ if bytesChunkEqualRaw token b"GEO"  ->
-            listIter maps { idx => map =>
-                  geo = mapLookupWithDefault map (bytesChunk b"GEOID" ) (bytesChunk b"") bytesChunkCmp;
-                  lat = mapLookupWithDefault map (bytesChunk b"LATTD" ) (bytesChunk b"") bytesChunkCmp;
-                  lon = mapLookupWithDefault map (bytesChunk b"LONGTD") (bytesChunk b"") bytesChunkCmp;
-                  (Cons (Geo geo lat lon) msgGeo^) >^ msgGeo;
-            };
-
-
-          of _ if bytesChunkEqualRaw token b"RTEPTS" ->
-            listIter maps { idx => map =>
-
-              case mapLookupWithDefault map (bytesChunk b"FL") (bytesChunkEmpty !) bytesChunkCmp
-                      |> extractFlightLevel
-                of $Some lvl ->
-                  ptid = mapLookupWithDefault map (bytesChunk b"PTID") (bytesChunkEmpty !) bytesChunkCmp;
-                  eto  = mapLookupWithDefault map (bytesChunk b"ETO" ) (bytesChunkEmpty !) bytesChunkCmp;
-                  (Cons (RoutePoints ptid lvl eto) msgRoutePoints^) >^ msgRoutePoints;
-
-                of $None -> prim kill-entire-process "unable to find flight level"
-              end
-            };
-
-          of _ -> prim kill-entire-process "process saw unxepcted complex token"
-        end
-
-    end
-  };
-
-  Message
-    AdexpType
-          msgTitle
-          msgAdep
-          msgAdes
-          msgAlternate
-          msgArcid
-          msgArctyp
-          msgCeqpt
-          msgMessageText
-          msgComment
-
-          msgEetfir
-          msgSpeed
-          msgEstdata
-          msgGeo
-          msgRoutePoints
-};
-
-atoi = { t:BytesChunk =>
-  base = 10;
-  REC loop = { idx => acc => pow =>
-    if idx <SInt64 0
-      then Some acc
-      else
-        c = bytesChunkGet! t idx;
-        if is_ascii_digit c
-          then
-            s = (sext_i8_to_i32 c) -Int32 48;
-            loop (idx -Int64 1) (acc +Int32 (s *Int32 pow))
-                                (pow *Int32 base)
-          else None
-        end
-    end
-  };
-  loop (bytesChunkLength t -Int64 1) 0 1
-};
-
-extractFlightLevel = { bytes => bytesChunkDrop bytes 1 |> atoi };
-
-main = {
-  expect_i32 3;
-  print_i32 (listLength32 (bytesChunkSplitByte (bytesChunk b"123-456-789") '-'));
-
-  expect_i32 3;
-  print_i32 (listLength32 (bytesChunkSplitByte (bytesChunk b"--") '-'));
-
-  expect_i32 2;
-  print_i32 (listLength32 (bytesChunkSplitByte (bytesChunk b"-") '-'));
-
-  expect_i32 1;
-  print_i32 (listLength32 (bytesChunkSplitByte (bytesChunk b"*") '-'));
-
-  main2 !
-};
-
diff --git a/third_party/nacl-20110221/README.txt b/third_party/nacl-20110221/README.txt
deleted file mode 100644
--- a/third_party/nacl-20110221/README.txt
+++ /dev/null
@@ -1,12 +0,0 @@
-URL: https://nacl.cr.yp.to/
-Version: 20110221
-Description: Cryptography library
-License: public domain
-License File: none
-Local Modifications:
-  Customized build script to:
-    * Use "foster" instead of machine name
-    * Not generate C++ wrappers
-
-TODO: randombytes can block;
-      randombytes is used by crypto_box and crypto_sign.
diff --git a/third_party/vstinner_perf/makej b/third_party/vstinner_perf/makej
deleted file mode 100755
--- a/third_party/vstinner_perf/makej
+++ /dev/null
@@ -1,7 +0,0 @@
-#!/usr/bin/env python
-
-from __future__ import print_function
-
-import cpu_utils
-
-print(cpu_utils.get_available_cpu_count())
diff --git a/tools/c2f/c2f.cpp b/tools/c2f/c2f.cpp
--- a/tools/c2f/c2f.cpp
+++ b/tools/c2f/c2f.cpp
@@ -24,7 +24,6 @@
 //------------------------------------------------------------------------------
 #include <string>
 #include <sstream>
-#include <set>
 
 #include "clang/AST/AST.h"
 #include "clang/AST/ASTConsumer.h"
@@ -32,7 +31,6 @@
 #include "clang/ASTMatchers/ASTMatchers.h"
 #include "clang/ASTMatchers/ASTMatchFinder.h"
 #include "clang/Analysis/CFG.h"
-#include "clang/Analysis/Analyses/FormatString.h"
 #include "clang/Frontend/CompilerInstance.h"
 #include "clang/Frontend/FrontendActions.h"
 #include "clang/Tooling/CommonOptionsParser.h"
@@ -58,12 +56,7 @@
     return handled;
   }
 
-  bool shouldIgnore(const std::string& name) {
-    return ignoredSymbolNames.find(name) != ignoredSymbolNames.end();
-  }
-
   std::map<std::string, std::string> enumPrefixForConstants;
-  std::set<std::string> ignoredSymbolNames;
 
   // I couldn't figure out a better way of communicating between the
   // ClangTool invocation site and the ASTConsumer itself.
@@ -162,9 +155,9 @@
   return ty && ty->isVoidType();
 }
 
-bool isTrivialIntegerLiteralInRange(const Expr* e, int64_t lo, int64_t hi) {
+bool isTrivialIntegerLiteralInRange(const Expr* e, int lo, int hi) {
   if (auto lit = dyn_cast<IntegerLiteral>(e)) {
-    int64_t se = lit->getValue().getSExtValue();
+    auto se = lit->getValue().getSExtValue();
     return se >= lo && se <= hi;
   }
   return false;
@@ -469,9 +462,6 @@
   if (name == "strlen") return "ptrStrlen";
   if (name == "strcmp") return "ptrStrcmp";
 
-  if (name == "fabs")   return "abs-f64";
-  if (name == "fabsf")  return "abs-f32";
-
   if (name == "feof")   return "c2f_feof";
   if (name == "ferror") return "c2f_ferror";
   if (name == "fwrite") return "c2f_fwrite";
@@ -535,33 +525,6 @@
 }
 
 
-void emitUTF8orAsciiStringLiteral(StringRef data) {
-  // Clang's outputString uses octal escapes, but we only support
-  // Unicode escape sequences in non-byte-strings.
-  bool useTriple = data.count('\n') > 1;
-  // TODO must also check the str doesn't contain 3 consecutive dquotes.
-  // TODO distinguish text vs byte strings...?
-  llvm::outs() << "(strLit " << "b" << (useTriple ? "\"\"\"" : "\"");
-  for (char c : data) {
-    switch(c) {
-      case '\n': llvm::outs() << (useTriple ? "\n" : "\\n"); break;
-      case '\t': llvm::outs() << "\\t"; break;
-      case '\\': llvm::outs() << "\\\\"; break;
-      case '"' : llvm::outs() << (useTriple ? "\"" : "\\\""); break;
-      default:
-        if (isprint(c)) {
-          llvm::outs() << c;
-        } else {
-          llvm::outs() << llvm::format("\\u{%02x}", (unsigned char) c);
-        }
-        break;
-    }
-  }
-  llvm::outs() << "\\x00";
-  llvm::outs() << (useTriple ? "\"\"\"" : "\"");
-  llvm::outs() << ")";
-}
-
 
 class MutableLocalHandler : public MatchFinder::MatchCallback {
 public:
@@ -846,29 +809,16 @@
 bool shouldProcessTopLevelDecl(const Decl* d, const FileClassifier& FC) {
   if (!FC.isFromMainFile(d)) {
     if (FC.isFromSystemHeader(d)) return false;
-    if (auto nd = dyn_cast<NamedDecl>(d)) {
-      if (globals.shouldIgnore(nd->getNameAsString())) {
-        return false;
-      }
-    }
     if (auto td = dyn_cast<TagDecl>(d)) {
       if (td->isThisDeclarationADefinition() && td->isCompleteDefinition()) {
         return true;
       }
     }
-    if (isa<FunctionDecl>(d)) {
-      return true;
-    }
+    if (isa<FunctionDecl>(d)) return true;
 
     // skip it if incomplete or from system header
     return false;
   }
-
-  if (auto nd = dyn_cast<NamedDecl>(d)) {
-    if (globals.shouldIgnore(nd->getNameAsString())) {
-      return false;
-    }
-  }
   return true;
 }
 
@@ -1036,117 +986,6 @@
   }
 };
 
-
-class C2F_FormatStringHandler : public clang::analyze_format_string::FormatStringHandler {
-public:
-  const char* prevBase;
-  ASTContext& ctx;
-
-  C2F_FormatStringHandler(const char* base, ASTContext& ctx) : prevBase(base), ctx(ctx) {}
-  ~C2F_FormatStringHandler() {}
-
-  void emitStringContentsUpTo(const char* place, unsigned offset) {
-    if (place == prevBase) {
-      return;
-    }
-
-    llvm::outs() << "(printStr ";
-    emitUTF8orAsciiStringLiteral(StringRef(prevBase, (place - prevBase)));
-    llvm::outs() << ")";
-    prevBase = place + offset;
-  }
-
-  void HandleNullChar(const char* nullChar) override {
-    emitStringContentsUpTo(nullChar, 1);
-    printf("/* handle null char */\n");
-    return;
-  }
-
-  void HandlePosition(const char* startPos, unsigned len) override {
-    emitStringContentsUpTo(startPos, len);
-    printf("/* handle position: %.*s */\n", len, startPos);
-    return;
-  }
-
-  void HandleInvalidPosition(const char* startPos, unsigned len, clang::analyze_format_string::PositionContext p) override {
-    //emitStringContentsUpTo(startPos, len);
-    printf("/* handle invalid position: %.*s */\n", len, startPos);
-    return;
-  }
-
-  void HandleZeroPosition(const char* startPos, unsigned len) override {
-    emitStringContentsUpTo(startPos, len);
-    printf("/* handle zero position: %.*s */\n", len, startPos);
-    return;
-  }
-
-  void HandleEmptyObjCModifierFlag(const char* startFlags, unsigned len) override {
-    emitStringContentsUpTo(startFlags, len);
-    printf("/* handle emtpy objc flags: %.*s */\n", len, startFlags);
-    return;
-  }
-
-  void HandleInvalidObjCModifierFlag(const char* startFlag, unsigned len) override {
-    emitStringContentsUpTo(startFlag, len);
-    printf("/* handle invalid objc flags: %.*s */\n", len, startFlag);
-    return;
-  }
-
-  void HandleObjCFlagsWithNonObjCConversion(const char* flagsStart, const char* flagsEnd, const char* conversionPos) override {
-    emitStringContentsUpTo(flagsStart, flagsEnd - flagsStart);
-    printf("/* handle objc flags: %.*s */\n", flagsEnd - flagsStart, flagsStart);
-    return;
-  }
-
-  bool HandleScanfSpecifier(const analyze_scanf::ScanfSpecifier& fs, const char* startSpecifier, unsigned specifierLen) override {
-    return true;
-  }
-
-  bool HandlePrintfSpecifier(const analyze_printf::PrintfSpecifier& fs, const char* startSpecifier, unsigned specifierLen) override {
-    emitStringContentsUpTo(startSpecifier, specifierLen);
-    fprintf(stderr, "/* handle printf specifier: %.*s */\n", specifierLen, startSpecifier);
-    fprintf(stderr, "/* format specifier: getArgIndex: %d */\n", fs.getArgIndex());
-    fprintf(stderr, "/* format specifier: usesPositionalArg: %d */\n", fs.usesPositionalArg());
-    fprintf(stderr, "/* format specifier: getPositionalArgIndex: %d */\n", fs.getPositionalArgIndex());
-    fprintf(stderr, "/* format specifier: hasStandardLengthModifier: %d */\n", fs.hasStandardLengthModifier());
-    //fprintf(stderr, "/* format specifier: hasStandardConversionSpecifier: %d */\n", fs.hasStandardConversionSpecifier());
-    fprintf(stderr, "/* format specifier: hasStandardLengthConversionCombination: %d */\n", fs.hasStandardLengthConversionCombination());
-    fprintf(stderr, "/* printf specifier: consumesDataArgument: %d */\n", fs.consumesDataArgument());
-    fprintf(stderr, "/* printf specifier: hasValidPlusPrefix: %d */\n", fs.hasValidPlusPrefix());
-    fprintf(stderr, "/* printf specifier: hasValidSpacePrefix: %d */\n", fs.hasValidSpacePrefix());
-    fprintf(stderr, "/* printf specifier: hasValidLeadingZeros: %d */\n", fs.hasValidLeadingZeros());
-    fprintf(stderr, "/* printf specifier: hasValidPrecision: %d */\n", fs.hasValidPrecision());
-    fprintf(stderr, "/* printf specifier: hasValidFieldWidth: %d */\n", fs.hasValidFieldWidth());
-    fprintf(stderr, "/* printf specifier: argType: isValid: %d */\n", fs.getArgType(ctx, false).isValid());
-    std::string tyname = fs.getArgType(ctx, false).getRepresentativeTypeName(ctx);
-    fprintf(stderr, "/* printf specifier: argType: %s */\n", tyname.c_str());
-    fflush(stderr);
-    llvm::errs() << "/* printf conversion: " << fs.getConversionSpecifier().getCharacters() << " */\n";
-    llvm::errs() << "/* printf precision: "; fs.getPrecision().toString(llvm::errs()); llvm::errs() << " */\n";
-    llvm::errs() << "/* printf field width: "; fs.getFieldWidth().toString(llvm::errs()); llvm::errs() << " */\n";
-    llvm::errs() << "/* printf length modifier: " << fs.getLengthModifier().toString() << " */\n";
-    return true;
-  }
-
-  bool HandleInvalidPrintfConversionSpecifier(const analyze_printf::PrintfSpecifier& fs, const char* startSpecifier, unsigned specifierLen) override {
-    emitStringContentsUpTo(startSpecifier, specifierLen);
-    fprintf(stderr, "/* invalid printf conversion specifier: %.*s */\n", specifierLen, startSpecifier);
-    return true;
-  }
-
-  bool HandleInvalidScanfConversionSpecifier(const analyze_scanf::ScanfSpecifier& fs, const char* startSpecifier, unsigned specifierLen) override {
-    emitStringContentsUpTo(startSpecifier, specifierLen);
-    fprintf(stderr, "/* invalid scanf conversion specifier: %.*s */\n", specifierLen, startSpecifier);
-    return true;
-  }
-
-  void HandleIncompleteScanList(const char* start, const char* end) override {
-    emitStringContentsUpTo(start, end - start);
-    printf("/* incomplete scan list: %.*s */\n", end - start, start);
-  }
-};
-
-
 class C2F_GlobalVariableDetector : public ASTConsumer {
 public:
   C2F_GlobalVariableDetector(const SourceManager &SM) : FC(SM) {}
@@ -1167,7 +1006,7 @@
 
 class MyASTConsumer : public ASTConsumer {
 public:
-  MyASTConsumer(const CompilerInstance &CI) : lastloc(), CI(CI), FC(CI.getSourceManager()) { }
+  MyASTConsumer(const SourceManager &SM) : lastloc(), FC(SM) { }
 
   void handleIfThenElse(ContextKind ctx, IfExprOrStmt ies, const Stmt* cnd, const Stmt* thn, const Stmt* els) {
     bool needTrailingUnit = ies == AnIfStmt && !isCompoundWithTrailingReturn(thn);
@@ -1196,7 +1035,7 @@
   std::string getBlockName(const CFGBlock& cb) {
     std::string s;
     std::stringstream ss(s);
-    ss << "forcecont_";
+    ss << "mustbecont_";
 
     if (const Stmt* lab = cb.getLabel()) {
       if (const LabelStmt* labstmt = dyn_cast<LabelStmt>(lab)) {
@@ -1323,8 +1162,8 @@
     if (optDumpCFGs) {
       llvm::outs().flush();
       llvm::errs() << "/*\n";
-      //LangOptions LO;
-      cfg->dump(CI.getLangOpts(), false);
+      LangOptions LO;
+      cfg->dump(LO, false);
       llvm::errs() << "\n*/\n";
       llvm::errs().flush();
     }
@@ -2026,9 +1865,7 @@
   }
 
   bool tryHandleCallPrintf(const CallExpr* ce) {
-    // TODO handle fprintf, etc.
     if (!isDeclNamed("printf", ce->getCallee()->IgnoreParenImpCasts())) return false;
-
     if (ce->getNumArgs() == 1) {
       // Assume one-arg printf means literal text.
       llvm::outs() << "(printStr ";
@@ -2036,18 +1873,9 @@
       llvm::outs() << ")";
       return true;
     }
-
     if (ce->getNumArgs() != 2) return false;
 
     if (auto slit = dyn_cast<StringLiteral>(ce->getArg(0)->IgnoreParenImpCasts())) {
-      const std::string& s = slit->getString();
-      bool isFreeBSDkprintf = false;
-      C2F_FormatStringHandler handler(s.c_str(), *Ctx);
-      fprintf(stderr, "// parsing format string: %s\n", s.c_str());
-      clang::analyze_format_string::ParsePrintfString(handler, &s[0], &s[s.size()],
-          CI.getLangOpts(), CI.getTarget(), isFreeBSDkprintf);
-
-
       if (slit->getString() == "%d\n") {
         std::string tynm = tyName(ce->getArg(1)->getType().getTypePtr());
         std::string printfn;
@@ -2322,25 +2150,6 @@
     }
   }
 
-  bool guaranteedNotToTruncate(const Expr* e, const std::string& ty, const Type* t) {
-    // t and ty are the destination type; the source "type" is irrelevant for literals.
-    if (ty == "Int8") {
-      return t->isSignedIntegerType()
-                ? isTrivialIntegerLiteralInRange(e, 0 - (1 << 7), (1 << 7) - 1)
-                : isTrivialIntegerLiteralInRange(e, 0           , (1 << 8) - 1);
-    } else if (ty == "Int16") {
-      return t->isSignedIntegerType()
-                ? isTrivialIntegerLiteralInRange(e, 0 - (1 << 15), (1 << 15) - 1)
-                : isTrivialIntegerLiteralInRange(e, 0            , (1 << 16) - 1);
-    } else if (ty == "Int32") {
-      return t->isSignedIntegerType()
-                ? isTrivialIntegerLiteralInRange(e, 0 - (1 << 31), (1 << 31) - 1)
-                : isTrivialIntegerLiteralInRange(e, 0            , 4294967295);
-    }
-
-    return false;
-  }
-
   bool isLargerOrEqualSizedType(const Type* t1, const Type* t2) {
     return Ctx->getTypeSize(t1) >= Ctx->getTypeSize(t2);
   }
@@ -2424,16 +2233,20 @@
       break;
     case CK_IntegralCast: {
       std::string cast = "";
-      std::string srcTy = tyName(exprTy(ce->getSubExpr()));
-      std::string dstTy = tyName(exprTy(ce));
-
-      if (srcTy == dstTy
-       || isNestedCastThatCancelsOut(ce)
+
+      if (isNestedCastThatCancelsOut(ce)
+       || isTrivialIntegerLiteralInRange(ce->getSubExpr(), 0, 127)
        || isa<CharacterLiteral>(ce->getSubExpr())
-       || guaranteedNotToTruncate(ce->getSubExpr(), dstTy, exprTy(ce))) {
+       || (exprTy(ce)->isUnsignedIntegerType() &&
+             isTrivialIntegerLiteralInRange(ce->getSubExpr(), 0, 255))) {
         // don't print anything, no cast needed
       } else {
-        cast = intCastFromTo(srcTy, dstTy, exprTy(ce->getSubExpr())->isSignedIntegerType());
+        std::string srcTy = tyName(exprTy(ce->getSubExpr())) ;
+        std::string dstTy = tyName(exprTy(ce));
+
+        if (srcTy != dstTy) {
+          cast = intCastFromTo(srcTy, dstTy, exprTy(ce->getSubExpr())->isSignedIntegerType());
+        }
       }
 
       if (cast == "") {
@@ -2709,7 +2522,31 @@
         llvm::outs() << "True";
       } else {
         if (lit->isUTF8() || lit->isAscii()) {
-          emitUTF8orAsciiStringLiteral(lit->getString());
+          // Clang's outputString uses octal escapes, but we only support
+          // Unicode escape sequences in non-byte-strings.
+          StringRef data = lit->getString();
+          bool useTriple = data.count('\n') > 1;
+          // TODO must also check the str doesn't contain 3 consecutive dquotes.
+          // TODO distinguish text vs byte strings...?
+          llvm::outs() << "(strLit " << "b" << (useTriple ? "\"\"\"" : "\"");
+          for (char c : data) {
+              switch(c) {
+              case '\n': llvm::outs() << (useTriple ? "\n" : "\\n"); break;
+              case '\t': llvm::outs() << "\\t"; break;
+              case '\\': llvm::outs() << "\\\\"; break;
+              case '"' : llvm::outs() << (useTriple ? "\"" : "\\\""); break;
+              default:
+                if (isprint(c)) {
+                  llvm::outs() << c;
+                } else {
+                  llvm::outs() << llvm::format("\\u{%02x}", (unsigned char) c);
+                }
+                break;
+              }
+          }
+          llvm::outs() << "\\x00";
+          llvm::outs() << (useTriple ? "\"\"\"" : "\"");
+          llvm::outs() << ")";
         } else {
           llvm::outs() << "// non UTF8 string\n";
           llvm::outs() << "(strLit ";
@@ -2722,12 +2559,8 @@
         llvm::outs() << (lit->getValueAsApproximateDouble() != 0.0 ? "True" : "False");
       } else {
         std::string litstr = FC.getText(*lit);
-        llvm::errs() << "// float lit str: " << litstr << "\n";
-        if (!litstr.empty()) {
-          char last = litstr[litstr.size() - 1];
-          if (last == 'f' || last == 'F') {
-            litstr[litstr.size() - 1] = ' ';
-          }
+        if (!litstr.empty() && litstr[litstr.size() - 1] == 'f') {
+          litstr[litstr.size() - 1] = ' ';
         }
         if (!litstr.empty()) llvm::outs() << litstr;
         else {
@@ -3146,7 +2979,6 @@
   std::map<std::string, bool> mutableLocals;
   std::map<std::string, bool> mutableLocalAliases;
   VoidPtrCasts voidPtrCasts;
-  const CompilerInstance& CI;
   const FileClassifier FC;
   ASTContext* Ctx;
 
@@ -3177,132 +3009,21 @@
   }
 };
 
-
 // For each source file provided to the tool, a new FrontendAction is created.
 class C2F_FrontendAction : public ASTFrontendAction {
 public:
   C2F_FrontendAction() {}
 
   std::unique_ptr<ASTConsumer> CreateASTConsumer(CompilerInstance &CI, StringRef file) override {
-    return llvm::make_unique<MyASTConsumer>(CI);
+    return llvm::make_unique<MyASTConsumer>(CI.getSourceManager());
   }
 };
 
-void initializeIgnoredSymbolNames() {
-  globals.ignoredSymbolNames.insert("csmith_sink_");
-  globals.ignoredSymbolNames.insert("__undefined");
-  globals.ignoredSymbolNames.insert("platform_main_begin");
-  globals.ignoredSymbolNames.insert("platform_main_end");
-  globals.ignoredSymbolNames.insert("transparent_crc");
-  globals.ignoredSymbolNames.insert("transparent_crc_bytes");
-  globals.ignoredSymbolNames.insert("crc32_byte");
-  globals.ignoredSymbolNames.insert("crc32_8bytes");
-  globals.ignoredSymbolNames.insert("crc32_gentab");
-  globals.ignoredSymbolNames.insert("safe_rshift_func_int64_t_s_u");
-  globals.ignoredSymbolNames.insert("safe_rshift_func_int64_t_u_u");
-  globals.ignoredSymbolNames.insert("safe_rshift_func_int64_t_u_s");
-  globals.ignoredSymbolNames.insert("safe_rshift_func_int64_t_s_s");
-  globals.ignoredSymbolNames.insert("safe_lshift_func_int64_t_s_u");
-  globals.ignoredSymbolNames.insert("safe_lshift_func_int64_t_u_u");
-  globals.ignoredSymbolNames.insert("safe_lshift_func_int64_t_u_s");
-  globals.ignoredSymbolNames.insert("safe_lshift_func_int64_t_s_s");
-  globals.ignoredSymbolNames.insert("safe_rshift_func_uint64_t_s_u");
-  globals.ignoredSymbolNames.insert("safe_rshift_func_uint64_t_u_u");
-  globals.ignoredSymbolNames.insert("safe_rshift_func_uint64_t_u_s");
-  globals.ignoredSymbolNames.insert("safe_rshift_func_uint64_t_s_s");
-  globals.ignoredSymbolNames.insert("safe_lshift_func_uint64_t_s_u");
-  globals.ignoredSymbolNames.insert("safe_lshift_func_uint64_t_u_u");
-  globals.ignoredSymbolNames.insert("safe_lshift_func_uint64_t_u_s");
-  globals.ignoredSymbolNames.insert("safe_lshift_func_uint64_t_s_s");
-  globals.ignoredSymbolNames.insert("safe_div_func_int8_t_s_s");
-  globals.ignoredSymbolNames.insert("safe_div_func_int16_t_s_s");
-  globals.ignoredSymbolNames.insert("safe_div_func_int32_t_s_s");
-  globals.ignoredSymbolNames.insert("safe_div_func_int64_t_s_s");
-  globals.ignoredSymbolNames.insert("safe_mod_func_int8_t_s_s");
-  globals.ignoredSymbolNames.insert("safe_mod_func_int16_t_s_s");
-  globals.ignoredSymbolNames.insert("safe_mod_func_int32_t_s_s");
-  globals.ignoredSymbolNames.insert("safe_mod_func_int64_t_s_s");
-  globals.ignoredSymbolNames.insert("safe_mul_func_int8_t_s_s");
-  globals.ignoredSymbolNames.insert("safe_mul_func_int16_t_s_s");
-  globals.ignoredSymbolNames.insert("safe_mul_func_int32_t_s_s");
-  globals.ignoredSymbolNames.insert("safe_mul_func_int64_t_s_s");
-  globals.ignoredSymbolNames.insert("safe_unary_minus_func_int8_t_s");
-  globals.ignoredSymbolNames.insert("safe_add_func_int8_t_s_s");
-  globals.ignoredSymbolNames.insert("safe_sub_func_int8_t_s_s");
-  globals.ignoredSymbolNames.insert("safe_lshift_func_int8_t_s_s");
-  globals.ignoredSymbolNames.insert("safe_lshift_func_int8_t_s_u");
-  globals.ignoredSymbolNames.insert("safe_rshift_func_int8_t_s_s");
-  globals.ignoredSymbolNames.insert("safe_rshift_func_int8_t_s_u");
-  globals.ignoredSymbolNames.insert("safe_unary_minus_func_int16_t_s");
-  globals.ignoredSymbolNames.insert("safe_add_func_int16_t_s_s");
-  globals.ignoredSymbolNames.insert("safe_sub_func_int16_t_s_s");
-  globals.ignoredSymbolNames.insert("safe_lshift_func_int16_t_s_s");
-  globals.ignoredSymbolNames.insert("safe_lshift_func_int16_t_s_u");
-  globals.ignoredSymbolNames.insert("safe_rshift_func_int16_t_s_s");
-  globals.ignoredSymbolNames.insert("safe_rshift_func_int16_t_s_u");
-  globals.ignoredSymbolNames.insert("safe_unary_minus_func_int32_t_s");
-  globals.ignoredSymbolNames.insert("safe_add_func_int32_t_s_s");
-  globals.ignoredSymbolNames.insert("safe_sub_func_int32_t_s_s");
-  globals.ignoredSymbolNames.insert("safe_lshift_func_int32_t_s_s");
-  globals.ignoredSymbolNames.insert("safe_lshift_func_int32_t_s_u");
-  globals.ignoredSymbolNames.insert("safe_rshift_func_int32_t_s_s");
-  globals.ignoredSymbolNames.insert("safe_rshift_func_int32_t_s_u");
-  globals.ignoredSymbolNames.insert("safe_unary_minus_func_int64_t_s");
-  globals.ignoredSymbolNames.insert("safe_add_func_int64_t_s_s");
-  globals.ignoredSymbolNames.insert("safe_sub_func_int64_t_s_s");
-  globals.ignoredSymbolNames.insert("safe_unary_minus_func_uint8_t_u");
-  globals.ignoredSymbolNames.insert("safe_add_func_uint8_t_u_u");
-  globals.ignoredSymbolNames.insert("safe_sub_func_uint8_t_u_u");
-  globals.ignoredSymbolNames.insert("safe_mul_func_uint8_t_u_u");
-  globals.ignoredSymbolNames.insert("safe_mod_func_uint8_t_u_u");
-  globals.ignoredSymbolNames.insert("safe_div_func_uint8_t_u_u");
-  globals.ignoredSymbolNames.insert("safe_lshift_func_uint8_t_u_s");
-  globals.ignoredSymbolNames.insert("safe_lshift_func_uint8_t_u_u");
-  globals.ignoredSymbolNames.insert("safe_rshift_func_uint8_t_u_s");
-  globals.ignoredSymbolNames.insert("safe_rshift_func_uint8_t_u_u");
-  globals.ignoredSymbolNames.insert("safe_unary_minus_func_uint16_t_u");
-  globals.ignoredSymbolNames.insert("safe_add_func_uint16_t_u_u");
-  globals.ignoredSymbolNames.insert("safe_sub_func_uint16_t_u_u");
-  globals.ignoredSymbolNames.insert("safe_mul_func_uint16_t_u_u");
-  globals.ignoredSymbolNames.insert("safe_mod_func_uint16_t_u_u");
-  globals.ignoredSymbolNames.insert("safe_div_func_uint16_t_u_u");
-  globals.ignoredSymbolNames.insert("safe_lshift_func_uint16_t_u_s");
-  globals.ignoredSymbolNames.insert("safe_lshift_func_uint16_t_u_u");
-  globals.ignoredSymbolNames.insert("safe_rshift_func_uint16_t_u_s");
-  globals.ignoredSymbolNames.insert("safe_rshift_func_uint16_t_u_u");
-  globals.ignoredSymbolNames.insert("safe_unary_minus_func_uint32_t_u");
-  globals.ignoredSymbolNames.insert("safe_add_func_uint32_t_u_u");
-  globals.ignoredSymbolNames.insert("safe_sub_func_uint33_t_u_u");
-  globals.ignoredSymbolNames.insert("safe_mul_func_uint32_t_u_u");
-  globals.ignoredSymbolNames.insert("safe_mod_func_uint32_t_u_u");
-  globals.ignoredSymbolNames.insert("safe_div_func_uint32_t_u_u");
-  globals.ignoredSymbolNames.insert("safe_lshift_func_uint32_t_u_s");
-  globals.ignoredSymbolNames.insert("safe_lshift_func_uint32_t_u_u");
-  globals.ignoredSymbolNames.insert("safe_rshift_func_uint32_t_u_s");
-  globals.ignoredSymbolNames.insert("safe_rshift_func_uint32_t_u_u");
-  globals.ignoredSymbolNames.insert("safe_unary_minus_func_uint64_t_u");
-  globals.ignoredSymbolNames.insert("safe_add_func_uint64_t_u_u");
-  globals.ignoredSymbolNames.insert("safe_sub_func_uint64_t_u_u");
-  globals.ignoredSymbolNames.insert("safe_mul_func_uint64_t_u_u");
-  globals.ignoredSymbolNames.insert("safe_mod_func_uint64_t_u_u");
-  globals.ignoredSymbolNames.insert("safe_div_func_uint64_t_u_u");
-  globals.ignoredSymbolNames.insert("safe_add_func_float_f_f");
-  globals.ignoredSymbolNames.insert("safe_sub_func_float_f_f");
-  globals.ignoredSymbolNames.insert("safe_mul_func_float_f_f");
-  globals.ignoredSymbolNames.insert("safe_div_func_float_f_f");
-  globals.ignoredSymbolNames.insert("safe_add_func_double_f_f");
-  globals.ignoredSymbolNames.insert("safe_sub_func_double_f_f");
-  globals.ignoredSymbolNames.insert("safe_mul_func_double_f_f");
-  globals.ignoredSymbolNames.insert("safe_div_func_double_f_f");
-  globals.ignoredSymbolNames.insert("safe_convert_func_float_to_int32_t");
-}
-
 // You'll probably want to invoke with -fparse-all-comments
 int main(int argc, const char **argv) {
   CommonOptionsParser op(argc, argv, CtoFosterCategory);
   ClangTool Tool(op.getCompilations(), op.getSourcePathList());
 
-  initializeIgnoredSymbolNames();
 
   Tool.run(newFrontendActionFactory<C2F_TypeDeclHandler_FA>().get());
 
@@ -3364,4 +3085,3 @@
 //     we won't properly distinguish the two (both become Foo).
 //   * Missing returns from non-void-returning functions will (reasonably!)
 //     lead to type errors in the generated Foster code.
-
